# Monolith Python → Monolith-RS Parity Master Plan

**Last updated:** 2026-01-19
**Owner:** Monolith-RS
**Status:** IN PROGRESS (living document)

This is the **single source of truth** for **porting every line of Python in `monolith/`** into Rust (`monolith-rs/`) to reach **full or максимально close parity**. It is intentionally exhaustive and process-heavy so nothing is missed.

---

## 0) Non-Negotiables (Read First)

- We are targeting **line-for-line parity** in behavior and public surface area, not necessarily 1:1 API style.
- **Every Python file in `monolith/` must be accounted for** in this plan with a concrete Rust destination or an explicit, justified N/A.
- We must **preserve semantics** even if the Rust implementation uses different internal structures.
- **No vendoring** of TensorFlow runtime libraries. TF runtime must be optional and dynamic (Linux x86_64 only, best effort elsewhere).
- **Candle remains the default** inference backend unless a true TF SavedModel runtime is available.
- **All parity gaps must be listed**, tracked, and closed or explicitly justified.

---

## 1) Definition of “Parity”

Parity means:
- **Behavior:** Rust outputs match Python outputs given the same inputs and configuration.
- **Protocol:** gRPC, protobuf, and disk formats remain compatible.
- **Config:** CLI flags, config files, and environment variables behave the same.
- **Ops:** Custom TensorFlow ops and side effects are supported or mapped.
- **I/O:** File formats (TFRecord, SavedModel, checkpointing) are compatible.
- **Tests:** Rust tests cover the same scenarios as Python tests.

Parity **does NOT require**:
- Identical code structure.
- Identical APIs (Rust idioms allowed if external behavior is the same).

---

## 2) Scope Inventory (Every Python Area)

We will port **all** of:
- `monolith/agent_service/**`
- `monolith/core/**`
- `monolith/native_training/**`
- `monolith/monolith_workspace.bzl`, `monolith/tf_serving_workspace.bzl`
- `monolith/utils.py`, `monolith/path_utils.py`, `monolith/base_runner.py`, `monolith/tpu_runner.py`, `monolith/gpu_runner.py`
- Any Python entry points under `monolith/**` and `monolith/native_training/**`

We will also account for:
- `third_party/org_tensorflow/**` (patches that affect behavior)
- Any Python-dependent tooling or code-gen (feature lists, ops, TF Serving config)

---

## 3) Master Porting Workflow (Every Line Tracked)

### 3.1 Generate a **Line-Level Inventory**
**Goal:** Track every Python line and its Rust destination.

**Tasks:**
- Build a script to enumerate:
  - File path
  - Line count
  - Top-level symbols (classes, functions)
- Create `monolith-rs/PYTHON_PARITY_INDEX.md` containing:
  - One row per file with line count and status
  - A link to a per-file checklist

**Status:** DONE\n\n**Artifacts:**\n- `monolith-rs/PYTHON_PARITY_INDEX.md` (334 files enumerated)

### 3.2 Per-File Parity Checklist
**Goal:** Each Python file gets a dedicated Rust parity checklist.

**Tasks:**
- For every Python file, create a matching checklist file:
  - `monolith-rs/parity/<path>.md` (mirrors Python path)
  - Each checklist includes:
    - Function/class list
    - Behavior notes
    - Rust mapping (file + symbol)
    - Test coverage
    - Open gaps

**Status:** DONE\n\n**Artifacts:**\n- `monolith-rs/parity/**` (per-file checklist for every `monolith/**/*.py`)

### 3.3 Porting Discipline
**Rule:** No file is considered “done” until:
- Feature parity verified against Python tests or equivalent.
- Rust tests added.
- Input/output formats validated.
- Performance regressions documented.

---

## 4) Cross-Cutting Infrastructure to Port First

These enable all module ports:

### 4.1 Config & CLI Parity
- CLI flags match Python entry points.
- Environment variable behavior identical.
- JSON/YAML config parsing for training/serving.

### 4.2 Protobufs & gRPC
- Ensure all `.proto` files compiled and exposed.
- TFS prediction and model service compatibility.

### 4.3 Data Formats
- TFRecord read/write
- Example / ExampleBatch / Instance formats
- Feature list and feature parsing rules

### 4.4 Checkpoint & Export
- SavedModel or SavedModel-like export
- Embeddings and optimizer states
- Manifest compatibility

### 4.5 Runtime & Scheduling
- Distributed training runtime hooks
- Service discovery (ZK/Consul)
- Parameter sync

---

## 5) Module-by-Module Port Plan (Exhaustive)

### 5.1 `monolith/agent_service/**`
**Goal:** Full parity with agent service, model manager, replica management, TF Serving integration.

**Key Submodules:**
- `agent_service.py`
- `agent_controller.py`
- `agent_v1.py`, `agent_v3.py`
- `backends.py`, `replica_manager.py`, `model_manager.py`
- `tfs_client.py`, `tfs_wrapper.py`, `tfs_monitor.py`
- `utils.py`, `constants.py`

**Porting Steps (per file):**
1. Map gRPC service behaviors to `monolith-serving`.
2. Implement config + environment behavior.
3. Recreate client utilities for Predict/ModelStatus/Metadata.
4. Validate with Python test vectors.

**Status:** IN PROGRESS (mapping phase)

### 5.2 `monolith/core/**`
**Goal:** Full parity with model registry, task base classes, hyperparams, core layers.

**Key Files:**
- `base_layer.py`
- `base_task.py`
- `base_model_params.py`
- `hyperparams.py`
- `model.py`
- `model_registry.py`

**Status:** IN PROGRESS (manual)

### 5.3 `monolith/native_training/**`
**Goal:** Full parity with estimator, data pipeline, distributed runtime, export.

**Subareas:**
- `data/**` (datasets, feature extraction, parsers)
- `runtime/**` (hash tables, parameter sync)
- `model_export/**`
- `distribution_ops.py`, `distributed_ps.py`

**Status:** TODO (not started)

### 5.4 `monolith/utils.py` + helpers
- `path_utils.py`
- `base_runner.py`, `gpu_runner.py`, `tpu_runner.py`

**Status:** IN PROGRESS (manual)

---

## 6) TensorFlow Runtime Integration (Optional, but Required for Full Parity)

**Purpose:** Enable running true TF SavedModel graphs when Candle cannot match.

**Key Requirements:**
- Dynamic `libtensorflow` loading (no vendoring)
- Custom op loading (`TF_LoadLibrary`)
- Signature-based tensor mapping
- Output decoding consistent with Python

**Status:** TODO (tracked separately with sub-plan embedded into this doc)

---

## 7) Mapping Strategy (Python → Rust)

For each Python file:
- Choose a destination Rust crate/module.
- Define equivalent Rust API(s).
- Record any unavoidable deviations.
- Add bridging adapters if required to preserve call patterns.

---

## 8) Test Parity Strategy

- Mirror Python tests in Rust where possible.
- For graph/TF behavior, validate against actual SavedModel outputs.
- For gRPC/proto compatibility, use golden fixtures.
- Maintain a parity test suite that runs Python and Rust against the same inputs.

---

## 9) Validation & Auditing (Triple-Check)

Every milestone must include:
- **Check 1:** File-level parity checklist complete.
- **Check 2:** Integration test across module boundaries.
- **Check 3:** End-to-end run of a full workflow.

We will not declare parity until all three checks pass.

---

## 10) Live Update Protocol (Incremental Updates)

Every time we progress:
- Update this file with:
  - Added files mapped
  - Closed gaps
  - New blockers
- Update `PYTHON_PARITY_INDEX.md` (line inventory)
- Update per-file checklists

**Triple-check ritual for each update:**
1. Verify all files touched are listed in `PYTHON_PARITY_INDEX.md`.
2. Verify any new Rust code has mapped Python source lines.
3. Re-run or re-validate the relevant tests.

---

## 11) Immediate Next Actions (Start Here)

1. ✅ **Generate full Python file inventory** with line counts.
2. ✅ **Create `PYTHON_PARITY_INDEX.md`** with one row per file.
3. ✅ **Create per-file checklists** under `monolith-rs/parity/`.
4. ⏳ **Define module mapping table** (Python → Rust crate/module).
5. ⏳ **Start with `monolith/agent_service`** (serving parity is highest priority).

---

## 12) Triple-Check Log (Do Not Skip)

**Latest verification (2026-01-19):**
- Python files discovered: **334**
- Parity checklist files created: **334**
- Missing checklist files: **0**
- Extra checklist files: **0**

**How to re-check:**
- Re-run the inventory generator and count parity files.
- Ensure `PYTHON_PARITY_INDEX.md` row count equals checklist file count.

---

## 13) Initial Mapping Table (Phase 1)

This is the first incremental mapping pass. It will be expanded **file-by-file** until every Python file has an explicit Rust destination.

### 13.1 `monolith/agent_service/**` → `monolith-rs/crates/monolith-serving`

| Python File | Rust Target | Status | Notes |
|---|---|---|---|
| `monolith/agent_service/agent_service.py` | `monolith-rs/crates/monolith-serving/src/grpc_agent.rs` (gRPC), `monolith-rs/crates/monolith-serving/src/server.rs` (runtime) | IN PROGRESS | Split responsibilities between coordination gRPC and serving runtime. |
| `monolith/agent_service/agent_controller.py` | `monolith-rs/crates/monolith-serving/src/*` | IN PROGRESS | Needs model layout/config orchestration parity. |
| `monolith/agent_service/agent_v1.py` | `monolith-rs/crates/monolith-serving/src/*` | IN PROGRESS | Defines agent logic; map to server + manager abstractions. |
| `monolith/agent_service/agent_v3.py` | `monolith-rs/crates/monolith-serving/src/*` | IN PROGRESS | Advanced routing / layout logic; likely new Rust module. |
| `monolith/agent_service/backends.py` | `monolith-rs/crates/monolith-serving/src/*` | IN PROGRESS | Backend abstraction for layout + storage. |
| `monolith/agent_service/replica_manager.py` | `monolith-rs/crates/monolith-serving/src/*` | IN PROGRESS | Replica discovery + updates. |
| `monolith/agent_service/model_manager.py` | `monolith-rs/crates/monolith-serving/src/*` | IN PROGRESS | Model lifecycle + watcher loops. |
| `monolith/agent_service/tfs_client.py` | `monolith-rs/crates/monolith-serving/src/tfserving.rs` | IN PROGRESS | Map client utilities to Rust TF Serving client. |
| `monolith/agent_service/tfs_wrapper.py` | `monolith-rs/crates/monolith-serving/src/tfserving.rs` | IN PROGRESS | Wrapper logic around Predict/ModelStatus. |
| `monolith/agent_service/tfs_monitor.py` | `monolith-rs/crates/monolith-serving/src/*` | IN PROGRESS | Monitoring + metrics; may need new module. |
| `monolith/agent_service/utils.py` | `monolith-rs/crates/monolith-serving/src/*` | IN PROGRESS | Config parsing, model specs, helper utilities. |
| `monolith/agent_service/constants.py` | `monolith-rs/crates/monolith-serving/src/*` | IN PROGRESS | Constants + enums. |
| `monolith/agent_service/data_def.py` | `monolith-rs/crates/monolith-serving/src/*` | IN PROGRESS | Data definition structures. |
| Tests in `monolith/agent_service/*_test.py` | `monolith-rs/crates/monolith-serving/tests/*` | IN PROGRESS | Port test cases and fixtures. |

---

### 13.2 `monolith/core/**` → `monolith-rs/crates/monolith-core` + `monolith-rs/crates/monolith-layers`

**Status:** TODO (mapping pending)
**Notes:** Use per-file checklists under `monolith-rs/parity/monolith/core/*.md` as the canonical mapping source until this table is populated.

---

### 13.3 `monolith/native_training/**` → `monolith-rs/crates/monolith-training`, `monolith-rs/crates/monolith-data`, `monolith-rs/crates/monolith-checkpoint`, `monolith-rs/crates/monolith-serving`

**Status:** TODO (mapping pending)
**Notes:** Use per-file checklists under `monolith-rs/parity/monolith/native_training/**/*.md` as the canonical mapping source until this table is populated.

---

## 14) Notes / Open Questions

- Which subset of Python behaviors should be prioritized for the first parity milestone?
- Are we targeting full parity for GPU/TPU runtime semantics, or “functional parity” only?
- Should we keep Python code synced into `monolith-rs/python/` or treat it as external?

---

## 15) Appendix: Placeholders for Future Expansion

- **Detailed TF runtime integration steps**
- **Custom op inventory + build steps**
- **Data pipeline micro-parity checklists**
- **CLI command mapping table**
- **ZK/Consul parity matrix**

---

> This document will be expanded with concrete, line-level checklists and mapping tables next. No file is left untracked.

## Line-Level Inventory (All Python Files)

This table enumerates **every** Python file under `monolith/` with line counts and a direct link to its checklist section.

| Python File | Lines | Status | Rust Mapping | Notes |
|---|---:|---|---|---|
| [`monolith/__init__.py`](#monolith-init-py) | 55 | IN PROGRESS | monolith-rs/crates/monolith-core | |
| [`monolith/agent_service/__init__.py`](#monolith-agent-service-init-py) | 0 | IN PROGRESS | monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/agent.py`](#monolith-agent-service-agent-py) | 100 | IN PROGRESS | monolith-rs/crates/monolith-cli, monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/agent_base.py`](#monolith-agent-service-agent-base-py) | 88 | IN PROGRESS | monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/agent_client.py`](#monolith-agent-service-agent-client-py) | 216 | IN PROGRESS | monolith-rs/crates/monolith-cli/src/bin/agent_client.rs | |
| [`monolith/agent_service/agent_controller.py`](#monolith-agent-service-agent-controller-py) | 145 | IN PROGRESS | monolith-rs/crates/monolith-cli/src/bin/agent_controller.rs | |
| [`monolith/agent_service/agent_controller_test.py`](#monolith-agent-service-agent-controller-test-py) | 95 | IN PROGRESS | monolith-rs/crates/monolith-cli/tests | |
| [`monolith/agent_service/agent_service.py`](#monolith-agent-service-agent-service-py) | 155 | IN PROGRESS | monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/agent_service_test.py`](#monolith-agent-service-agent-service-test-py) | 107 | IN PROGRESS | monolith-rs/crates/monolith-serving/tests | |
| [`monolith/agent_service/agent_v1.py`](#monolith-agent-service-agent-v1-py) | 390 | IN PROGRESS | monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/agent_v3.py`](#monolith-agent-service-agent-v3-py) | 210 | IN PROGRESS | monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/agent_v3_test.py`](#monolith-agent-service-agent-v3-test-py) | 114 | IN PROGRESS | monolith-rs/crates/monolith-serving/tests | |
| [`monolith/agent_service/backends.py`](#monolith-agent-service-backends-py) | 518 | IN PROGRESS | monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/backends_test.py`](#monolith-agent-service-backends-test-py) | 134 | IN PROGRESS | monolith-rs/crates/monolith-serving/tests | |
| [`monolith/agent_service/client.py`](#monolith-agent-service-client-py) | 126 | IN PROGRESS | monolith-rs/crates/monolith-cli/src/bin/serving_client.rs | |
| [`monolith/agent_service/constants.py`](#monolith-agent-service-constants-py) | 15 | IN PROGRESS | monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/data_def.py`](#monolith-agent-service-data-def-py) | 171 | IN PROGRESS | monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/data_def_test.py`](#monolith-agent-service-data-def-test-py) | 52 | IN PROGRESS | monolith-rs/crates/monolith-serving/tests | |
| [`monolith/agent_service/mocked_tfserving.py`](#monolith-agent-service-mocked-tfserving-py) | 399 | IN PROGRESS | monolith-rs/crates/monolith-serving/tests/support | |
| [`monolith/agent_service/mocked_tfserving_test.py`](#monolith-agent-service-mocked-tfserving-test-py) | 92 | IN PROGRESS | monolith-rs/crates/monolith-serving/tests | |
| [`monolith/agent_service/mocked_zkclient.py`](#monolith-agent-service-mocked-zkclient-py) | 377 | IN PROGRESS | monolith-rs/crates/monolith-serving/tests/support | |
| [`monolith/agent_service/mocked_zkclient_test.py`](#monolith-agent-service-mocked-zkclient-test-py) | 130 | IN PROGRESS | monolith-rs/crates/monolith-serving/tests | |
| [`monolith/agent_service/model_manager.py`](#monolith-agent-service-model-manager-py) | 371 | IN PROGRESS | monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/model_manager_test.py`](#monolith-agent-service-model-manager-test-py) | 113 | IN PROGRESS | monolith-rs/crates/monolith-serving/tests | |
| [`monolith/agent_service/replica_manager.py`](#monolith-agent-service-replica-manager-py) | 835 | IN PROGRESS | monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/replica_manager_test.py`](#monolith-agent-service-replica-manager-test-py) | 126 | IN PROGRESS | monolith-rs/crates/monolith-serving/tests | |
| [`monolith/agent_service/resource_utils.py`](#monolith-agent-service-resource-utils-py) | 269 | IN PROGRESS | monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/resource_utils_test.py`](#monolith-agent-service-resource-utils-test-py) | 36 | IN PROGRESS | monolith-rs/crates/monolith-serving/tests | |
| [`monolith/agent_service/run.py`](#monolith-agent-service-run-py) | 39 | IN PROGRESS | monolith-rs/crates/monolith-cli/src/bin | |
| [`monolith/agent_service/svr_client.py`](#monolith-agent-service-svr-client-py) | 70 | IN PROGRESS | monolith-rs/crates/monolith-cli/src | |
| [`monolith/agent_service/tfs_client.py`](#monolith-agent-service-tfs-client-py) | 503 | IN PROGRESS | monolith-rs/crates/monolith-cli/src/bin/tfs_client.rs | |
| [`monolith/agent_service/tfs_client_test.py`](#monolith-agent-service-tfs-client-test-py) | 50 | IN PROGRESS | monolith-rs/crates/monolith-cli/tests | |
| [`monolith/agent_service/tfs_monitor.py`](#monolith-agent-service-tfs-monitor-py) | 303 | IN PROGRESS | monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/tfs_monitor_test.py`](#monolith-agent-service-tfs-monitor-test-py) | 182 | IN PROGRESS | monolith-rs/crates/monolith-serving/tests | |
| [`monolith/agent_service/tfs_wrapper.py`](#monolith-agent-service-tfs-wrapper-py) | 202 | IN PROGRESS | monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/utils.py`](#monolith-agent-service-utils-py) | 1167 | IN PROGRESS | monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/utils_test.py`](#monolith-agent-service-utils-test-py) | 170 | IN PROGRESS | monolith-rs/crates/monolith-serving/tests | |
| [`monolith/agent_service/zk_mirror.py`](#monolith-agent-service-zk-mirror-py) | 672 | IN PROGRESS | monolith-rs/crates/monolith-serving/src | |
| [`monolith/agent_service/zk_mirror_test.py`](#monolith-agent-service-zk-mirror-test-py) | 229 | IN PROGRESS | monolith-rs/crates/monolith-serving/tests | |
| [`monolith/base_runner.py`](#monolith-base-runner-py) | 46 | IN PROGRESS | monolith-rs/crates/monolith-training/src | |
| [`monolith/common/python/mem_profiling.py`](#monolith-common-python-mem-profiling-py) | 51 | IN PROGRESS | monolith-rs/crates/monolith-training/src | |
| [`monolith/core/__init__.py`](#monolith-core-init-py) | 0 | IN PROGRESS | monolith-rs/crates/monolith-core/src | |
| [`monolith/core/auto_checkpoint_feed_hook.py`](#monolith-core-auto-checkpoint-feed-hook-py) | 376 | IN PROGRESS | monolith-rs/crates/monolith-tf/src |  |
| [`monolith/core/base_embedding_host_call.py`](#monolith-core-base-embedding-host-call-py) | 643 | IN PROGRESS | monolith-rs/crates/monolith-core/src | |
| [`monolith/core/base_embedding_host_call_test.py`](#monolith-core-base-embedding-host-call-test-py) | 77 | IN PROGRESS | monolith-rs/crates/monolith-core/tests | |
| [`monolith/core/base_embedding_task.py`](#monolith-core-base-embedding-task-py) | 611 | IN PROGRESS | monolith-rs/crates/monolith-core/src | |
| [`monolith/core/base_host_call.py`](#monolith-core-base-host-call-py) | 145 | IN PROGRESS | monolith-rs/crates/monolith-core/src | |
| [`monolith/core/base_layer.py`](#monolith-core-base-layer-py) | 161 | IN PROGRESS | monolith-rs/crates/monolith-core/src | |
| [`monolith/core/base_layer_test.py`](#monolith-core-base-layer-test-py) | 41 | IN PROGRESS | monolith-rs/crates/monolith-core/tests | |
| [`monolith/core/base_model_params.py`](#monolith-core-base-model-params-py) | 25 | IN PROGRESS | monolith-rs/crates/monolith-core/src | |
| [`monolith/core/base_task.py`](#monolith-core-base-task-py) | 95 | IN PROGRESS | monolith-rs/crates/monolith-core/src | |
| [`monolith/core/base_tpu_test.py`](#monolith-core-base-tpu-test-py) | 73 | IN PROGRESS | monolith-rs/crates/monolith-training/tests | |
| [`monolith/core/core_test_suite.py`](#monolith-core-core-test-suite-py) | 35 | IN PROGRESS | monolith-rs/crates/monolith-core/tests | |
| [`monolith/core/dense.py`](#monolith-core-dense-py) | 179 | IN PROGRESS | monolith-rs/crates/monolith-layers/src | |
| [`monolith/core/dense_test.py`](#monolith-core-dense-test-py) | 108 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests | |
| [`monolith/core/feature.py`](#monolith-core-feature-py) | 611 | IN PROGRESS | monolith-rs/crates/monolith-core/src/feature.rs |  |
| [`monolith/core/feature_test.py`](#monolith-core-feature-test-py) | 178 | IN PROGRESS | monolith-rs/crates/monolith-core/tests |  |
| [`monolith/core/host_call.py`](#monolith-core-host-call-py) | 248 | IN PROGRESS | monolith-rs/crates/monolith-core/src | |
| [`monolith/core/hyperparams.py`](#monolith-core-hyperparams-py) | 439 | IN PROGRESS | monolith-rs/crates/monolith-core/src/hyperparams.rs |  |
| [`monolith/core/hyperparams_test.py`](#monolith-core-hyperparams-test-py) | 277 | IN PROGRESS | monolith-rs/crates/monolith-core/tests |  |
| [`monolith/core/mixed_emb_op_comb_nws.py`](#monolith-core-mixed-emb-op-comb-nws-py) | 421 | IN PROGRESS | monolith-rs/crates/monolith-layers/src |  |
| [`monolith/core/model.py`](#monolith-core-model-py) | 320 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/core/model_imports.py`](#monolith-core-model-imports-py) | 104 | IN PROGRESS | monolith-rs/crates/monolith-core/src | |
| [`monolith/core/model_registry.py`](#monolith-core-model-registry-py) | 174 | IN PROGRESS | monolith-rs/crates/monolith-core/src | |
| [`monolith/core/optimizers.py`](#monolith-core-optimizers-py) | 25 | IN PROGRESS | monolith-rs/crates/monolith-optimizer/src | |
| [`monolith/core/py_utils.py`](#monolith-core-py-utils-py) | 313 | IN PROGRESS | monolith-rs/crates/monolith-core/src | |
| [`monolith/core/testing_utils.py`](#monolith-core-testing-utils-py) | 203 | IN PROGRESS | monolith-rs/crates/monolith-core/tests | |
| [`monolith/core/tpu_variable.py`](#monolith-core-tpu-variable-py) | 214 | IN PROGRESS | monolith-rs/crates/monolith-tf/src | |
| [`monolith/core/util.py`](#monolith-core-util-py) | 269 | IN PROGRESS | monolith-rs/crates/monolith-core/src | |
| [`monolith/core/util_test.py`](#monolith-core-util-test-py) | 149 | IN PROGRESS | monolith-rs/crates/monolith-core/tests | |
| [`monolith/core/variance_scaling.py`](#monolith-core-variance-scaling-py) | 188 | IN PROGRESS | monolith-rs/crates/monolith-core/src | |
| [`monolith/gpu_runner.py`](#monolith-gpu-runner-py) | 226 | IN PROGRESS | monolith-rs/crates/monolith-training/src | |
| [`monolith/native_training/alert/alert_manager.py`](#monolith-native-training-alert-alert-manager-py) | 31 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/alert/alert_manager_test.py`](#monolith-native-training-alert-alert-manager-test-py) | 32 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/barrier_ops.py`](#monolith-native-training-barrier-ops-py) | 158 | IN PROGRESS | monolith-rs/crates/monolith-training/src/barrier.rs |  |
| [`monolith/native_training/barrier_ops_test.py`](#monolith-native-training-barrier-ops-test-py) | 104 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/basic_restore_hook.py`](#monolith-native-training-basic-restore-hook-py) | 72 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/basic_restore_hook_test.py`](#monolith-native-training-basic-restore-hook-test-py) | 137 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/clip_ops.py`](#monolith-native-training-clip-ops-py) | 80 | IN PROGRESS | monolith-rs/crates/monolith-optimizer/src |  |
| [`monolith/native_training/clip_ops_test.py`](#monolith-native-training-clip-ops-test-py) | 92 | IN PROGRESS | monolith-rs/crates/monolith-optimizer/tests |  |
| [`monolith/native_training/cluster_manager.py`](#monolith-native-training-cluster-manager-py) | 184 | IN PROGRESS | monolith-rs/crates/monolith-training/src/distributed.rs |  |
| [`monolith/native_training/cluster_manager_test.py`](#monolith-native-training-cluster-manager-test-py) | 35 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/consul.py`](#monolith-native-training-consul-py) | 149 | IN PROGRESS | monolith-rs/crates/monolith-training/src/discovery.rs |  |
| [`monolith/native_training/consul_test.py`](#monolith-native-training-consul-test-py) | 59 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/cpu_sync_training_test.py`](#monolith-native-training-cpu-sync-training-test-py) | 360 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/cpu_training.py`](#monolith-native-training-cpu-training-py) | 2449 | IN PROGRESS | monolith-rs/crates/monolith-training/src/cpu_training.rs (new), monolith-rs/crates/monolith-training/src/distributed.rs (new), monolith-rs/crates/monolith-training/src/local.rs (new) |  |
| [`monolith/native_training/cpu_training_distributed_test_binary.py`](#monolith-native-training-cpu-training-distributed-test-binary-py) | 226 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/cpu_training_test.py`](#monolith-native-training-cpu-training-test-py) | 597 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/data/__init__.py`](#monolith-native-training-data-init-py) | 20 | IN PROGRESS | monolith-rs/crates/monolith-data/src |  |
| [`monolith/native_training/data/data_ops_test.py`](#monolith-native-training-data-data-ops-test-py) | 502 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/data_service_parquet_test.py`](#monolith-native-training-data-data-service-parquet-test-py) | 145 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/data_service_test.py`](#monolith-native-training-data-data-service-test-py) | 98 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/datasets.py`](#monolith-native-training-data-datasets-py) | 1642 | IN PROGRESS | monolith-rs/crates/monolith-data/src |  |
| [`monolith/native_training/data/eager_mode_test.py`](#monolith-native-training-data-eager-mode-test-py) | 186 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/extract_fid_test.py`](#monolith-native-training-data-extract-fid-test-py) | 30 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/feature_list.py`](#monolith-native-training-data-feature-list-py) | 409 | IN PROGRESS | monolith-rs/crates/monolith-data/src/feature_list.rs |  |
| [`monolith/native_training/data/feature_list_test.py`](#monolith-native-training-data-feature-list-test-py) | 0 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/feature_utils.py`](#monolith-native-training-data-feature-utils-py) | 1070 | IN PROGRESS | monolith-rs/crates/monolith-data/src |  |
| [`monolith/native_training/data/feature_utils_test.py`](#monolith-native-training-data-feature-utils-test-py) | 1414 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/item_pool_hook.py`](#monolith-native-training-data-item-pool-hook-py) | 109 | IN PROGRESS | monolith-rs/crates/monolith-data/src |  |
| [`monolith/native_training/data/item_pool_test.py`](#monolith-native-training-data-item-pool-test-py) | 58 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/kafka_dataset_test.py`](#monolith-native-training-data-kafka-dataset-test-py) | 239 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/multi_flow_test.py`](#monolith-native-training-data-multi-flow-test-py) | 125 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/negative_gen_test.py`](#monolith-native-training-data-negative-gen-test-py) | 253 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/parse_sparse_feature_test.py`](#monolith-native-training-data-parse-sparse-feature-test-py) | 1833 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/parsers.py`](#monolith-native-training-data-parsers-py) | 782 | IN PROGRESS | monolith-rs/crates/monolith-data/src |  |
| [`monolith/native_training/data/tf_example_to_example_test.py`](#monolith-native-training-data-tf-example-to-example-test-py) | 183 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/training_instance/python/instance_dataset_op.py`](#monolith-native-training-data-training-instance-python-instance-dataset-op-py) | 166 | IN PROGRESS | monolith-rs/crates/monolith-data/src |  |
| [`monolith/native_training/data/training_instance/python/instance_dataset_op_test_stdin.py`](#monolith-native-training-data-training-instance-python-instance-dataset-op-test-stdin-py) | 58 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/training_instance/python/instance_negative_gen_dataset_op_test.py`](#monolith-native-training-data-training-instance-python-instance-negative-gen-dataset-op-test-py) | 283 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/training_instance/python/parse_instance_ops.py`](#monolith-native-training-data-training-instance-python-parse-instance-ops-py) | 245 | IN PROGRESS | monolith-rs/crates/monolith-data/src |  |
| [`monolith/native_training/data/training_instance/python/parse_instance_ops_test.py`](#monolith-native-training-data-training-instance-python-parse-instance-ops-test-py) | 185 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/training_instance/python/parser_utils.py`](#monolith-native-training-data-training-instance-python-parser-utils-py) | 85 | IN PROGRESS | monolith-rs/crates/monolith-data/src |  |
| [`monolith/native_training/data/training_instance/python/pb_datasource_ops.py`](#monolith-native-training-data-training-instance-python-pb-datasource-ops-py) | 48 | IN PROGRESS | monolith-rs/crates/monolith-data/src |  |
| [`monolith/native_training/data/training_instance/python/test_data_utils.py`](#monolith-native-training-data-training-instance-python-test-data-utils-py) | 15 | IN PROGRESS | none |  |
| [`monolith/native_training/data/transform/transforms.py`](#monolith-native-training-data-transform-transforms-py) | 250 | IN PROGRESS | monolith-rs/crates/monolith-data/src |  |
| [`monolith/native_training/data/transform/transforms_test.py`](#monolith-native-training-data-transform-transforms-test-py) | 70 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/transform_dataset_test.py`](#monolith-native-training-data-transform-dataset-test-py) | 168 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/data/utils.py`](#monolith-native-training-data-utils-py) | 55 | IN PROGRESS | monolith-rs/crates/monolith-data/src |  |
| [`monolith/native_training/debugging/debugging_client.py`](#monolith-native-training-debugging-debugging-client-py) | 98 | IN PROGRESS | monolith-rs/crates/monolith-training/src/debugging |  |
| [`monolith/native_training/debugging/debugging_server.py`](#monolith-native-training-debugging-debugging-server-py) | 217 | IN PROGRESS | monolith-rs/crates/monolith-training/src/debugging |  |
| [`monolith/native_training/demo.py`](#monolith-native-training-demo-py) | 57 | IN PROGRESS | monolith-rs/crates/monolith-training/examples |  |
| [`monolith/native_training/dense_reload_utils.py`](#monolith-native-training-dense-reload-utils-py) | 457 | IN PROGRESS | monolith-rs/crates/monolith-training/src/checkpoint |  |
| [`monolith/native_training/dense_reload_utils_test.py`](#monolith-native-training-dense-reload-utils-test-py) | 192 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/device_utils.py`](#monolith-native-training-device-utils-py) | 231 | IN PROGRESS | monolith-rs/crates/monolith-training/src/device |  |
| [`monolith/native_training/device_utils_test.py`](#monolith-native-training-device-utils-test-py) | 104 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/distribute/distributed_dataset.py`](#monolith-native-training-distribute-distributed-dataset-py) | 81 | IN PROGRESS | monolith-rs/crates/monolith-data/src |  |
| [`monolith/native_training/distribute/distributed_dataset_test.py`](#monolith-native-training-distribute-distributed-dataset-test-py) | 124 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/distribute/str_queue.py`](#monolith-native-training-distribute-str-queue-py) | 114 | IN PROGRESS | monolith-rs/crates/monolith-data/src |  |
| [`monolith/native_training/distribute/str_queue_test.py`](#monolith-native-training-distribute-str-queue-test-py) | 67 | IN PROGRESS | monolith-rs/crates/monolith-data/tests |  |
| [`monolith/native_training/distributed_ps.py`](#monolith-native-training-distributed-ps-py) | 2108 | IN PROGRESS | monolith-rs/crates/monolith-training/src/ps |  |
| [`monolith/native_training/distributed_ps_benchmark.py`](#monolith-native-training-distributed-ps-benchmark-py) | 168 | IN PROGRESS | monolith-rs/crates/monolith-training/benches |  |
| [`monolith/native_training/distributed_ps_factory.py`](#monolith-native-training-distributed-ps-factory-py) | 262 | IN PROGRESS | monolith-rs/crates/monolith-training/src/ps |  |
| [`monolith/native_training/distributed_ps_factory_test.py`](#monolith-native-training-distributed-ps-factory-test-py) | 87 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/distributed_ps_sync.py`](#monolith-native-training-distributed-ps-sync-py) | 531 | IN PROGRESS | monolith-rs/crates/monolith-training/src/ps |  |
| [`monolith/native_training/distributed_ps_sync_test.py`](#monolith-native-training-distributed-ps-sync-test-py) | 109 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/distributed_ps_test.py`](#monolith-native-training-distributed-ps-test-py) | 979 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/distributed_serving_ops.py`](#monolith-native-training-distributed-serving-ops-py) | 160 | IN PROGRESS | monolith-rs/crates/monolith-training/src/serving |  |
| [`monolith/native_training/distributed_serving_ops_test.py`](#monolith-native-training-distributed-serving-ops-test-py) | 142 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/distribution_ops.py`](#monolith-native-training-distribution-ops-py) | 889 | IN PROGRESS | monolith-rs/crates/monolith-training/src/ops |  |
| [`monolith/native_training/distribution_ops_benchmark.py`](#monolith-native-training-distribution-ops-benchmark-py) | 118 | IN PROGRESS | monolith-rs/crates/monolith-training/benches |  |
| [`monolith/native_training/distribution_ops_fused_benchmark.py`](#monolith-native-training-distribution-ops-fused-benchmark-py) | 61 | IN PROGRESS | monolith-rs/crates/monolith-training/benches |  |
| [`monolith/native_training/distribution_ops_fused_test.py`](#monolith-native-training-distribution-ops-fused-test-py) | 148 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/distribution_ops_test.py`](#monolith-native-training-distribution-ops-test-py) | 536 | IN PROGRESS | monolith-rs/crates/monolith-tf/tests |  |
| [`monolith/native_training/distribution_utils.py`](#monolith-native-training-distribution-utils-py) | 443 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/embedding_combiners.py`](#monolith-native-training-embedding-combiners-py) | 102 | IN PROGRESS | monolith-rs/crates/monolith-layers/src |  |
| [`monolith/native_training/embedding_combiners_test.py`](#monolith-native-training-embedding-combiners-test-py) | 47 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests |  |
| [`monolith/native_training/entry.py`](#monolith-native-training-entry-py) | 630 | IN PROGRESS | monolith-rs/crates/monolith-hash-table/src |  |
| [`monolith/native_training/entry_test.py`](#monolith-native-training-entry-test-py) | 84 | IN PROGRESS | monolith-rs/crates/monolith-hash-table/tests |  |
| [`monolith/native_training/env_utils.py`](#monolith-native-training-env-utils-py) | 32 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/env_utils_test.py`](#monolith-native-training-env-utils-test-py) | 23 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/estimator.py`](#monolith-native-training-estimator-py) | 667 | IN PROGRESS | monolith-rs/crates/monolith-training/src/estimator.rs |  |
| [`monolith/native_training/estimator_dist_test.py`](#monolith-native-training-estimator-dist-test-py) | 166 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/estimator_mode_test.py`](#monolith-native-training-estimator-mode-test-py) | 417 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/estimator_test.py`](#monolith-native-training-estimator-test-py) | 112 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/feature.py`](#monolith-native-training-feature-py) | 663 | IN PROGRESS | monolith-rs/crates/monolith-core/src/feature.rs |  |
| [`monolith/native_training/feature_test.py`](#monolith-native-training-feature-test-py) | 266 | IN PROGRESS | monolith-rs/crates/monolith-core/tests |  |
| [`monolith/native_training/feature_utils.py`](#monolith-native-training-feature-utils-py) | 419 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/feature_utils_test.py`](#monolith-native-training-feature-utils-test-py) | 144 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/file_ops.py`](#monolith-native-training-file-ops-py) | 51 | IN PROGRESS | monolith-rs/crates/monolith-training/src/file_ops.rs (new) |  |
| [`monolith/native_training/file_ops_test.py`](#monolith-native-training-file-ops-test-py) | 56 | IN PROGRESS | monolith-rs/crates/monolith-training/tests/file_ops.rs (new) |  |
| [`monolith/native_training/fused_embedding_to_layout_test.py`](#monolith-native-training-fused-embedding-to-layout-test-py) | 1333 | IN PROGRESS | monolith-rs/crates/monolith-training/tests/fused_embedding_to_layout.rs (new) |  |
| [`monolith/native_training/gen_seq_mask.py`](#monolith-native-training-gen-seq-mask-py) | 26 | IN PROGRESS | monolith-rs/crates/monolith-tf/src |  |
| [`monolith/native_training/gen_seq_mask_test.py`](#monolith-native-training-gen-seq-mask-test-py) | 42 | IN PROGRESS | monolith-rs/crates/monolith-tf/tests |  |
| [`monolith/native_training/gflags_utils.py`](#monolith-native-training-gflags-utils-py) | 282 | IN PROGRESS | monolith-rs/crates/monolith-cli/src |  |
| [`monolith/native_training/gflags_utils_test.py`](#monolith-native-training-gflags-utils-test-py) | 217 | IN PROGRESS | monolith-rs/crates/monolith-cli/tests |  |
| [`monolith/native_training/graph_meta.py`](#monolith-native-training-graph-meta-py) | 30 | IN PROGRESS | monolith-rs/crates/monolith-tf/src |  |
| [`monolith/native_training/graph_utils.py`](#monolith-native-training-graph-utils-py) | 26 | IN PROGRESS | monolith-rs/crates/monolith-tf/src |  |
| [`monolith/native_training/hash_filter_ops.py`](#monolith-native-training-hash-filter-ops-py) | 326 | IN PROGRESS | monolith-rs/crates/monolith-tf/src |  |
| [`monolith/native_training/hash_filter_ops_test.py`](#monolith-native-training-hash-filter-ops-test-py) | 228 | IN PROGRESS | monolith-rs/crates/monolith-tf/tests |  |
| [`monolith/native_training/hash_table_ops.py`](#monolith-native-training-hash-table-ops-py) | 738 | IN PROGRESS | monolith-rs/crates/monolith-tf/src |  |
| [`monolith/native_training/hash_table_ops_benchmark.py`](#monolith-native-training-hash-table-ops-benchmark-py) | 148 | IN PROGRESS | monolith-rs/crates/monolith-examples/src/bin/hash_table_ops_benchmark.rs (new) |  |
| [`monolith/native_training/hash_table_ops_test.py`](#monolith-native-training-hash-table-ops-test-py) | 1200 | IN PROGRESS | monolith-rs/crates/monolith-tf/tests |  |
| [`monolith/native_training/hash_table_utils.py`](#monolith-native-training-hash-table-utils-py) | 50 | IN PROGRESS | monolith-rs/crates/monolith-hash-table/src |  |
| [`monolith/native_training/hash_table_utils_test.py`](#monolith-native-training-hash-table-utils-test-py) | 45 | IN PROGRESS | monolith-rs/crates/monolith-hash-table/tests |  |
| [`monolith/native_training/hooks/ckpt_hooks.py`](#monolith-native-training-hooks-ckpt-hooks-py) | 193 | IN PROGRESS | monolith-rs/crates/monolith-training/src/hooks |  |
| [`monolith/native_training/hooks/ckpt_hooks_test.py`](#monolith-native-training-hooks-ckpt-hooks-test-py) | 181 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/hooks/ckpt_info.py`](#monolith-native-training-hooks-ckpt-info-py) | 98 | IN PROGRESS | monolith-rs/crates/monolith-training/src/hooks |  |
| [`monolith/native_training/hooks/ckpt_info_test.py`](#monolith-native-training-hooks-ckpt-info-test-py) | 45 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/hooks/controller_hooks.py`](#monolith-native-training-hooks-controller-hooks-py) | 170 | IN PROGRESS | monolith-rs/crates/monolith-training/src/hooks |  |
| [`monolith/native_training/hooks/controller_hooks_test.py`](#monolith-native-training-hooks-controller-hooks-test-py) | 82 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/hooks/feature_engineering_hooks.py`](#monolith-native-training-hooks-feature-engineering-hooks-py) | 99 | IN PROGRESS | monolith-rs/crates/monolith-training/src/hooks |  |
| [`monolith/native_training/hooks/hook_utils.py`](#monolith-native-training-hooks-hook-utils-py) | 41 | IN PROGRESS | monolith-rs/crates/monolith-training/src/hooks |  |
| [`monolith/native_training/hooks/hook_utils_test.py`](#monolith-native-training-hooks-hook-utils-test-py) | 35 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/hooks/ps_check_hooks.py`](#monolith-native-training-hooks-ps-check-hooks-py) | 97 | IN PROGRESS | monolith-rs/crates/monolith-training/src/hooks |  |
| [`monolith/native_training/hooks/ps_check_hooks_test.py`](#monolith-native-training-hooks-ps-check-hooks-test-py) | 112 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/hooks/server/client_lib.py`](#monolith-native-training-hooks-server-client-lib-py) | 30 | IN PROGRESS | monolith-rs/crates/monolith-training/src/hooks/server |  |
| [`monolith/native_training/hooks/server/constants.py`](#monolith-native-training-hooks-server-constants-py) | 15 | IN PROGRESS | monolith-rs/crates/monolith-training/src/hooks/server |  |
| [`monolith/native_training/hooks/server/server_lib.py`](#monolith-native-training-hooks-server-server-lib-py) | 95 | IN PROGRESS | monolith-rs/crates/monolith-training/src/hooks/server |  |
| [`monolith/native_training/hooks/server/server_lib_test.py`](#monolith-native-training-hooks-server-server-lib-test-py) | 54 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/hooks/session_hooks.py`](#monolith-native-training-hooks-session-hooks-py) | 44 | IN PROGRESS | monolith-rs/crates/monolith-training/src/hooks |  |
| [`monolith/native_training/hooks/session_hooks_test.py`](#monolith-native-training-hooks-session-hooks-test-py) | 33 | IN PROGRESS | monolith-rs/crates/monolith-training/tests |  |
| [`monolith/native_training/hvd_lib.py`](#monolith-native-training-hvd-lib-py) | 65 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/input.py`](#monolith-native-training-input-py) | 45 | IN PROGRESS | monolith-rs/crates/monolith-data/src |  |
| [`monolith/native_training/layers/__init__.py`](#monolith-native-training-layers-init-py) | 46 | IN PROGRESS | monolith-rs/crates/monolith-layers/src |  |
| [`monolith/native_training/layers/add_bias.py`](#monolith-native-training-layers-add-bias-py) | 110 | IN PROGRESS | monolith-rs/crates/monolith-layers/src/add_bias.rs |  |
| [`monolith/native_training/layers/add_bias_test.py`](#monolith-native-training-layers-add-bias-test-py) | 65 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests/add_bias_test.rs |  |
| [`monolith/native_training/layers/advanced_activations.py`](#monolith-native-training-layers-advanced-activations-py) | 217 | IN PROGRESS | monolith-rs/crates/monolith-layers/src/activation.rs |  |
| [`monolith/native_training/layers/advanced_activations_test.py`](#monolith-native-training-layers-advanced-activations-test-py) | 84 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests/advanced_activations_test.rs |  |
| [`monolith/native_training/layers/agru.py`](#monolith-native-training-layers-agru-py) | 295 | IN PROGRESS | monolith-rs/crates/monolith-layers/src/agru.rs |  |
| [`monolith/native_training/layers/agru_test.py`](#monolith-native-training-layers-agru-test-py) | 112 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests/agru_test.rs |  |
| [`monolith/native_training/layers/dense.py`](#monolith-native-training-layers-dense-py) | 307 | IN PROGRESS | monolith-rs/crates/monolith-layers/src/dense.rs |  |
| [`monolith/native_training/layers/dense_test.py`](#monolith-native-training-layers-dense-test-py) | 147 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests/dense_test.rs |  |
| [`monolith/native_training/layers/feature_cross.py`](#monolith-native-training-layers-feature-cross-py) | 805 | IN PROGRESS | monolith-rs/crates/monolith-layers/src/feature_cross.rs |  |
| [`monolith/native_training/layers/feature_cross_test.py`](#monolith-native-training-layers-feature-cross-test-py) | 286 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests/feature_cross_test.rs |  |
| [`monolith/native_training/layers/feature_seq.py`](#monolith-native-training-layers-feature-seq-py) | 361 | IN PROGRESS | monolith-rs/crates/monolith-layers/src |  |
| [`monolith/native_training/layers/feature_seq_test.py`](#monolith-native-training-layers-feature-seq-test-py) | 126 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests/feature_seq_test.rs |  |
| [`monolith/native_training/layers/feature_trans.py`](#monolith-native-training-layers-feature-trans-py) | 340 | IN PROGRESS | monolith-rs/crates/monolith-layers/src/feature_trans.rs |  |
| [`monolith/native_training/layers/feature_trans_test.py`](#monolith-native-training-layers-feature-trans-test-py) | 140 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests/feature_trans_test.rs |  |
| [`monolith/native_training/layers/layer_ops.py`](#monolith-native-training-layers-layer-ops-py) | 131 | IN PROGRESS | monolith-rs/crates/monolith-layers/src |  |
| [`monolith/native_training/layers/layer_ops_test.py`](#monolith-native-training-layers-layer-ops-test-py) | 232 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests/layer_ops_test.rs |  |
| [`monolith/native_training/layers/lhuc.py`](#monolith-native-training-layers-lhuc-py) | 296 | IN PROGRESS | monolith-rs/crates/monolith-layers/src/lhuc.rs |  |
| [`monolith/native_training/layers/lhuc_test.py`](#monolith-native-training-layers-lhuc-test-py) | 73 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests/lhuc_test.rs |  |
| [`monolith/native_training/layers/logit_correction.py`](#monolith-native-training-layers-logit-correction-py) | 88 | IN PROGRESS | monolith-rs/crates/monolith-layers/src/logit_correction.rs |  |
| [`monolith/native_training/layers/logit_correction_test.py`](#monolith-native-training-layers-logit-correction-test-py) | 65 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests/logit_correction_test.rs |  |
| [`monolith/native_training/layers/mlp.py`](#monolith-native-training-layers-mlp-py) | 211 | IN PROGRESS | monolith-rs/crates/monolith-layers/src/mlp.rs |  |
| [`monolith/native_training/layers/mlp_test.py`](#monolith-native-training-layers-mlp-test-py) | 78 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests/mlp_test.rs |  |
| [`monolith/native_training/layers/multi_task.py`](#monolith-native-training-layers-multi-task-py) | 448 | IN PROGRESS | monolith-rs/crates/monolith-layers/src |  |
| [`monolith/native_training/layers/multi_task_test.py`](#monolith-native-training-layers-multi-task-test-py) | 128 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests/multi_task_test.rs |  |
| [`monolith/native_training/layers/norms.py`](#monolith-native-training-layers-norms-py) | 343 | IN PROGRESS | monolith-rs/crates/monolith-layers/src/normalization.rs |  |
| [`monolith/native_training/layers/norms_test.py`](#monolith-native-training-layers-norms-test-py) | 84 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests/norms_test.rs |  |
| [`monolith/native_training/layers/pooling.py`](#monolith-native-training-layers-pooling-py) | 101 | IN PROGRESS | monolith-rs/crates/monolith-layers/src/pooling.rs |  |
| [`monolith/native_training/layers/pooling_test.py`](#monolith-native-training-layers-pooling-test-py) | 141 | IN PROGRESS | monolith-rs/crates/monolith-layers/tests/pooling_test.rs |  |
| [`monolith/native_training/layers/sparse_nas.py`](#monolith-native-training-layers-sparse-nas-py) | 31 | IN PROGRESS | N/A (stub) |  |
| [`monolith/native_training/layers/sparse_nas_test.py`](#monolith-native-training-layers-sparse-nas-test-py) | 23 | IN PROGRESS | N/A (empty test) |  |
| [`monolith/native_training/layers/utils.py`](#monolith-native-training-layers-utils-py) | 159 | IN PROGRESS | monolith-rs/crates/monolith-layers/src/merge.rs |  |
| [`monolith/native_training/learning_rate_functions.py`](#monolith-native-training-learning-rate-functions-py) | 112 | IN PROGRESS | N/A (no Rust schedule yet) |  |
| [`monolith/native_training/learning_rate_functions_test.py`](#monolith-native-training-learning-rate-functions-test-py) | 76 | IN PROGRESS | N/A (no Rust schedule yet) |  |
| [`monolith/native_training/logging_ops.py`](#monolith-native-training-logging-ops-py) | 56 | IN PROGRESS | N/A (TF custom ops) |  |
| [`monolith/native_training/logging_ops_test.py`](#monolith-native-training-logging-ops-test-py) | 57 | IN PROGRESS | N/A (TF custom ops) |  |
| [`monolith/native_training/losses/batch_softmax_loss.py`](#monolith-native-training-losses-batch-softmax-loss-py) | 57 | IN PROGRESS | N/A (no Rust loss yet) |  |
| [`monolith/native_training/losses/batch_softmax_loss_test.py`](#monolith-native-training-losses-batch-softmax-loss-test-py) | 35 | IN PROGRESS | N/A (no Rust loss yet) |  |
| [`monolith/native_training/losses/inbatch_auc_loss.py`](#monolith-native-training-losses-inbatch-auc-loss-py) | 41 | IN PROGRESS | N/A (TF custom op) |  |
| [`monolith/native_training/losses/inbatch_auc_loss_test.py`](#monolith-native-training-losses-inbatch-auc-loss-test-py) | 71 | IN PROGRESS | N/A (TF custom op) |  |
| [`monolith/native_training/losses/ltr_losses.py`](#monolith-native-training-losses-ltr-losses-py) | 1233 | IN PROGRESS | N/A (no Rust ranking losses yet) |  |
| [`monolith/native_training/metric/cli.py`](#monolith-native-training-metric-cli-py) | 28 | IN PROGRESS | N/A (stub) |  |
| [`monolith/native_training/metric/deep_insight_ops.py`](#monolith-native-training-metric-deep-insight-ops-py) | 134 | IN PROGRESS | N/A (TF custom ops) |  |
| [`monolith/native_training/metric/deep_insight_ops_test.py`](#monolith-native-training-metric-deep-insight-ops-test-py) | 33 | IN PROGRESS | N/A (empty test) |  |
| [`monolith/native_training/metric/exit_hook.py`](#monolith-native-training-metric-exit-hook-py) | 48 | IN PROGRESS | N/A (no Rust hook) |  |
| [`monolith/native_training/metric/kafka_utils.py`](#monolith-native-training-metric-kafka-utils-py) | 119 | IN PROGRESS | N/A (no Rust Kafka wrapper) |  |
| [`monolith/native_training/metric/metric_hook.py`](#monolith-native-training-metric-metric-hook-py) | 563 | IN PROGRESS | N/A (TF hooks + Kafka) |  |
| [`monolith/native_training/metric/metric_hook_test.py`](#monolith-native-training-metric-metric-hook-test-py) | 189 | IN PROGRESS | N/A (TF hooks) |  |
| [`monolith/native_training/metric/utils.py`](#monolith-native-training-metric-utils-py) | 104 | IN PROGRESS | N/A (TF custom ops) |  |
| [`monolith/native_training/metric/utils_test.py`](#monolith-native-training-metric-utils-test-py) | 50 | IN PROGRESS | N/A (TF custom ops) |  |
| [`monolith/native_training/mlp_utils.py`](#monolith-native-training-mlp-utils-py) | 444 | IN PROGRESS | N/A (TF distributed runtime) |  |
| [`monolith/native_training/model.py`](#monolith-native-training-model-py) | 182 | IN PROGRESS | N/A (test model) |  |
| [`monolith/native_training/model_comp_test.py`](#monolith-native-training-model-comp-test-py) | 183 | IN PROGRESS | N/A (TF/Horovod test) |  |
| [`monolith/native_training/model_dump/dump_utils.py`](#monolith-native-training-model-dump-dump-utils-py) | 757 | IN PROGRESS | N/A (TF model dump) |  |
| [`monolith/native_training/model_dump/graph_utils.py`](#monolith-native-training-model-dump-graph-utils-py) | 845 | IN PROGRESS | N/A (TF graph utils) |  |
| [`monolith/native_training/model_dump/graph_utils_test.py`](#monolith-native-training-model-dump-graph-utils-test-py) | 86 | IN PROGRESS | N/A (TF graph utils) |  |
| [`monolith/native_training/model_export/__init__.py`](#monolith-native-training-model-export-init-py) | 22 | IN PROGRESS | N/A (module alias) |  |
| [`monolith/native_training/model_export/data_gen_utils.py`](#monolith-native-training-model-export-data-gen-utils-py) | 732 | IN PROGRESS | N/A (data generator) |  |
| [`monolith/native_training/model_export/data_gen_utils_test.py`](#monolith-native-training-model-export-data-gen-utils-test-py) | 0 | IN PROGRESS | N/A (no tests) |  |
| [`monolith/native_training/model_export/demo_export.py`](#monolith-native-training-model-export-demo-export-py) | 100 | IN PROGRESS | N/A (demo exporter) |  |
| [`monolith/native_training/model_export/demo_export_test.py`](#monolith-native-training-model-export-demo-export-test-py) | 48 | IN PROGRESS | N/A (TF export test) |  |
| [`monolith/native_training/model_export/demo_predictor.py`](#monolith-native-training-model-export-demo-predictor-py) | 110 | IN PROGRESS | N/A (demo predictor) |  |
| [`monolith/native_training/model_export/demo_predictor_client.py`](#monolith-native-training-model-export-demo-predictor-client-py) | 93 | IN PROGRESS | N/A (demo gRPC client) |  |
| [`monolith/native_training/model_export/export_context.py`](#monolith-native-training-model-export-export-context-py) | 141 | IN PROGRESS | N/A (export context) |  |
| [`monolith/native_training/model_export/export_hooks.py`](#monolith-native-training-model-export-export-hooks-py) | 137 | IN PROGRESS | N/A (TF export hook) |  |
| [`monolith/native_training/model_export/export_hooks_test.py`](#monolith-native-training-model-export-export-hooks-test-py) | 141 | IN PROGRESS | N/A (export hook test) |  |
| [`monolith/native_training/model_export/export_state_utils.py`](#monolith-native-training-model-export-export-state-utils-py) | 46 | IN PROGRESS | N/A (export state) |  |
| [`monolith/native_training/model_export/export_state_utils_test.py`](#monolith-native-training-model-export-export-state-utils-test-py) | 36 | IN PROGRESS | N/A (export state test) |  |
| [`monolith/native_training/model_export/export_utils.py`](#monolith-native-training-model-export-export-utils-py) | 98 | IN PROGRESS | N/A (remote predict helper) |  |
| [`monolith/native_training/model_export/export_utils_test.py`](#monolith-native-training-model-export-export-utils-test-py) | 43 | IN PROGRESS | N/A (remote predict test) |  |
| [`monolith/native_training/model_export/saved_model_exporters.py`](#monolith-native-training-model-export-saved-model-exporters-py) | 739 | IN PROGRESS | N/A (SavedModel exporters) |  |
| [`monolith/native_training/model_export/saved_model_exporters_test.py`](#monolith-native-training-model-export-saved-model-exporters-test-py) | 153 | IN PROGRESS | N/A (exporter tests) |  |
| [`monolith/native_training/model_export/saved_model_visulizer.py`](#monolith-native-training-model-export-saved-model-visulizer-py) | 89 | IN PROGRESS | N/A (tensorboard visualizer) |  |
| [`monolith/native_training/model_export/warmup_data_decoder.py`](#monolith-native-training-model-export-warmup-data-decoder-py) | 55 | IN PROGRESS | N/A (warmup decoder) |  |
| [`monolith/native_training/model_export/warmup_data_gen.py`](#monolith-native-training-model-export-warmup-data-gen-py) | 253 | IN PROGRESS | N/A (warmup generator) |  |
| [`monolith/native_training/model_export/warmup_example_batch.py`](#monolith-native-training-model-export-warmup-example-batch-py) | 57 | IN PROGRESS | N/A (warmup example batch) |  |
| [`monolith/native_training/monolith_export.py`](#monolith-native-training-monolith-export-py) | 18 | IN PROGRESS | N/A (decorator) |  |
| [`monolith/native_training/multi_hash_table_ops.py`](#monolith-native-training-multi-hash-table-ops-py) | 695 | IN PROGRESS | N/A (TF custom ops) |  |
| [`monolith/native_training/multi_hash_table_ops_test.py`](#monolith-native-training-multi-hash-table-ops-test-py) | 249 | IN PROGRESS | N/A (TF custom ops) |  |
| [`monolith/native_training/multi_type_hash_table.py`](#monolith-native-training-multi-type-hash-table-py) | 435 | IN PROGRESS | N/A (hash table wrapper) |  |
| [`monolith/native_training/multi_type_hash_table_test.py`](#monolith-native-training-multi-type-hash-table-test-py) | 326 | IN PROGRESS | N/A (hash table tests) |  |
| [`monolith/native_training/native_model.py`](#monolith-native-training-native-model-py) | 1109 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/native_task.py`](#monolith-native-training-native-task-py) | 213 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/native_task_context.py`](#monolith-native-training-native-task-context-py) | 58 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/nested_tensors.py`](#monolith-native-training-nested-tensors-py) | 110 | IN PROGRESS | monolith-rs/crates/monolith-tensor/src |  |
| [`monolith/native_training/nested_tensors_test.py`](#monolith-native-training-nested-tensors-test-py) | 57 | IN PROGRESS | monolith-rs/crates/monolith-tensor/src |  |
| [`monolith/native_training/net_utils.py`](#monolith-native-training-net-utils-py) | 133 | IN PROGRESS | monolith-rs/crates/monolith-core/src |  |
| [`monolith/native_training/net_utils_test.py`](#monolith-native-training-net-utils-test-py) | 94 | IN PROGRESS | monolith-rs/crates/monolith-core/src |  |
| [`monolith/native_training/optimizers/adamom.py`](#monolith-native-training-optimizers-adamom-py) | 68 | IN PROGRESS | monolith-rs/crates/monolith-optimizer/src |  |
| [`monolith/native_training/optimizers/adamom_test.py`](#monolith-native-training-optimizers-adamom-test-py) | 57 | IN PROGRESS | monolith-rs/crates/monolith-optimizer/src |  |
| [`monolith/native_training/optimizers/rmsprop.py`](#monolith-native-training-optimizers-rmsprop-py) | 102 | IN PROGRESS | monolith-rs/crates/monolith-optimizer/src |  |
| [`monolith/native_training/optimizers/rmsprop_test.py`](#monolith-native-training-optimizers-rmsprop-test-py) | 77 | IN PROGRESS | monolith-rs/crates/monolith-optimizer/src |  |
| [`monolith/native_training/optimizers/rmspropv2_test.py`](#monolith-native-training-optimizers-rmspropv2-test-py) | 112 | IN PROGRESS | monolith-rs/crates/monolith-optimizer/src |  |
| [`monolith/native_training/optimizers/shampoo.py`](#monolith-native-training-optimizers-shampoo-py) | 207 | IN PROGRESS | monolith-rs/crates/monolith-optimizer/src |  |
| [`monolith/native_training/prefetch_queue.py`](#monolith-native-training-prefetch-queue-py) | 379 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/prefetch_queue_test.py`](#monolith-native-training-prefetch-queue-test-py) | 305 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/ps_benchmark.py`](#monolith-native-training-ps-benchmark-py) | 273 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/ps_benchmark_test.py`](#monolith-native-training-ps-benchmark-test-py) | 57 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/ragged_utils.py`](#monolith-native-training-ragged-utils-py) | 29 | IN PROGRESS | monolith-rs/crates/monolith-tensor/src |  |
| [`monolith/native_training/ragged_utils_test.py`](#monolith-native-training-ragged-utils-test-py) | 32 | IN PROGRESS | monolith-rs/crates/monolith-tensor/src |  |
| [`monolith/native_training/remote_predict_ops.py`](#monolith-native-training-remote-predict-ops-py) | 0 | IN PROGRESS | N/A (empty stub) |  |
| [`monolith/native_training/restore_test.py`](#monolith-native-training-restore-test-py) | 240 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/runner_utils.py`](#monolith-native-training-runner-utils-py) | 396 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/runner_utils_test.py`](#monolith-native-training-runner-utils-test-py) | 108 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/runtime/ops/gen_monolith_ops.py`](#monolith-native-training-runtime-ops-gen-monolith-ops-py) | 23 | IN PROGRESS | monolith-rs/crates/monolith-tf/src |  |
| [`monolith/native_training/save_utils.py`](#monolith-native-training-save-utils-py) | 1309 | IN PROGRESS | monolith-rs/crates/monolith-checkpoint/src |  |
| [`monolith/native_training/save_utils_test.py`](#monolith-native-training-save-utils-test-py) | 1740 | IN PROGRESS | monolith-rs/crates/monolith-checkpoint/src |  |
| [`monolith/native_training/service_discovery.py`](#monolith-native-training-service-discovery-py) | 481 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/service_discovery_test.py`](#monolith-native-training-service-discovery-test-py) | 407 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/serving_ps_test.py`](#monolith-native-training-serving-ps-test-py) | 231 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/session_run_hooks.py`](#monolith-native-training-session-run-hooks-py) | 171 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/session_run_hooks_test.py`](#monolith-native-training-session-run-hooks-test-py) | 144 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/signal_utils.py`](#monolith-native-training-signal-utils-py) | 37 | IN PROGRESS | monolith-rs/crates/monolith-core/src |  |
| [`monolith/native_training/signal_utils_test.py`](#monolith-native-training-signal-utils-test-py) | 30 | IN PROGRESS | monolith-rs/crates/monolith-core/src |  |
| [`monolith/native_training/static_reshape_op.py`](#monolith-native-training-static-reshape-op-py) | 58 | IN PROGRESS | monolith-rs/crates/monolith-tf/src |  |
| [`monolith/native_training/static_reshape_op_test.py`](#monolith-native-training-static-reshape-op-test-py) | 79 | IN PROGRESS | monolith-rs/crates/monolith-tf/src |  |
| [`monolith/native_training/summary/summary_ops.py`](#monolith-native-training-summary-summary-ops-py) | 78 | IN PROGRESS | monolith-rs/crates/monolith-tf/src |  |
| [`monolith/native_training/summary/summary_ops_test.py`](#monolith-native-training-summary-summary-ops-test-py) | 122 | IN PROGRESS | monolith-rs/crates/monolith-tf/src |  |
| [`monolith/native_training/summary/utils.py`](#monolith-native-training-summary-utils-py) | 114 | IN PROGRESS | monolith-rs/crates/monolith-tf/src |  |
| [`monolith/native_training/summary/utils_test.py`](#monolith-native-training-summary-utils-test-py) | 43 | IN PROGRESS | monolith-rs/crates/monolith-tf/src |  |
| [`monolith/native_training/sync_hooks.py`](#monolith-native-training-sync-hooks-py) | 176 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/sync_hooks_test.py`](#monolith-native-training-sync-hooks-test-py) | 119 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/sync_training_hooks.py`](#monolith-native-training-sync-training-hooks-py) | 355 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/sync_training_hooks_test.py`](#monolith-native-training-sync-training-hooks-test-py) | 92 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/tensor_utils.py`](#monolith-native-training-tensor-utils-py) | 162 | IN PROGRESS | monolith-rs/crates/monolith-tensor/src |  |
| [`monolith/native_training/tensor_utils_test.py`](#monolith-native-training-tensor-utils-test-py) | 175 | IN PROGRESS | monolith-rs/crates/monolith-tensor/src |  |
| [`monolith/native_training/test_utils.py`](#monolith-native-training-test-utils-py) | 65 | IN PROGRESS | monolith-rs/crates/monolith-training/src |  |
| [`monolith/native_training/touched_key_set_ops.py`](#monolith-native-training-touched-key-set-ops-py) | 61 | IN PROGRESS | monolith-rs/crates/monolith-hash-table/src |  |
| [`monolith/native_training/touched_key_set_ops_test.py`](#monolith-native-training-touched-key-set-ops-test-py) | 51 | IN PROGRESS | monolith-rs/crates/monolith-hash-table/src |  |
| [`monolith/native_training/utils.py`](#monolith-native-training-utils-py) | 320 | IN PROGRESS | monolith-rs/crates/monolith-training/src/utils.rs (new) |  |
| [`monolith/native_training/utils_test.py`](#monolith-native-training-utils-test-py) | 70 | IN PROGRESS | monolith-rs/crates/monolith-training/tests/utils.rs (new) |  |
| [`monolith/native_training/variables.py`](#monolith-native-training-variables-py) | 147 | IN PROGRESS | monolith-rs/crates/monolith-training/src/variables.rs (new) |  |
| [`monolith/native_training/variables_test.py`](#monolith-native-training-variables-test-py) | 89 | IN PROGRESS | monolith-rs/crates/monolith-training/tests/variables.rs (new) |  |
| [`monolith/native_training/yarn_runtime.py`](#monolith-native-training-yarn-runtime-py) | 127 | IN PROGRESS | monolith-rs/crates/monolith-training/src/yarn_runtime.rs (new) |  |
| [`monolith/native_training/yarn_runtime_test.py`](#monolith-native-training-yarn-runtime-test-py) | 133 | IN PROGRESS | monolith-rs/crates/monolith-training/tests/yarn_runtime.rs (new) |  |
| [`monolith/native_training/zk_utils.py`](#monolith-native-training-zk-utils-py) | 96 | IN PROGRESS | monolith-rs/crates/monolith-training/src/zk_utils.rs (new) |  |
| [`monolith/path_utils.py`](#monolith-path-utils-py) | 47 | IN PROGRESS | monolith-rs/crates/monolith-core/src | |
| [`monolith/tpu_runner.py`](#monolith-tpu-runner-py) | 429 | IN PROGRESS | monolith-rs/crates/monolith-training/src | |
| [`monolith/utils.py`](#monolith-utils-py) | 81 | IN PROGRESS | monolith-rs/crates/monolith-tf/src | |
| [`monolith/utils_test.py`](#monolith-utils-test-py) | 65 | IN PROGRESS | monolith-rs/crates/monolith-tf/tests | |

## Per-File Parity Checklists (All Python Files)

Every file listed below must be fully mapped to Rust with parity behavior verified.

### `monolith/__init__.py`
<a id="monolith-init-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 55
- Purpose/role: Package bootstrap that re-exports key submodules and enables TensorFlow monkey patching.
- Key symbols/classes/functions: `add_module`, side-effect imports of `data`, `layers`, `model_export`, `entry`, `native_model` (as `base_model`), `estimator`.
- External dependencies: `tensorflow.python.tools.module_util` (imported), `absl.logging`, `importlib`, `monolith.utils.enable_monkey_patch`.
- Side effects: imports training modules on import; injects modules into `sys.modules`; may modify TensorFlow monitored_session behavior.

**Required Behavior (Detailed)**
- `add_module(module)`:
  - Accepts a module object or a module string.
  - If a string, imports it; derives `name` from the last path component.
  - If `name == 'native_model'`, rename to `'base_model'`.
  - Registers module in `sys.modules` under `monolith.<name>`.
- On import:
  - Calls `add_module` for `data`, `layers`, `model_export`, `entry`, `native_model` (as `base_model`), `estimator`.
  - Calls `enable_monkey_patch()`; on exception, logs `enable_monkey_patch failed`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/lib.rs` (crate-level re-exports), plus `monolith-rs/crates/monolith-training` for training APIs.
- Rust public API surface: `pub use` re-exports of subcrates/modules to mimic `monolith.*` namespace.
- Data model mapping: N/A (module wiring only).

**Implementation Steps (Detailed)**
1. Define a top-level Rust crate that re-exports equivalent submodules (data/layers/training/export/estimator).
2. Ensure any TF monkey patch logic is represented (or explicitly documented as unsupported) in Rust.
3. Avoid heavy side effects on import; if unavoidable, document and feature-gate.

**Tests (Detailed)**
- Python tests: covered indirectly by `monolith/utils_test.py` monkey patch test.
- Rust tests: add a smoke test that `monolith` re-exports are accessible.

**Gaps / Notes**
- Python import side effects are heavy; Rust should provide explicit init if needed.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/file_ops.py`
<a id="monolith-native-training-file-ops-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 51
- Purpose/role: TensorFlow custom-op wrappers for writing to files inside graph execution; provides a close hook.
- Key symbols/classes/functions: `WritableFile`, `FileCloseHook`.
- External dependencies: TensorFlow, `gen_monolith_ops` (custom ops).
- Side effects: Creates/updates files on disk via custom ops.

**Required Behavior (Detailed)**
- `WritableFile.__init__(filename)`:
  - Calls `monolith_writable_file(filename)` and stores a resource handle.
- `WritableFile.append(content)`:
  - Appends a 0-D string tensor to the file (via `monolith_writable_file_append`).
- `WritableFile.append_entry_dump(item_id, bias, embedding)`:
  - Calls `monolith_entry_dump_file_append(handle, item_id, bias, embedding)`.
  - Used for embedding entry dump format (custom op handles encoding).
- `WritableFile.close()`:
  - Calls `monolith_writable_file_close(handle)`.
- `FileCloseHook(files)`:
  - `files` must be a `list` of `WritableFile`.
  - Builds `_close_ops = [f.close() for f in files]`.
  - `end(session)` runs all close ops.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/file_ops.rs` (new).
- Rust public API surface:
  - `struct WritableFile { handle: ... }`
  - `impl WritableFile { fn new(path: &str) -> Self; fn append(&self, s: &str) -> Op; fn append_entry_dump(...); fn close(&self) -> Op }`
  - `struct FileCloseHook` implementing a hook/callback trait.
- Data model mapping:
  - TF custom ops ↔ Rust TF runtime wrapper (if using TF backend).
  - Native backend may use direct file I/O instead of graph ops.
- Feature gating:
  - TF runtime required for graph-op parity; native mode may use std::fs writes.
- Integration points:
  - Used by summary/export logic that writes per-worker outputs.

**Implementation Steps (Detailed)**
1. Add Rust wrappers for the custom file ops (or a native file writer in non-TF mode).
2. Ensure `append` supports scalar string tensors and preserves order.
3. Implement `append_entry_dump` with the same encoding as the TF op.
4. Provide a hook that closes files at session end.

**Tests (Detailed)**
- Python tests: `monolith/native_training/file_ops_test.py`.
- Rust tests:
  - `writable_file_basic` (append 1000 times and verify content).
  - `file_close_hook_runs` (MonitoredSession equivalent if TF backend).
- Cross-language parity test:
  - Compare output file contents after a fixed sequence of appends.

**Gaps / Notes**
- `WritableFile.append` assumes a scalar string tensor; no Python-side validation.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/file_ops_test.py`
<a id="monolith-native-training-file-ops-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 56
- Purpose/role: Tests `WritableFile` append behavior and `FileCloseHook` closure semantics.
- Key symbols/classes/functions: `WritableFileTest.test_basic`, `WritableFileTest.test_hook`.
- External dependencies: TensorFlow, `file_ops`, `tf.io.gfile`.
- Side effects: Writes files under `TEST_TMPDIR`.

**Required Behavior (Detailed)**
- `test_basic`:
  - Create `WritableFile`, append `"1234"` in a `tf.function` loop `times=1000`, close.
  - Verify file content equals `"1234" * times`.
- `test_hook`:
  - Create `WritableFile`, run append in a `MonitoredSession` with `FileCloseHook`.
  - Verify file content equals `"1234"`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/file_ops.rs` (new).
- Rust public API surface: `WritableFile`, `FileCloseHook`.
- Data model mapping: TF runtime session + ops (TF backend).
- Feature gating: tests require TF backend custom ops.
- Integration points: file ops wrapper + hook system.

**Implementation Steps (Detailed)**
1. Port `test_basic` using Rust TF runtime or native file writer.
2. Port `test_hook` with a session hook that triggers close at end.
3. Ensure file output matches exact concatenation ordering.

**Tests (Detailed)**
- Python tests: `WritableFileTest.test_basic`, `.test_hook`.
- Rust tests:
  - `writable_file_basic`
  - `writable_file_close_hook`
- Cross-language parity test:
  - Compare file bytes for identical append sequences.

**Gaps / Notes**
- Tests rely on `TEST_TMPDIR` env var; Rust tests should mirror temp dir handling.

### `monolith/native_training/fused_embedding_to_layout_test.py`
<a id="monolith-native-training-fused-embedding-to-layout-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 1333
- Purpose/role: Validates `distribution_ops.fused_embedding_to_layout` output and gradients across multiple op versions, sharding modes, and example/example_batch variants.
- Key symbols/classes/functions: `infer_shape`, `pooling`, `FusedEmbeddingToLayoutTest`, `FusedEmbeddingToLayoutFitPreTest`.
- External dependencies: TensorFlow, `distribution_ops`, `parse_examples`, `sharding_sparse_fids`, Example/ExampleBatch protos.
- Side effects: Disables TF v2 behavior, sets RNG seed, logs debug output.

**Required Behavior (Detailed)**
- Helpers:
  - `infer_shape(out_conf, out_type, max_sequence_length=0)`:
    - Populates `out_conf.shape` for `OutType.NONE/CONCAT/STACK/ADDN`.
    - Uses `[-1, ...]` batch dimension, optionally `[max_sequence_length]`.
  - `get_key(ln, sc)` returns `"{layout}_{feature_name}_{start}_{end}"`.
  - `pooling(pooling_type, in_data, max_length)`:
    - `SUM`: elementwise sum.
    - `MEAN`: elementwise mean.
    - Else: returns padded/truncated sequence matrix of shape `(max_length, dim)`.
- `FusedEmbeddingToLayoutTest`:
  - `get_feature_cfg` builds per-feature/table metadata and indices (sorted by table/feature names).
  - `test_fused_embedding_to_layout(...)`:
    - Builds `FeatureConfigs` with multiple tables, slice configs, pooling types, and output configs (`bias`, `vec`, `ffm1`, `ffm2`, `firstN`).
    - Generates random fids per slot and expected pooled outputs.
    - Builds offsets (`fid_offset_list`, `feature_offset_list`, `nfl_offset_list`) and embedding lists per PS/table.
    - If `shard_op_version` provided, uses `parse_examples` + `sharding_sparse_fids` to generate offsets.
    - Calls `distribution_ops.fused_embedding_to_layout` with `version` and `parallel_flag`, optionally GPU.
    - Asserts output tensors match numpy-truth via `np.allclose`.
  - Variants:
    - `test_fused_embedding_to_layout_use_shard_op` (version 2).
    - `test_fused_embedding_to_layout_use_shard_op3(_gpu)` (version 3, GPU optional).
    - `test_fused_embedding_to_layout_use_shard_op4(_gpu)` (version 4, GPU optional).
    - `test_fused_embedding_to_layout_parallel` (parallel_flag=0).
  - `test_fused_embedding_to_layout_grad(...)`:
    - Constructs inputs similarly, computes `tf.gradients` of layouts w.r.t embeddings.
    - Builds “truth” counts for each fid (SUM/MEAN/FIRSTN handling).
    - For version 4, splits gradients to per-table/per-ps ordering.
    - Asserts grads are uniform per fid and equal to expected count.
    - Wrapper tests mirror the output tests for shard op versions and GPU.
- `FusedEmbeddingToLayoutFitPreTest`:
  - `test_fused_embedding_to_layout`:
    - Uses `ExampleBatch` with shared/individual features; sets `SHARD_BIT` for shared.
    - Builds offsets, embeddings, and expected outputs for `bias/vec/ffm1/ffm2`.
    - Calls `fused_embedding_to_layout` version 1 (variant_type `example_batch`) and compares outputs.
  - `test_fused_embedding_to_layout_grad`:
    - Smaller gradient test (version 1) on `Example` input; verifies gradient counts.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/fused_embedding_to_layout.rs` (new).
- Rust public API surface: `distribution_ops::fused_embedding_to_layout` equivalent + gradient support.
- Data model mapping:
  - Example/ExampleBatch proto parsing + fid offsets.
  - Output layout configs (`FeatureConfigs`, `OutConfig`, `SliceConfig`).
- Feature gating:
  - GPU tests under a `cuda`/GPU feature.
  - Sharding op versions (2/3/4) require corresponding Rust implementations.
- Integration points:
  - `distribution_ops`, `parser_utils`, and fused layout feature pipeline.

**Implementation Steps (Detailed)**
1. Implement fused layout op in Rust with version semantics (1–4) and `parallel_flag`.
2. Port sharding_sparse_fids logic for versions 2/3/4 so test inputs can be generated.
3. Implement pooling semantics and expected-shape inference for test verification.
4. Add gradient checks (per-fid uniform gradient, equals usage counts).
5. Provide GPU test path for versions >=3.

**Tests (Detailed)**
- Python tests: this file (multiple output + gradient tests).
- Rust tests:
  - `fused_embedding_to_layout_v1_example_batch`
  - `fused_embedding_to_layout_v2_v3_v4` (CPU/GPU)
  - `fused_embedding_to_layout_grad` variants
- Cross-language parity test:
  - Generate identical inputs in Python and Rust and compare layout outputs + grads.

**Gaps / Notes**
- For `op_version >= 3` without `shard_op_version`, the test raises `TypeError('Not imple')`.
- GPU tests require `test_util.use_gpu()` and only run for versions 3/4.
- Uses global RNG seeds (`np.random.seed(2)` and `random.randint`), so determinism depends on Python RNG behavior.

### `monolith/agent_service/__init__.py`
<a id="monolith-agent-service-init-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 0
- Purpose/role: Empty package initializer for `monolith.agent_service`.
- Key symbols/classes/functions: none
- External dependencies: none
- Side effects: none

**Required Behavior (Detailed)**
- Module is intentionally empty; importing it must have no side effects.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/src/lib.rs` (module boundary only).
- Rust public API surface: none required.
- Data model mapping: none.

**Implementation Steps (Detailed)**
1. Ensure the Rust crate module layout mirrors the Python package (no runtime behavior needed).

**Tests (Detailed)**
- Python tests: none
- Rust tests: none (module boundary only)

**Gaps / Notes**
- None; this file is a no-op.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/agent.py`
<a id="monolith-agent-service-agent-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 96
- Purpose/role: Main entrypoint for running agent processes; selects AgentV1/AgentV3 and manages dense multi-process mode.
- Key symbols/classes/functions: `run_agent`, `main`
- External dependencies: `absl`, `multiprocessing`, `subprocess`, `signal`, `ModelManager`, `AgentV1`, `AgentV3`
- Side effects: env var mutations, process spawning, logging, OS signals.

**Required Behavior (Detailed)**
- `run_agent(conf_path, tfs_log, use_mps, replica_id, dense_service_index)`:
  - Mutates `REPLICA_ID` and `DENSE_SERVICE_IDX` when `use_mps` is true.
  - Loads `AgentConfig` from file and instantiates AgentV1 or AgentV3 based on `agent_version`.
  - Starts `ModelManager` for rough sort model; terminates self on failure.
- `main()`:
  - Initializes HDFS env via `env_utils.setup_hdfs_env()`.
  - Loads AgentConfig, handles `DeployType.DENSE` + `dense_service_num > 1` by spawning multiple processes.
  - Otherwise runs single agent.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-cli` or `monolith-rs/crates/monolith-serving/src/bin/*` (new binary).
- Rust public API surface: CLI command mirroring flags `--conf` and `--tfs_log`.
- Integration points: use Rust `AgentConfig` loader, start AgentV1/V3 ported classes.

**Implementation Steps (Detailed)**
1. Add Rust CLI command equivalent to Python `agent.py` entrypoint.
2. Implement env var behavior for `REPLICA_ID` and `DENSE_SERVICE_IDX`.
3. Add multiprocessing behavior for dense service count.
4. Port `ModelManager` startup and failure semantics.
5. Wire into AgentV1/AgentV3 implementations.

**Tests (Detailed)**
- Python tests: none specific.
- Rust tests: add integration test to verify process launch plan and env var behavior.

**Gaps / Notes**
- Rust currently lacks equivalent entrypoint for agent daemonization.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/agent_base.py`
<a id="monolith-agent-service-agent-base-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 86
- Purpose/role: Agent base class + helpers for launching TFS/proxy binaries and log redirection.
- Key symbols/classes/functions: `get_cmd_path`, `get_cmd_and_port`, `ServingLog`, `AgentBase`
- External dependencies: `os`, `abc`, `AgentConfig`, `TFS_HOME`, `TFSServerType`
- Side effects: resolves binary paths, builds shell commands, changes CWD in context manager.

**Required Behavior (Detailed)**
- Constants:
  - `TFS_BINARY = {TFS_HOME}/bin/tensorflow_model_server`.
  - `PROXY_BINARY = {TFS_HOME}/bin/server`.
- `get_cmd_path()` returns `os.path.abspath(__file__)`.
- `get_cmd_and_port(config, conf_path=None, server_type=None, config_file=None, tfs_binary=TFS_BINARY, proxy_binary=PROXY_BINARY)`:
  - For `server_type` in `{PS, ENTRY, DENSE}`: delegates to `config.get_cmd_and_port(tfs_binary, server_type=..., config_file=...)`.
  - Else (proxy): uses `{conf_path}/proxy.conf` if present.
    - Command string: `{proxy_binary} --port={config.proxy_port} --grpc_target=localhost:{config.tfs_entry_port} [--conf_file=proxy.conf] &`.
    - Returns `(cmd, config.proxy_port)`.
- `ServingLog` context manager:
  - On enter: builds `log_filename = dirname(tfs_log)/{log_prefix}_{basename(tfs_log)}`.
  - Saves current cwd, `chdir` to `{TFS_HOME}/bin`, returns `open(log_filename, 'a')`.
  - On exit: `chdir` back to previous cwd (does not close file handle itself).
- `AgentBase`:
  - Stores `config` and defines abstract `start()` and `wait_for_termination()`.

**Rust Mapping (Detailed)**
- Target crate/module: new module in `monolith-rs/crates/monolith-serving/src/agent_base.rs` (or similar).
- Rust public API surface: `AgentBase` trait, `get_cmd_and_port`, `ServingLog` equivalent.
- Data model mapping: `AgentConfig` in Rust config module.

**Implementation Steps (Detailed)**
1. Port command construction rules exactly (including proxy.conf behavior).
2. Add Rust CWD + log file handling in a scoped guard.
3. Define a trait for Agent lifecycle parity (`start`, `wait_for_termination`).
4. Ensure paths match Python defaults (`TFS_HOME`, binary names).

**Tests (Detailed)**
- Python tests: none specific.
- Rust tests: add unit tests for command generation + log filename behavior.

**Gaps / Notes**
- Rust currently lacks explicit AgentBase and ServingLog helpers.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/agent_client.py`
<a id="monolith-agent-service-agent-client-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 216
- Purpose/role: CLI for AgentService + ZooKeeper operations (heartbeat, replica lookup, publish/load/unload, resource and portal inspection).
- Key symbols/classes/functions: `main`
- External dependencies: `grpc`, `kazoo`, `monolith.agent_service.*`, `monolith.native_training.env_utils`
- Side effects: gRPC calls, ZooKeeper reads/writes/deletes, prints to stdout, reads env vars.

**Required Behavior (Detailed)**
- Startup:
  - `env_utils.setup_hdfs_env()` is called.
  - `AgentConfig.from_file(FLAGS.conf)` is loaded; if `FLAGS.port != 0` overrides `agent_port`.
  - Host is `MY_HOST_IP` env var or `socket.gethostbyname(gethostname())`.
  - gRPC channel to `{host}:{agent_port}`; `AgentServiceStub` created.
  - `model_name` resolved as `agent_conf.base_name` or `FLAGS.model_name`.
- `FLAGS.server_type` mapping: `ps` -> `ServerType.PS`, `dense` -> `ServerType.DENSE`, else `ServerType.ENTRY`.
- `FLAGS.cmd_type` dispatch:
  - `hb`: send `HeartBeatRequest(server_type=...)`; print each key with number of addrs and list.
  - `gr`: assert `model_name`; send `GetReplicasRequest(server_type, task, model_name)`; print reply address list.
  - `addr` or (`get` + `args=addr`):
    - Connect `MonolithKazooClient` with `agent_conf.zk_servers`.
    - Traverse `/{bzid}/service/{model_name}`; support dc-aware layout (`idc:cluster/server_type:task`) and non-dc (`server_type:task`).
    - Uses regex `TASK = r'^(\\w+):(\\d+)$'` to detect `server_type:task` nodes; otherwise treats as `idc:cluster` prefix.
    - For each replica node, read `ReplicaMeta`, print path + `archon_address`, `address`, and `ModelState.Name`.
    - Sort output; handle `NoNodeError` by printing "{model_name} has not load !" and returning.
  - `get` + `args=info`: print `cal_model_info_v2(model_dir, ckpt)`.
  - `get` + `args in {res,pub,portal,lock,elect}`:
    - Select ZK path prefix: `/bzid/resource|publish|portal|lock|election`.
    - If path missing, print `no {args} found !`.
    - Otherwise list children, read data for each, print sorted keys with values.
  - `load`: assert `model_name`; create `ModelMeta(model_name, model_dir, ckpt, num_shard)`; write to `/bzid/portal/{model_name}` (create or set).
  - `unload`: delete `/bzid/portal/{model_name}`; ignore errors.
  - `clean`: delete all nodes under portal/publish/service/resource based on `FLAGS.args`.
- All ZK clients are started and stopped per operation.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-cli/src/bin/agent_client.rs` (or CLI subcommand).
- Rust public API surface: CLI entrypoint + small helper functions for each `cmd_type`.
- Data model mapping: use Rust protobuf types for `AgentService` and `ReplicaMeta`/`ModelMeta` equivalents.
- Feature gating: `tf-runtime` or `zk` features for ZK access; gRPC feature for AgentService.
- Integration points: reuse Rust ZK client + gRPC client modules.

**Implementation Steps (Detailed)**
1. Recreate CLI flags: `port`, `args`, `server_type`, `task`, `model_dir`, `ckpt`, `num_shard`, plus shared `FLAGS` from `client.py`.
2. Implement gRPC client calls (`HeartBeat`, `GetReplicas`) with identical formatting to stdout.
3. Implement ZK traversal for `addr` respecting dc-aware and non-dc-aware layouts.
4. Implement portal/publish/resource/lock/election reads with exact error messages.
5. Implement `load/unload/clean` ZK mutations with same node paths.
6. Mirror env var usage for host selection and config overrides.

**Tests (Detailed)**
- Python tests: none specific
- Rust tests: add CLI integration tests with fake ZK + fake gRPC stub
- Cross-language parity test: run Python CLI and Rust CLI against same fake ZK/gRPC environment and compare output.

**Gaps / Notes**
- This CLI hard-depends on ZK and gRPC; Rust implementation needs compatible test doubles.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/agent_controller.py`
<a id="monolith-agent-service-agent-controller-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 145
- Purpose/role: CLI to declare SavedModel configs in ZK, and publish/unpublish layouts.
- Key symbols/classes/functions: `find_model_name`, `declare_saved_model`, `map_model_to_layout`, `bzid_info`, `main`
- External dependencies: `tensorflow`, `saved_model_pb2`, `compat`, `monolith.agent_service.backends.*`, ZK backend.
- Side effects: reads `saved_model.pb`, writes/updates ZK nodes, prints JSON.

**Required Behavior (Detailed)**
- `find_model_name(exported_models_path)`:
  - Uses `entry/` subdirectory and picks `sorted(tf.io.gfile.listdir(entry_path))[0]` as timestamp.
  - Reads `saved_model.pb`, parses `SavedModel`, scans graph nodes with `op == 'TfServingRemotePredict'`.
  - Returns first `model_name` attribute (decoded, before `:`), or `None` if not present.
- `declare_saved_model(bd, export_base, model_name=None, overwrite=False, arch='entry_ps')`:
  - Asserts `arch == 'entry_ps'`.
  - Determines `model_name` via `find_model_name` if not supplied.
  - Logs mismatch if supplied name differs from export name; asserts non-None.
  - Asserts no existing saved_models unless `overwrite`.
  - For each subgraph in `export_base`:
    - Build `SavedModelDeployConfig(model_base_path=..., version_policy='latest' for entry, else 'latest_once')`.
    - Call `bd.decl_saved_model(SavedModel(model_name, sub_graph), deploy_config)`.
  - Logs success; returns `model_name`.
- `map_model_to_layout(bd, model_pattern, layout_path, action)`:
  - Parses `model_pattern` as `model_name:sub_graph_pattern`.
  - `fnmatch.filter` on available subgraphs.
  - `pub` => `bd.add_to_layout`, `unpub` => `bd.remove_from_layout`.
- `bzid_info(bd)`: `print(json.dumps(bd.bzid_info(), indent=2))`.
- `main`:
  - Validates `FLAGS.cmd` in `decl|pub|unpub|bzid_info`.
  - Creates `ZKBackend`, `start()` then executes command; `stop()` in `finally`.
  - For `pub/unpub`, layout path is `/{bzid}/layouts/{layout}`.
  - Uses `env_utils.setup_hdfs_env()` in `__main__` guard; sets logging.
  - Flags: `zk_servers`, `bzid`, `export_base`, `overwrite`, `model_name`, `layout`, `arch`, `cmd`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-cli/src/bin/agent_controller.rs`.
- Rust public API surface: CLI entrypoint + helpers for `declare_saved_model` and layout mapping.
- Data model mapping: `SavedModel`, `SavedModelDeployConfig`, `ZKBackend` analogs in Rust.
- Feature gating: `tf-runtime`/`saved-model` parsing feature for `find_model_name` (optional if using tf runtime).
- Integration points: ZK backend + layout manager.

**Implementation Steps (Detailed)**
1. Port CLI flags (`cmd`, `bzid`, `zk_servers`, `export_base`, `model_name`, `layout`, `arch`, `overwrite`).
2. Implement saved_model.pb parsing in Rust (protobuf decode + graph scan for `TfServingRemotePredict`).
3. Recreate `declare_saved_model` and `map_model_to_layout` logic exactly.
4. Ensure `latest` vs `latest_once` policy mapping is preserved.
5. Port `bzid_info` output formatting to JSON.

**Tests (Detailed)**
- Python tests: `monolith/agent_service/agent_controller_test.py`
- Rust tests: add equivalent tests using fake ZK and test saved_model fixture.
- Cross-language parity test: compare `bzid_info`/layout changes after publish/unpublish.

**Gaps / Notes**
- Rust needs SavedModel graph parsing for remote predict op discovery.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/agent_controller_test.py`
<a id="monolith-agent-service-agent-controller-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 95
- Purpose/role: Tests ZKBackend + agent_controller declare/publish/unpublish flows using FakeKazooClient.
- Key symbols/classes/functions: `AgentControllerTest.test_decl_saved_models`, `.test_pub`
- External dependencies: `FakeKazooClient`, `ZKBackend`, test saved_model fixtures.
- Side effects: creates ZK nodes in fake client.

**Required Behavior (Detailed)**
- `setUpClass`:
  - `bzid='gip'`.
  - `bd = ZKBackend(bzid, zk_servers='127.0.0.1:9999')`.
  - Replace `bd._zk` with `FakeKazooClient()`.
  - Call `bd.start()`.
- `test_decl_saved_models`:
  - Uses exported saved_model dir under `TEST_SRCDIR/TEST_WORKSPACE/monolith/native_training/model_export/testdata/saved_model`.
  - Calls `declare_saved_model(..., model_name='test_ffm_model', overwrite=True)`.
  - Verify `bd.list_saved_models('test_ffm_model')` matches `{ps_0..ps_4, entry}`.
- `test_pub`:
  - Declare saved model again.
  - `map_model_to_layout(..., "test_ffm_model:entry", "/gip/layouts/test_layout1", "pub")` -> `layout_info['test_layout1'] == ['test_ffm_model:entry']`.
  - `map_model_to_layout(..., "test_ffm_model:ps_*", "pub")` -> adds `ps_0..ps_4` (ordered list).
  - `map_model_to_layout(..., "test_ffm_model:ps_*", "unpub")` -> back to entry only.
  - `map_model_to_layout(..., "test_ffm_model:entry", "unpub")` -> empty list.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-cli/tests/agent_controller.rs` (or serving tests if backend lives there).
- Rust public API surface: ZK backend + controller helpers.
- Data model mapping: `SavedModel` and `SavedModelDeployConfig` equality comparisons.

**Implementation Steps (Detailed)**
1. Port fake ZK backend for tests.
2. Port `declare_saved_model` and `map_model_to_layout` in Rust.
3. Add fixtures for saved_model testdata or stub saved_model parsing.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: exact parity assertions for saved_models and layout contents.

**Gaps / Notes**
- Requires test saved_model fixture to exist and be accessible in Rust tests.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/agent_service.py`
<a id="monolith-agent-service-agent-service-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 155
- Purpose/role: gRPC AgentService implementation + server wrapper for replica discovery and heartbeat.
- Key symbols/classes/functions: `AgentDataProvider`, `AgentServiceImpl`, `AgentService`
- External dependencies: `grpc`, `concurrent.futures`, `functools.singledispatchmethod`, `monolith.agent_service.*`
- Side effects: gRPC server binds to ports; reads env/config; logs heartbeat.

**Required Behavior (Detailed)**
- `AgentServiceImpl.__init__` is **overloaded** via `@singledispatchmethod`:
  - `ReplicaWatcher` + optional `AgentConfig`
  - `ZKMirror` + `AgentConfig`
  - `AgentDataProvider` + `AgentConfig`
- `GetReplicas` behavior:
  - If `conf is None` or `agent_version == 1`, fetch replicas via `ReplicaWatcher` with `idc/cluster`.
  - If `agent_version == 2`, fetch via ZKMirror (`get_task_replicas`).
  - Else: `NotImplementedError` for v3.
- `HeartBeat` behavior:
  - `agent_version == 1`: call `ReplicaWatcher.get_all_replicas`, optionally strip DC prefix when `dc_aware`.
  - `agent_version == 2`: call `ZKMirror.get_all_replicas`.
  - `agent_version == 3`: use `AgentDataProvider` callback map to fill addresses.
- `GetResource` behavior:
  - `agent_version == 1`: return empty `GetResourceResponse`.
  - Else: fill address with `get_local_ip()` + `agent_port`, plus memory via `cal_available_memory_v2()`.
- `AgentService` wrapper:
  - Constructs grpc server with `ThreadPoolExecutor`, registers `AgentServiceImpl`, binds to port.
  - `AgentDataProvider` wraps `addrs_fn` callback returning `{saved_model_name: [addr...]}`.
  - `GetReplicas` v1 uses `_watcher._conf.idc/cluster` and `get_replicas(server_type, task, idc, cluster)`; v2 maps `ReplicaMeta.address`.
  - `HeartBeat` v1 uses `dc_aware` to strip path prefix (`key.split('/')[-1]`) and populates `AddressList`.
  - `HeartBeat` v3 uses `_data_provider._addrs_fn()` if non-empty.
  - `GetResource` v2+ fills `shard_id`, `replica_id`, `memory`, `cpu/network/work_load=-1.0`.
  - `AgentService` binds `[::]:{port or 0}` for watcher path; uses `conf.agent_port` for zk/data_provider.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/src/grpc_agent.rs` (gRPC), `monolith-rs/crates/monolith-serving/src/server.rs` (server lifecycle), plus new module for AgentConfig integration.
- Rust public API surface: `AgentServiceRealImpl` + `AgentGrpcServer` must be extended to mirror multi-backend behaviors and `AgentServiceImpl` logic.
- Data model mapping: use `monolith_proto::monolith::serving::agent_service::*` for request/response.
- Feature gating: always available with `grpc` feature.
- Integration points: `monolith-serving::server::Server` should optionally host AgentService parity endpoints.

**Implementation Steps (Detailed)**
1. Add AgentConfig wiring to Rust AgentService (support v1/v2/v3 equivalent selection).
2. Port `ReplicaWatcher` + `ZKMirror` behaviors or stub with clear TODOs and guards.
3. Implement `dc_aware` key rewriting in HeartBeat response.
4. Implement `GetResource` response with local IP and memory (port parity).
5. Add AgentDataProvider path for v3-style maps.
6. Mirror Python error semantics (NotImplemented) when agent_version == 3 in GetReplicas.
7. Add tests mirroring `agent_service_test.py`.

**Tests (Detailed)**
- Python tests: `monolith/agent_service/agent_service_test.py`
- Rust tests: add to `monolith-rs/crates/monolith-serving/tests/*` covering GetReplicas/HeartBeat/GetResource.
- Cross-language parity test: run Python client against Rust server for v1/v2 paths.

**Gaps / Notes**
- Rust currently provides minimal AgentService; needs full v1/v2/v3 parity.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/agent_service_test.py`
<a id="monolith-agent-service-agent-service-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 107
- Purpose/role: Tests gRPC AgentService heartbeats and replica lookup using FakeKazooClient + ReplicaWatcher.
- Key symbols/classes/functions: `AgentServiceTest.setUpClass`, `test_heart_beat`, `test_get_replicas`
- External dependencies: Fake ZK, `ReplicaWatcher`, `AgentService`, gRPC client stubs.
- Side effects: starts gRPC server, creates ephemeral ZK nodes.

**Required Behavior (Detailed)**
- `setUpClass`:
  - Set `TCE_INTERNAL_IDC='lf'`, `TCE_LOGICAL_CLUSTER='default'`.
  - Start `FakeKazooClient`.
  - Build `AgentConfig(bzid='test_model', base_name=MODEL_NAME, deploy_type='ps', base_path=BASE_PATH, num_ps=20, dc_aware=True)`.
  - Create `ReplicaWatcher(zk, agent_conf)`.
  - Call `register(zk)` to seed replica nodes; then `watcher.watch_data()`.
  - Start `AgentService(watcher, port=agent_conf.agent_port)`.
  - Create `SvrClient(agent_conf)`.
- `register(zk)`:
  - `path_prefix = agent_conf.path_prefix`.
  - For each PS task `0..num_ps-1` and replica `0..1`:
    - `ReplicaMeta(address='192.168.1.{idx}:{find_free_port()}', stat=ModelState.AVAILABLE)`.
    - Create ephemeral node at `{path_prefix}/ps:{task_id}/{replica_id}` with `makepath=True`.
  - For entry replicas `0..1`: create `{path_prefix}/entry:0/{replica_id}` similarly.
  - Uses `zk.retry(zk.create, ...)` and falls back to `zk.set` on `NodeExistsError`.
- `test_heart_beat`:
  - `client.heart_beat(server_type=ServerType.PS)`; asserts `len(resp.addresses) == 20`.
- `test_get_replicas`:
  - `client.get_replicas(server_type=ServerType.PS, task=NUM_PS_REPLICAS - 1)` (task=1).
  - Asserts `len(resp.address_list.address) == NUM_PS_REPLICAS` (2).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/tests/agent_service.rs`.
- Rust public API surface: gRPC AgentService + ReplicaWatcher equivalent.
- Data model mapping: `ReplicaMeta`, `ModelState`, and gRPC proto types.

**Implementation Steps (Detailed)**
1. Implement fake ZK and replica registration helpers in Rust tests.
2. Start Rust AgentService server on a free port.
3. Call HeartBeat and GetReplicas via gRPC client; assert counts.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: parity test with fake ZK + gRPC client.

**Gaps / Notes**
- Rust needs fake ZK client and ReplicaWatcher equivalent to reproduce dc-aware paths.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/agent_v1.py`
<a id="monolith-agent-service-agent-v1-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 390
- Purpose/role: v1 agent orchestrator that launches TensorFlow Serving processes (ps/entry/dense) and hosts AgentService.
- Key symbols/classes/functions: `ProcessType`, `ProcessNode`, `ProcessMgr`, `AgentV1`
- External dependencies: `subprocess`, `signal`, `threading`, `ReplicaManager`, `AgentService`, `AgentBase`
- Side effects: starts and kills subprocesses; registers signal handlers; logs to files.

**Required Behavior (Detailed)**
- `ProcessNode.__init__`:
  - Chooses command and port via `get_cmd_and_port` based on process type.
  - For ENTRY sets env `PORT2` to agent port.
  - Tracks `_sub_procs` map and `_popen` handle.
- `ProcessNode.run()`:
  - If ENTRY: waits for PS replicas (and dense if `dense_alone`) via `ReplicaManager` before starting; timeout 3600s.
  - Launches subprocess with `ServingLog` output redirection (unless `MLP_POD_NAME` env set).
  - Waits for port open; starts sub-procs recursively; returns success boolean.
- `ProcessNode.wait_for_started()`: polls `check_port_open` every 10s up to 3600s; returns True on success.
- `ProcessNode.kill()`: kills sub-procs then self with retries; uses `poll`/`returncode` guards.
- `ProcessMgr`:
  - Registers SIGTERM/SIGINT handler to kill all processes.
  - Background `_poll` thread watches all processes; if any exits, kills all.
  - `start()` runs `ProcessNode.run()` for each and starts poll thread.
- `AgentV1`:
  - Starts ZK client, ReplicaManager (watcher/updater), AgentService, and ProcessMgr in that order.
  - Build process graph based on `DeployType` (MIXED/ENTRY/PS/DENSE) and `dense_alone`.
  - `stop()` shuts down processes, AgentService, ReplicaManager, ZK.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/src/agent_v1.rs` + process supervisor module.
- Rust public API surface: `AgentV1` struct implementing `AgentBase` trait.
- Data model mapping: `AgentConfig`, `DeployType`, `TFSServerType` in Rust config module.
- Integration points: gRPC AgentService + ReplicaManager + process management.

**Implementation Steps (Detailed)**
1. Implement process supervisor with child processes and log redirection.
2. Port ENTRY wait-for-PS/dense logic with same timeouts and intervals.
3. Port signal handling to trigger shutdown of all children.
4. Port startup order and error handling semantics.
5. Ensure `PORT2` env var is injected for ENTRY.

**Tests (Detailed)**
- Python tests: none directly
- Rust tests: integration test that spawns dummy processes and validates kill-on-exit behavior.
- Cross-language parity test: simulate ReplicaManager readiness and verify ENTRY wait logic.

**Gaps / Notes**
- Requires replacement for `check_port_open` and process supervision in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/agent_v3.py`
<a id="monolith-agent-service-agent-v3-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 210
- Purpose/role: v3 unified agent that manages TFS model config via layout updates, registers service info, and serves address maps via AgentService.
- Key symbols/classes/functions: `gen_empty_model_config_file`, `AgentV3`.
- External dependencies: `TFSWrapper`, `ZKBackend`, `Container`, `ContainerServiceInfo`, protobuf text/json, `model_server_config_pb2`.
- Side effects: writes model_config file, starts/stops TFS process, registers ZK service info, background threads, SIGKILL on exit.

**Required Behavior (Detailed)**
- `gen_empty_model_config_file()`:
  - Uses `tempfile.mktemp()` and writes `model_config_list {}`.
- `AgentV3.__init__`:
  - Asserts `deploy_type == UNIFIED` and `agent_version == 3`.
  - Installs SIGTERM/SIGINT handlers to set `_exit_event`.
  - Creates `_model_config_path` and initializes `TFSWrapper` with ports + config path.
  - Builds `_layout_filters` from `config.layout_filters`:
    - Replaces `${shard_id}` and `${shard_num}`.
    - Splits `match;cond`, normalizes regex via `normalize_regex`.
  - Builds `ContainerServiceInfo` with local IP and ports; `debug_info` JSON includes layout path + filters.
  - Initializes `ZKBackend` and `AgentService(AgentDataProvider(_gen_addrs_map))`.
- `_gen_addrs_map()`:
  - Reads `backend.get_service_map()` and returns `{model:sub_graph: [addr...]}`.
  - Uses `grpc` addr if `tfs_wrapper.is_grpc_remote_op`, else `archon` addr.
- `sync_available_saved_models()`:
  - Calls `tfs_wrapper.list_saved_models_status()`.
  - For `State.AVAILABLE`, converts `model_name:sub_graph` into `SavedModel` and syncs via backend.
- `layout_update_callback(saved_models)`:
  - Builds `ModelServerConfig` with `model_config_list.SetInParent()`.
  - Applies layout filters: `re.match(match, sub_graph)` + `eval(cond, None, {groupdict as int})`.
  - Adds `ModelConfig` generated by `gen_model_config(name, base_path, version_policy)`.
  - Writes protobuf text to `_model_config_path`.
- `start_bg_thread(fn, interval)`:
  - Runs `fn()` in loop until `_exit_event` set; logs exceptions; sleeps `interval`.
- `start()`:
  - Starts TFSWrapper, ZKBackend, AgentService.
  - Starts background threads:
    - `backend.report_service_info(container, service_info)` every 60s.
    - `sync_available_saved_models` every 30s.
  - Registers layout callback on `config.layout_path`.
- `stop()`:
  - Sets exit event, joins threads, stops AgentService, backend, and TFSWrapper (logs warnings on error).
- `wait_for_termination()`:
  - Polls `tfs_wrapper.poll()`; if exit, sets `_exit_event`, stops, sleeps 1s, and `SIGKILL`s self.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/src/agent_v3.rs`.
- Rust public API surface: `AgentV3` implementing `AgentBase` with background tasks.
- Data model mapping: `Container`, `SavedModel`, `SavedModelDeployConfig`, `ContainerServiceInfo`, layout filters, service map.
- Feature gating: `tf-runtime` (TFSWrapper), `zk` (ZKBackend), gRPC for AgentService.

**Implementation Steps (Detailed)**
1. Port layout filter parsing and evaluation (regex + safe expression engine).
2. Implement model_config protobuf text writer (matching TF Serving config).
3. Port service info reporting + available model syncing to ZK.
4. Implement address map provider for AgentService v3.
5. Preserve shutdown semantics (exit event, thread joins, SIGKILL).

**Tests (Detailed)**
- Python tests: `monolith/agent_service/agent_v3_test.py`
- Rust tests: FakeTFSWrapper + FakeZK to verify layout updates + service map.
- Cross-language parity test: generate identical config updates and compare model_config text.

**Gaps / Notes**
- Python uses `tempfile.mktemp()` (unsafe); Rust should use secure temp files but preserve behavior.
- Layout filter `cond` uses `eval` on regex groupdict (needs a safe Rust equivalent).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/agent_v3_test.py`
<a id="monolith-agent-service-agent-v3-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 114
- Purpose/role: Tests AgentV3 layout/publish flow with FakeTFSWrapper and FakeKazooClient.
- Key symbols/classes/functions: `AgentV3Test.test_service_info`, `.test_publish_models`
- External dependencies: Fake TFS wrapper, fake ZK backend.
- Side effects: starts AgentV3 (with fake components), writes model_config file.

**Required Behavior (Detailed)**
- Class setup:
  - `bzid='gip'`, set `MY_HOST_IP=127.0.0.1`.
  - Build `AgentConfig(bzid='gip', deploy_type='unified', agent_version=3, layout_pattern='/gip/layout', zk_servers='127.0.0.1:8888')`.
  - `base_path = os.environ['TEST_TMPDIR']`.
  - Construct `AgentV3` with:
    - `conf_path = os.path.join(base_path, '/monolith_serving/conf')` (note: absolute suffix).
    - `tfs_log = os.path.join('monolith_serving/logs/log.log')` (relative path).
  - Replace `_tfs_wrapper` with `FakeTFSWrapper(agent._model_config_path)`.
  - Replace `_backend._zk` with `FakeKazooClient`.
  - Call `agent.start()`.
  - For sub_graph in `['entry','ps_0','ps_1','ps_2']`:
    - `config={'model_base_path': TEST_TMPDIR/test_ffm_model/exported_models/{sub_graph}, 'version_policy': 'latest'}`.
    - Write JSON bytes to ZK path `/gip/saved_models/test_ffm_model/{sub_graph}` with `makepath=True`.
- `test_service_info`: backend `get_service_info(container)` equals agent's `_service_info`.
- `test_publish_models`:
  - Assert `tfs_wrapper.list_saved_models()` initially empty.
  - `zk.ensure_path('/gip/layout/test_ffm_model:entry')` and `...:ps_0`.
  - Expect `list_saved_models()` == `['test_ffm_model:entry','test_ffm_model:ps_0']` (order matters).
  - Call `agent.sync_available_saved_models()`.
  - Expect `backend.get_service_map()` equals:
    - `{'test_ffm_model': {'entry': [agent._service_info], 'ps_0': [agent._service_info]}}`.
  - Delete `/gip/layout/test_ffm_model:ps_0`.
  - Expect `list_saved_models()` == `['test_ffm_model:entry']`.
  - Call `sync_available_saved_models()` and expect service_map only has `entry`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/tests/agent_v3.rs`.
- Rust public API surface: AgentV3, FakeTFSWrapper, Fake ZK backend.

**Implementation Steps (Detailed)**
1. Implement FakeTFSWrapper in Rust with model_config file parsing.
2. Implement Fake ZK backend with nodes under saved_models/layouts.
3. Port tests for service_info equality and publish/unpublish behavior.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: direct parity test using fake components.

**Gaps / Notes**
- Requires deterministic model_config parsing in Rust to match FakeTFSWrapper behavior.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/backends.py`
<a id="monolith-agent-service-backends-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 402
- Purpose/role: Backend abstractions + ZK implementation for layouts, saved models, service info, and sync targets.
- Key symbols/classes/functions: `SavedModel`, `SavedModelDeployConfig`, `Container`, `ContainerServiceInfo`, `AgentBackend`, `CtrlBackend`, `SyncBackend`, `ZKBackend`.
- External dependencies: `kazoo` (ChildrenWatch, state, errors), `dataclasses_json`, `MonolithKazooClient`.
- Side effects: ZK reads/writes, ephemeral nodes, watchers, retry/set on conflicts.

**Required Behavior (Detailed)**
- Data classes:
  - `SavedModel(model_name, sub_graph)` is frozen; `__str__` → `"model:sub_graph"`.
  - `SavedModelDeployConfig` + `ContainerServiceInfo` are `dataclass_json` with `serialize()` → UTF-8 JSON bytes; `deserialize()` uses `from_json`.
  - `Container(ctx_cluster, ctx_id)` string format `"cluster:id"`.
- Abstract backends:
  - `AgentBackend`: layout callbacks, sync_available_saved_models, report/get service info, get_service_map, start/stop.
  - `CtrlBackend`: list/declare saved models, add/remove layout, bzid_info, start/stop.
  - `SyncBackend`: subscribe_model, get_sync_targets, start/stop.
- `ZKBackend.__init__(bzid, zk_servers)`:
  - Creates `MonolithKazooClient` and registers a listener:
    - `KazooState.LOST` → sets `_is_lost` event.
    - other states → clears `_is_lost`.
  - Tracks `_available_saved_model` (set), `_service_info_map` (dict), `_children_watcher_map` (path→ChildrenWatch), `_sync_model_name`.
- `sync_available_saved_models(container, saved_models)`:
  - If `_is_lost` set: clears available set, calls `_zk.restart()`, returns.
  - Computes add/remove sets; for each add: create ephemeral binding node at
    `/{bzid}/binding/{model}/{sub_graph}:{container}` (makepath=True).
  - For each remove: delete the binding node.
  - Updates `_available_saved_model` to the new set.
- `register_layout_callback(layout_path, callback)`:
  - Ensures layout path exists and sets a ChildrenWatch via `_children_watch`.
  - `callback_wrap(children)`:
    - Parses `model_name:sub_graph` from each child.
    - Reads deploy config from `/saved_models/{model}/{sub_graph}`; if missing logs error.
    - Builds list of `(SavedModel, SavedModelDeployConfig)` and `model_names` set.
    - Resets `_service_info_map` to only those model_names (preserving existing sub-maps).
    - For each model_name, registers binding watch on `/{bzid}/binding/{model_name}` with `_bind_callback`.
    - Calls `callback(saved_models)` and returns its result.
- `_bind_callback(model_name, children)`:
  - If model_name not tracked, returns False.
  - For each child `sub_graph:ctx_cluster:ctx_id:...`, fetches `ContainerServiceInfo`.
  - Populates `_service_info_map[model_name][sub_graph]` with service infos.
- `report_service_info(container, service_info)`:
  - Creates ephemeral node at `/{bzid}/container_service/{container}` with serialized JSON bytes.
- `get_service_info(container)`:
  - Reads node and deserializes `ContainerServiceInfo` (returns None on NoNode).
- `_children_watch(path, callback)`:
  - If watcher exists and not stopped, log and skip.
  - Ensures path exists before creating `ChildrenWatch`.
- `list_saved_models(model_name)`:
  - Reads children under `/{bzid}/saved_models/{model_name}`, returns `SavedModel` list.
  - Returns empty list on `NoNodeError`.
- `decl_saved_model(saved_model, deploy_config)`:
  - Creates znode at `/{bzid}/saved_models/{model}/{sub_graph}` with JSON bytes (makepath=True).
- `add_to_layout(layout, saved_model)`:
  - Ensures path `"{layout}/{model:sub_graph}"` exists.
- `remove_from_layout(layout, saved_model)`:
  - Deletes path; ignores NoNodeError.
- `bzid_info()`:
  - Collects `model_info` (deploy configs + bindings), `container_info` (service info + saved_models), `layout_info` (sorted saved_model list).
  - Increments `sub_graphs_available` when bindings exist.
  - Returns sorted dicts for stable output.
- Sync backend:
  - `subscribe_model(model_name)`: sets `_sync_model_name` (asserts only once) and registers binding watch.
  - `get_sync_targets(sub_graph)`:
    - If `_is_lost` set, clears available and restarts ZK.
    - Returns `(f"{model}:{sub_graph}", [service_info.grpc...])`.
- ZK helpers:
  - `create_znode`: on `NodeExistsError` uses `_zk.retry(_zk.set, path=..., value=...)`; logs other exceptions.
  - `delete_znode`: logs errors on exception.
  - `get_znode`: returns bytes or None on `NoNodeError`.
- `start()`/`stop()` delegate to `_zk.start()` / `_zk.stop()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/src/agent_backend.rs` + ZK integration (feature gated).
- Rust public API surface:
  - Structs: `SavedModel`, `SavedModelDeployConfig`, `Container`, `ContainerServiceInfo` (serde JSON).
  - Traits: `AgentBackend`, `CtrlBackend`, `SyncBackend`.
  - `ZkBackend` implementing all three.
- Data model mapping:
  - ZK nodes: `/saved_models`, `/layouts`, `/binding`, `/container_service` with JSON payloads.
  - Service map: `HashMap<Model, HashMap<SubGraph, Vec<ContainerServiceInfo>>>`.
- Integration points: `AgentV3`, `agent_controller`, `tfs_monitor`, `replica_manager`.

**Implementation Steps (Detailed)**
1. Port data classes with `serde_json` encode/decode to bytes.
2. Implement backend traits and ZK client wrapper with watch support.
3. Match ChildrenWatch semantics (invoke callback on changes).
4. Preserve ZK LOST behavior (restart + clear available set).
5. Mirror `bzid_info` output structure and sorting.

**Tests (Detailed)**
- Python tests: `monolith/agent_service/backends_test.py`
- Rust tests: mock ZK for layout watches, binding updates, and bzid_info structure.
- Cross-language parity test: compare serialized JSON payloads and service_map updates.

**Gaps / Notes**
- Requires Rust ZK client with watcher support; ensure watcher stop semantics match `ChildrenWatch`.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/backends_test.py`
<a id="monolith-agent-service-backends-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 134
- Purpose/role: Tests ZKBackend layout callbacks, service info, binding updates, and sync targets using a fake ZK client.
- Key symbols/classes/functions: `ZKBackendTest` methods (`test_register_service`, `test_layout_callback`, `test_sync_available_models`, `test_service_map`, `test_sync_backend`).
- External dependencies: `FakeKazooClient`, `ZKBackend`, `SavedModel`, `SavedModelDeployConfig`.
- Side effects: Creates/deletes ZK nodes in fake client.

**Required Behavior (Detailed)**
- `setUpClass`:
  - `bzid='gip'`, `container=Container("default","asdf")`.
  - `service_info` with grpc/http/archon/agent/idc.
  - Instantiate `ZKBackend`, replace `_zk` with `FakeKazooClient`.
  - Register layout callback on `"/gip/layouts/test_layout/mixed"`.
  - Call `report_service_info(container, service_info)`.
- `test_register_service`:
  - `get_service_info(container)` returns the same `service_info`.
- `test_layout_callback`:
  - Declares saved models `entry, ps_0, ps_1, ps_2` and adds to layout.
  - Expects `layout_record` to be list of `(SavedModel, SavedModelDeployConfig)` for each subgraph.
  - After removing `entry`, expects callback list with only ps_* entries.
- `test_sync_available_models`:
  - Syncs available models `entry, ps_0, ps_1`.
  - Asserts binding znodes exist at `/gip/binding/test_ffm_model/<sub_graph>:<container>`.
- `test_service_map`:
  - Syncs available models `entry, ps_0`.
  - Expects service map `{'test_ffm_model': {'ps_0': [service_info], 'entry': [service_info]}}`.
- `test_sync_backend`:
  - `subscribe_model("test_ffm_model")`, then sync available models `ps_0, ps_1, ps_2`.
  - `get_sync_targets("ps_1")` returns `("test_ffm_model:ps_1", [service_info.grpc])`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/tests/backends.rs` (new).
- Rust public API surface: `ZkBackend` + fake ZK client.
- Data model mapping: JSON serialization for deploy config + service info.
- Feature gating: tests require fake ZK or in-memory backend.

**Implementation Steps (Detailed)**
1. Implement FakeZK client in Rust with create/get/delete and ChildrenWatch semantics.
2. Port layout callback test to verify `SavedModelDeployConfig` list ordering.
3. Port binding and service_map assertions.
4. Port sync backend target selection test.

**Tests (Detailed)**
- Python tests: `ZKBackendTest` in this file.
- Rust tests:
  - `zk_backend_register_service`
  - `zk_backend_layout_callback`
  - `zk_backend_sync_available_models`
  - `zk_backend_service_map`
  - `zk_backend_sync_backend`
- Cross-language parity test:
  - Compare service_map and binding paths generated by Python vs Rust for identical inputs.

**Gaps / Notes**
- Tests rely on `TEST_TMPDIR` for `SavedModelDeployConfig` base paths.
- `test_register_service`: `get_service_info(container)` equals originally reported service info.
- `test_layout_callback`:
  - Declare saved_models and add to layout; callback receives list of `(SavedModel, DeployConfig)` in order.
  - Removing entry updates callback list.
- `test_sync_available_models`: `sync_available_saved_models` creates binding nodes under `/bzid/binding/...`.
- `test_service_map`: `get_service_map` returns service info list for entry/ps_0.
- `test_sync_backend`: `subscribe_model` + `sync_available_saved_models` produces `get_sync_targets` for a subgraph.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/tests/backends.rs`.
- Rust public API surface: ZKBackend + SavedModel + DeployConfig.

**Implementation Steps (Detailed)**
1. Port Fake ZK backend + ZKBackend logic.
2. Port tests for layout callback and binding map parity.
3. Ensure ordering of saved_models matches Python list order.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: parity tests mirroring the same assertions.

**Gaps / Notes**
- Requires deterministic layout callback ordering in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/client.py`
<a id="monolith-agent-service-client-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 126
- Purpose/role: Lightweight CLI client for load/unload/status via ZKMirror (portal/publish/service inspection).
- Key symbols/classes/functions: `ServingClient`, `LoadSate`, `main`
- External dependencies: `MonolithKazooClient`, `ZKMirror`, `ModelMeta`, `ReplicaMeta`, TF Serving `ModelVersionStatus`.
- Side effects: ZK reads/writes, prints status, env setup.

**Required Behavior (Detailed)**
- `LoadSate` dataclass: `portal: bool`, `publish: bool`, `service: dict`.
- `ServingClient.__init__`:
  - Creates `MonolithKazooClient` and `ZKMirror(zk, bzid)`; starts mirror with `is_client=True`.
- `load(model_name, model_dir, ckpt=None, num_shard=-1)`:
  - Creates `ModelMeta`, computes path under `portal_base_path`.
  - If path exists, raise `RuntimeError('{model_name} has exists')`.
  - Otherwise create node with serialized meta.
- `unload(model_name)`:
  - Delete portal node if exists; else log warning.
- `get_status(model_name)`:
  - `portal` True if `/bzid/portal/{model_name}` exists.
  - `publish` True if any `/bzid/publish/{shard}:{replica}:{model_name}` exists.
  - `service` map from `server_type:task:replica` to `ReplicaMeta.stat` for all replicas under `/bzid/service/{model_name}`.
- `main`:
  - Requires `zk_servers` and `bzid` flags.
  - `cmd_type` `load` or `unload`, otherwise prints `get_status`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-cli/src/bin/serving_client.rs`.
- Rust public API surface: CLI entrypoint + `ServingClient` struct.
- Data model mapping: `ModelMeta`, `ReplicaMeta`, `ModelState` enums.

**Implementation Steps (Detailed)**
1. Port `ServingClient` with ZKMirror client mode.
2. Implement portal node create/delete semantics and error message parity.
3. Implement status inspection across portal/publish/service paths.
4. Port CLI flags and default cmd_type handling.

**Tests (Detailed)**
- Python tests: none specific
- Rust tests: add fake ZK tests for `load/unload/get_status`.

**Gaps / Notes**
- Rust needs ZKMirror equivalent and a way to read/write serialized ModelMeta.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/constants.py`
<a id="monolith-agent-service-constants-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 3
- Purpose/role: constant definitions for agent service.
- Key symbols/classes/functions: `HOST_SHARD_ENV`
- External dependencies: none
- Side effects: none

**Required Behavior (Detailed)**
- Export string constant `MONOLITH_HOST_SHARD_N`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/src/constants.rs` (new) or existing config module.

**Implementation Steps (Detailed)**
1. Add Rust constant with identical name/value.
2. Ensure all config code references same constant.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: optional constant presence test.

**Gaps / Notes**
- Trivial but must be mirrored.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/data_def.py`
<a id="monolith-agent-service-data-def-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 173
- Purpose/role: Data model definitions for agent service (ModelMeta, ResourceSpec, PublishMeta, ReplicaMeta, Event, enums).
- Key symbols/classes/functions: `ModelMeta`, `ResourceSpec`, `PublishMeta`, `ReplicaMeta`, `Event`, `PublishType`, `EventType`.
- External dependencies: `dataclasses_json`, `tensorflow_serving` protos, `AddressFamily`.
- Side effects: none; pure data serialization + address selection logic.

**Required Behavior (Detailed)**
- Type aliases:
  - `ModelState = ModelVersionStatus.State`.
  - `ModelName`, `SubModelName`, `SubModelSize`, `TFSModelName`, `VersionPath` are `NewType` wrappers.
  - `EmptyStatus = StatusProto()` (defined but unused).
- `ModelMeta`:
  - Fields: `model_name`, `model_dir`, `ckpt`, `num_shard=-1`, `action='NONE'`, `spec_replicas=[]`.
  - `get_path(base_path)` -> `os.path.join(base_path, model_name)`.
  - `serialize()` -> UTF-8 JSON bytes via `dataclasses_json`.
  - `deserialize(bytes)` -> `from_json` on UTF-8 string.
- `ResourceSpec`:
  - Fields: `address`, `shard_id`, `replica_id`, `memory`, `cpu=-1.0`, `network=-1.0`, `work_load=-1.0`.
  - `get_path(base_path)` -> `base_path/{shard_id}:{replica_id}`.
  - `serialize`/`deserialize` mirror `ModelMeta`.
- `PublishType` enum: `LOAD=1`, `UNLOAD=2`.
- `PublishMeta`:
  - Fields: `shard_id`, `replica_id=-1`, `model_name`, `num_ps`, `total_publish_num=1`, `sub_models`, `ptype=PublishType.LOAD`, `is_spec=False`.
  - `get_path(base_path)` -> `base_path/{shard_id}:{replica_id}:{model_name}`.
  - `serialize`/`deserialize` mirror `ModelMeta`.
- `ReplicaMeta`:
  - Fields: `address`, `address_ipv6`, `stat=ModelState.UNKNOWN`, `model_name`, `server_type`, `task=-1`, `replica=-1`, `archon_address`, `archon_address_ipv6`.
  - `get_path(bzid, sep='/')` -> `['', bzid, 'service', model_name, f'{server_type}:{task}', str(replica)]` joined by `sep`.
  - `get_address(use_archon=False, address_family=AddressFamily.IPV4)`:
    - Chooses `archon_address`/`archon_address_ipv6` when `use_archon`.
    - Treats `0.0.0.0*` and `[::]*` as invalid (set to None).
    - If `address_family == IPV4`: prefer ipv4, fall back to ipv6; else prefer ipv6 then ipv4.
- `EventType` enum: `PORTAL=1`, `SERVICE=2`, `PUBLISH=3`, `RESOURCE=4`, `UNKNOWN=1` (alias of PORTAL).
- `Event`:
  - Fields: `path=None`, `data=b''`, `etype=EventType.UNKNOWN`.
  - `serialize`/`deserialize` via `dataclasses_json` UTF-8 JSON bytes.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/src/data_def.rs` (new) or `monolith-rs/crates/monolith-core/src`.
- Rust public API surface: structs with serde JSON; enums for state.
- Data model mapping: use `monolith_proto` for ModelState where appropriate.

**Implementation Steps (Detailed)**
1. Port all dataclasses to Rust structs with serde JSON.
2. Implement `serialize`/`deserialize` helpers to match Python bytes encoding.
3. Port `ReplicaMeta.get_address` and path helpers exactly.

**Tests (Detailed)**
- Python tests: `monolith/agent_service/data_def_test.py`
- Rust tests: add roundtrip JSON + address selection tests.

**Gaps / Notes**
- No Rust equivalents yet.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/data_def_test.py`
<a id="monolith-agent-service-data-def-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 52
- Purpose/role: Tests serialization/deserialization roundtrip for `ModelMeta`, `ResourceSpec`, `ReplicaMeta`.
- Key symbols/classes/functions: `DataDefTest.serde` and tests.
- External dependencies: `monolith.agent_service.data_def`.
- Side effects: none.

**Required Behavior (Detailed)**
- `serde(item)`:
  - `cls = item.__class__`.
  - `serialized = item.serialize()`.
  - `recom = cls.deserialize(serialized)`.
  - Asserts `item == recom`.
- `test_model_info`:
  - `ModelMeta(model_name='monolith', num_shard=3, model_dir='/tmp/opt', ckpt='model.ckpt-1234')`.
  - Roundtrip equality via `serde`.
- `test_resource`:
  - `ResourceSpec(address='localhost:123', shard_id=10, replica_id=2, memory=12345, cpu=3.5)` roundtrip.
- `test_replica_meta`:
  - `ReplicaMeta(address='localhost:123', model_name='monolith', server_type='ps', task=0, replica=0)` roundtrip.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/tests/data_def.rs`.
- Rust public API surface: serialization/deserialization methods for the same structs.

**Implementation Steps (Detailed)**
1. Port `serialize`/`deserialize` format (JSON or bytes) to Rust.
2. Ensure equality comparisons are field-wise identical.
3. Add tests for roundtrip parity.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: same data roundtrip assertions.

**Gaps / Notes**
- Serialization format must match Python exactly (field names, defaults).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/mocked_tfserving.py`
<a id="monolith-agent-service-mocked-tfserving-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 399
- Purpose/role: In-process fake TensorFlow Serving gRPC server for tests.
- Key symbols/classes/functions: `ModelConf`, `ModelVersion`, `ModelMeta`, `Event`, `ModelMgr`, `ModelServiceImpl`, `PredictionServiceImpl`, `FakeTFServing`.
- External dependencies: `grpc`, `tensorflow_serving.apis.*`, `model_server_config_pb2`, utils for status/model_config.
- Side effects: starts gRPC server; spawns background thread to update model states.

**Required Behavior (Detailed)**
- Data classes:
  - `ModelConf`: `model_name`, `base_path`, `version_policy='latest'`, `version_data=None`, `model_platform='tensorflow'`, `signature_name=('update','predict')`.
  - `ModelVersion`: `version=0`, `version_label=None`, `state=ModelState.UNKNOWN`.
  - `ModelMeta`: holds `conf`, `versions` (defaults to `[ModelVersion()]`), `_unloading` flag; `is_unloading()`/`set_unloading()`.
  - `Event`: `model_name`, `version`, `state`.
- `ModelMgr`:
  - Holds `_models` dict, `_lock`, `_queue`, `_thread`, `_has_stopped`.
  - `__init__(model_config_list=None)` calls `load` if provided.
  - `load(model_config_list)`:
    - If `latest`: `version_policy='latest'`, `version_data=num_versions`, versions `1..num_versions`.
    - If `all`: `version_policy='latest'`, `version_data=None`, versions `[1]`.
    - Else: `version_policy='specific'`, `version_data=sorted(versions)`, versions per list.
    - Adds `ModelMeta` to `_models` and enqueues `Event(START)` per version.
  - `remove(model_name_list)`:
    - Marks model unloading and enqueues `UNLOADING` for each version.
  - `get_status(model_spec)`:
    - If model exists and no version_choice: returns status for all versions.
    - If `version` set: returns matching version.
    - If `version_label` set: returns matching label.
    - If none found: returns single status with `version=-1`, `NOT_FOUND`, message `{name} is not found`.
  - `get_metadata(model_spec, metadata_field)`:
    - For each requested field, pulls from `ModelConf` if present.
    - If `model_spec.version` set, overlays fields from matching `ModelVersion`.
  - `get_alive_model_names()` returns models not unloading.
  - `start()` spawns `_poll` thread; `stop()` sets `_has_stopped` and joins.
  - `_poll()`:
    - Processes queued events via `_event_handler`.
    - Every 30s: pick random model; if policy != `specific`, append new version (last+1) and enqueue `START`.
  - `_event_handler(event)` transitions:
    - `UNKNOWN -> START` (enqueue LOADING) -> `LOADING` (enqueue AVAILABLE) -> `AVAILABLE`.
    - On AVAILABLE: if policy `latest` and `len(versions) > version_data`, enqueue `UNLOADING` for oldest.
    - `UNLOADING` enqueues `END` (unless already UNLOADING/END).
    - `END` removes version; if none left remove model.
    - Logs transitions; logs error on missing model or unknown event.
- `ModelServiceImpl`:
  - `GetModelStatus` builds response with `model_mgr.get_status`.
  - `HandleReloadConfigRequest`:
    - `old_names = alive`, `new_names` from request configs.
    - Remove `old_names - new_names`, load configs for `new_names - old_names`.
    - Returns `ReloadConfigResponse` with `OK` status.
- `PredictionServiceImpl`:
  - `Predict` is no-op.
  - `GetModelMetadata`:
    - Uses `metadata_field = set(request.metadata_field)`.
    - For each metadata item: `Any(value=bytes(repr(v), 'utf-8'))` assigned to response.
- `FakeTFServing`:
  - `model_config_file` handling:
    - None: create `ModelMgr` from single `gen_model_config(model_name, base_path, version_data=num_versions)`.
    - str: parse pbtxt to `ModelServerConfig`, use config list.
    - else: expect `ModelServerConfig` instance.
  - Starts gRPC server with ModelService + PredictionService, binds `[::]:{port}`.
  - `start()` starts model_mgr and waits for termination; `stop()` stops server and model_mgr.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/tests/support/fake_tfserving.rs`.
- Rust public API surface: Fake gRPC server that emulates ModelService and PredictionService.
- Data model mapping: ModelVersionStatus, ModelSpec, ReloadConfigRequest, MetadataResponse.

**Implementation Steps (Detailed)**
1. Implement in-memory model manager with same state transitions and policies.
2. Implement gRPC services for ModelService and PredictionService.
3. Parse ModelServerConfig text/proto to initialize models.
4. Provide `start()`/`stop()` APIs for tests.

**Tests (Detailed)**
- Python tests: `monolith/agent_service/mocked_tfserving_test.py`
- Rust tests: ensure fake server supports metadata/status/reload as in Python.

**Gaps / Notes**
- Must preserve timing semantics for state transitions used by other tests.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/mocked_tfserving_test.py`
<a id="monolith-agent-service-mocked-tfserving-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 92
- Purpose/role: Tests FakeTFServing gRPC endpoints for metadata, status, and reload config.
- Key symbols/classes/functions: `MockedTFSTest` methods.
- External dependencies: gRPC stubs for ModelService and PredictionService.
- Side effects: starts FakeTFServing in background thread.

**Required Behavior (Detailed)**
- Class setup:
  - `MODEL_NAME='test_model_test'`, `BASE_PATH='/tmp/test_model/monolith'`.
  - `PORT = utils.find_free_port()`.
  - `Address = f'{socket.gethostbyname(socket.gethostname())}:{PORT}'`.
  - Start `FakeTFServing(MODEL_NAME, BASE_PATH, num_versions=2, port=PORT)` in a thread; `sleep(5)`.
- `test_get_model_metadata`:
  - Build `GetModelMetadataRequest` with `gen_model_spec(MODEL_NAME, 2, signature_name='predict')`.
  - `metadata_field` includes `base_path`, `num_versions`, `signature_name`.
  - Call `PredictionServiceStub.GetModelMetadata` and assert `GetModelMetadataResponse` type.
- `test_get_model_status`:
  - Build `GetModelStatusRequest` with `gen_model_spec(MODEL_NAME, 1, signature_name='predict')`.
  - Call `ModelServiceStub.GetModelStatus` and assert `GetModelStatusResponse` type.
- `test_handle_reload_config_request`:
  - Build `ReloadConfigRequest`, extend config list with two `gen_model_config` entries (same name `test_model`, different base paths/versions).
  - Call `HandleReloadConfigRequest` and assert `ReloadConfigResponse` type.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/tests/mocked_tfserving.rs`.
- Rust public API surface: FakeTFServing server for test; gRPC client stubs.

**Implementation Steps (Detailed)**
1. Recreate FakeTFServing server in Rust tests.
2. Add client requests for metadata/status/reload.
3. Assert response types and basic fields.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: parity tests for gRPC responses.

**Gaps / Notes**
- Ensure server thread lifecycle (start/stop) matches Python behavior.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/mocked_zkclient.py`
<a id="monolith-agent-service-mocked-zkclient-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 377
- Purpose/role: In-memory fake Kazoo/ZooKeeper client with watches, nodes, and basic CRUD.
- Key symbols/classes/functions: `ChildrenWatch`, `DataWatch`, `Election`, `Node`, `Catalog`, `FakeKazooClient`.
- External dependencies: `kazoo.protocol.states`, `kazoo.exceptions`.
- Side effects: in-memory state updates and watch callbacks.

**Required Behavior (Detailed)**
- Watch helpers:
  - `ChildrenWatch`: registers with catalog; `__call__` passes `(children,event)` if `send_event`, else `(children)`.
  - `DataWatch`: registers with catalog; `__call__` first tries `(data,state,event)` then falls back to `(data,state)` on `TypeError`.
  - `Election.run` executes callback under a lock; `cancel()` calls `self.lock.cancel()` (not supported by threading.Lock).
- `Node`:
  - Fields: `path`, `value`, `ephemeral`, `children`, `_ctime/_mtime` (unix seconds), `_version`.
  - On init: fires CREATED event to data/children watches if present.
  - `state` returns `ZnodeStat` with `dataLength=len(value)` and `numChildren=len(children)`.
  - `set(value)` updates mtime/version, triggers CHANGED event.
  - `set_data_watch`/`set_children_watch` immediately invoke watchers with current state and `event=None`.
  - `create_child(path, ...)`:
    - Computes child path from basename (root special-cased).
    - Creates `Node`, adds to children.
    - Triggers CHILD event on parent children watch.
  - `get_or_create_child`, `get_child`, `has_child` helpers.
  - `remove_child(path, recursive=False)`:
    - Raises `NotEmptyError` if child has children and `recursive` False.
    - Deletes child and triggers CHILD event; raises `NoNodeError` if missing.
  - `__del__`: fires DELETED event to data/children watches, deletes children, clears fields.
- `Catalog`:
  - Holds root `Node('/')`, watch registries, and `_sequence_paths`.
  - `add_data_watch`/`add_children_watch` attach to existing node if found.
  - `ensure_path(path)` creates nodes along path and attaches registered watches.
  - `create(path, value=b'', ephemeral=False, makepath=False, sequence=False)`:
    - If `sequence`: appends zero-padded 10-digit counter (starts at 0000000000).
    - If `makepath`: ensures parent path; else requires parent to exist.
    - Raises `NodeExistsError` if child exists.
  - `delete`, `set`, `get` delegate to nodes (raises `NoNodeError` if missing).
- `FakeKazooClient`:
  - `start()` initializes `Catalog`; `stop()` clears it.
  - CRUD: `create`, `delete(recursive=True)`, `set`, `get` (returns `(value,state)`), `exists`, `get_children`.
  - `ensure_path` delegates to catalog.
  - `retry` calls function directly.
  - `DataWatch`, `ChildrenWatch`, `Election` exposed via `partial`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/tests/support/fake_zk.rs`.
- Rust public API surface: fake ZK client with watches and CRUD for tests.

**Implementation Steps (Detailed)**
1. Implement Node tree with watch callbacks and version tracking.
2. Implement create/delete/set/get semantics and exceptions.
3. Support `sequence` numbering for `create`.
4. Provide compatible DataWatch/ChildrenWatch interfaces for tests.

**Tests (Detailed)**
- Python tests: `monolith/agent_service/mocked_zkclient_test.py`
- Rust tests: add unit tests for CRUD and watches.

**Gaps / Notes**
- Must mimic Kazoo event types and ordering as closely as possible.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/mocked_zkclient_test.py`
<a id="monolith-agent-service-mocked-zkclient-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 130
- Purpose/role: Tests FakeKazooClient CRUD and watch behaviors.
- Key symbols/classes/functions: `MockedZKClientTest` methods.
- External dependencies: `FakeKazooClient`, kazoo exceptions.
- Side effects: creates and deletes in-memory nodes; prints watch outputs.

**Required Behavior (Detailed)**
- Class setup/teardown:
  - `setUpClass`: `client = FakeKazooClient(); client.start()`.
  - `tearDownClass`: `client.stop()`.
- `test_create`:
  - `client.create('/monolith/zk/data', makepath=True)` returns same path.
  - Catches/ logs `NoNodeError` or `NodeExistsError`.
- `test_set_get`:
  - Create with `include_data=True`; expect `(path, state)`.
  - `set` on non-existent child raises `NoNodeError` (logged).
  - `get` on non-existent child raises `NoNodeError` (logged).
  - `get` on existing path returns original bytes.
- `test_delete`:
  - Create then `client.delete(path)` and `client.delete('/monolith')`.
- `test_data_watch`:
  - Create path, register `DataWatch(path, func)`; callback prints args.
- `test_children_watch`:
  - Register `ChildrenWatch('/monolith/zk', send_event=True)`; create child path.
  - Register `DataWatch` on `/monolith/zk/data`.
  - Create `/monolith/zk/test` to trigger watch callbacks.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/tests/mocked_zkclient.rs`.
- Rust public API surface: Fake ZK client and watch callbacks.

**Implementation Steps (Detailed)**
1. Port FakeKazooClient tests to Rust.
2. Assert correct errors for missing nodes.
3. Ensure watch callbacks are invoked with expected events.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: direct port with similar assertions.

**Gaps / Notes**
- Rust watch callback API may need adaptation; keep behavior parity.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/model_manager.py`
<a id="monolith-agent-service-model-manager-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 371
- Purpose/role: Copies latest model versions from a source (p2p) path to a receive path with lock/marker semantics and periodic refresh.
- Key symbols/classes/functions: `ModelManager` and its helpers (`start`, `loop_once`, `copy_model`, `get_source_data`, `remove_read_lock`).
- External dependencies: `os`, `shutil`, `threading`, `monolith.native_training.metric.cli`.
- Side effects: filesystem reads/writes, directory copies, lock files, optional metrics emission.

**Required Behavior (Detailed)**
- Constants: `WRITE_DONE=".write.done"`, `READ_LOCK=".read.lock"`.
- Defaults:
  - `_wait_timeout=1200s`, `_loop_interval=30s`, `_remain_version_num=5`.
- `start()` / `_start()`:
  - If `model_name` is None: log and return True.
  - Deletes receive path (`delete(receive_path)`).
  - `wait_for_download()` waits for `source_path` existence and a matching `model_name*.write.done` marker.
  - Runs `loop_once()` until a model copy succeeds; then `remove_read_lock()` and starts background thread.
- `run()`:
  - While not `_exist`:
    - `loop_once()` then `remove_read_lock()`.
    - Optionally `check_model_update_time()`; sleeps `loop_interval`.
    - `remove_old_file()` to trim versions.
- Lock files:
  - `create_read_lock(path)` creates `<path>.read.lock` by touching; adds to `_lock_files`.
  - `remove_read_lock()` deletes all `_lock_files` and any `*.read.lock` in source root.
- `wait_for_download()`:
  - Polls every 10s up to `wait_timeout`.
  - Requires a `.write.done` file with prefix `model_name`.
- `get_source_data()`:
  - Walks `source_path` root; collects `.write.done` files and model dirs `model@version`.
  - For each model dir: creates read lock, checks done file exists, parses `model@version`.
  - `real_path = root/model@version/model_name`.
  - `get_version_data(real_path, version)` returns list of `(sub_model/version, full_path)` for each subgraph.
  - Selects latest version by **string comparison** (`old_data[0] < version`).
  - Returns `{model_name: (version, version_data, real_path)}`.
- `copy_model(model_name, version, model_data)`:
  - For each `(sub_model/version, src_path)`:
    - `dst_file = receive_path/model_name/sub_model/version`.
    - Copy to `dst_file-temp` via `shutil.copytree`, then rename to `dst_file`.
    - If `dst_file` exists, counts as ready; if `dst_file-temp` exists, rename later.
  - If any copy fails or `ready_num != sub_model_num`, cleans temp dirs and returns False.
  - Returns `(True, [dst_file...])` on success.
- `loop_once()`:
  - If new version > old, calls `copy_model`; on success updates `_models` and `_latest_models[model]= (version, update_time)`.
- Metrics (`use_metrics=True`):
  - Prefix `data.monolith_serving.online`.
  - `check_model_update_time()` emits:
    - `version.delay = now - int(version)`.
    - `update.delay = now - update_time`.
  - If model missing in `_latest_models`, emits counter `loop_once_failed`.
- `remove_old_file()`:
  - Keeps newest `_remain_version_num` per model; deletes older file paths via `delete()`.
- `delete(path)`:
  - Removes file or directory; logs error on failure.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/src/model_manager.rs`.
- Rust public API surface: `ModelManager` with `start/stop` and loop helpers.
- Data model mapping: filesystem-only (no TF dependencies).

**Implementation Steps (Detailed)**
1. Port lock/marker semantics and path layout.
2. Implement `copy_model` with temp dirs + atomic rename.
3. Preserve polling intervals/timeouts and string-based version comparison.
4. Port metrics emission (feature-gated) with matching names.

**Tests (Detailed)**
- Python tests: `monolith/agent_service/model_manager_test.py`
- Rust tests: build temp tree; verify latest version selection and ignore-old behavior.

**Gaps / Notes**
- `remove_read_lock` uses `os.join` (likely typo) when deleting stray locks.
- Version comparison is string-based, not numeric.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/model_manager_test.py`
<a id="monolith-agent-service-model-manager-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 113
- Purpose/role: Tests ModelManager copy behavior and ignores older versions.
- Key symbols/classes/functions: `ModelManagerTest.create_file`, `test_start`, `test_ignore_old`.
- External dependencies: filesystem, `ModelManager`.
- Side effects: creates/deletes temp directories under `TEST_TMPDIR`.

**Required Behavior (Detailed)**
- `create_file(model_name, timestamp, p2p_data_path)`:
  - Creates directories:
    - `p2p/<model>@<ts>/<model>/ps_item_embedding_0/<ts>`
    - `p2p/<model>@<ts>/<model>/ps_item_embedding_1/<ts>`
  - Writes marker `<model>@<ts>.write.done` in `p2p`.
- `test_start`:
  - Creates one version (`1234567`), starts ModelManager.
  - Sets `_wait_timeout=5`, `_loop_interval=5`.
  - Asserts copied directories exist under `model_data/<model>/ps_item_embedding_*`.
  - Stops manager and removes paths.
- `test_ignore_old`:
  - Creates newer version (`1234567`) and starts manager.
  - Adds an older version (`1234566`) afterward; sleeps 11s.
  - Asserts older version dirs do **not** exist under receive path.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/tests/model_manager.rs`.
- Rust public API surface: `ModelManager` with configurable wait/intervals.

**Implementation Steps (Detailed)**
1. Port `create_file` helper to build p2p layout + .write.done.
2. Validate successful copy of newest version.
3. Ensure older version is ignored after manager is running.

**Tests (Detailed)**
- Python tests: `ModelManagerTest.test_start`, `.test_ignore_old`.
- Rust tests: parity tests with temp dirs and shortened intervals.

**Gaps / Notes**
- Ensure Rust tests use temp directories and cleanup to avoid flakiness.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/replica_manager.py`
<a id="monolith-agent-service-replica-manager-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 835
- Purpose/role: Maintains replica registration and status updates in ZK, watches for replica changes, and exposes lookup APIs for serving + parameter sync.
- Key symbols/classes/functions: `ReplicaWatcher`, `ReplicaUpdater`, `ZKListener`, `ReplicaManager`, `SyncBackendWrapper`.
- External dependencies: `kazoo`, `TFSMonitor`, `AgentConfig`, `ReplicaMeta`, `ModelState`, `ClientConfig.TargetExtraInfo`, metrics CLI.
- Side effects: ZK watches + ephemeral nodes, periodic polling, metrics emission.

**Required Behavior (Detailed)**
- `ReplicaWatcher`:
  - Chooses `_zk_watch_address_family`:
    - If requested IPv4 but `is_ipv6_only()` returns True, switches to IPv6.
  - `path_prefix = /{bzid}/service/{base_name}`.
  - `watch_data()`:
    - DC-aware: watch `path_prefix` for `idc:cluster`, then tasks, then replicas.
    - Non-DC: watch tasks directly.
    - Starts `_poll` thread (daemon).
  - `DataWatch` handler:
    - If data empty: mark stat UNKNOWN if existing.
    - Event `CREATED`/None: add/update.
    - `CHANGED`: update.
    - `DELETED`: remove replica; delete task path if empty.
    - `NONE`: set UNKNOWN.
  - `_poll()` every 60s:
    - Rebuilds `replicas_tmp` by scanning ZK.
    - Computes removed replicas; for PS/DENSE deploys, re-registers missing local replicas with correct grpc/archon ports.
    - Updates `self.replicas = replicas_tmp`.
  - Lookup helpers:
    - `get_all_replicas`: returns `{task -> [addr...]}` for AVAILABLE replicas; dc-aware key includes `idc/cluster`.
    - `get_replicas`: returns list for specific task.
    - `get_replica`: returns single addr or list or None.
    - `get_replicas_with_extra_info`: returns `{addr -> TargetExtraInfo(idc, cluster, replica_id)}`.
  - `to_sync_wrapper()`: returns `SyncBackendWrapper`.
- `ReplicaUpdater`:
  - Tracks `meta` map of `replica_path -> ReplicaMeta`.
  - `model_names` includes:
    - `ps_{task_id}` for PS shards owned by this shard id.
    - `entry` if deploy_type MIXED or ENTRY.
    - `dense_0` if dense_alone and deploy_type MIXED or DENSE.
  - `_do_register(replica_path, grpc_port, archon_port)`:
    - Builds host from `MY_HOST_IP` or hostname; IPv6 from `MY_HOST_IPV6` or getaddrinfo.
    - Creates ReplicaMeta with UNKNOWN state and addresses (ipv4/ipv6 + archon).
    - Creates ephemeral znode; for ENTRY with replica_id == -1 uses `sequence=True` and updates config.replica_id.
    - If node exists, updates value if different.
  - `register()`:
    - Registers entry, ps shards, and dense based on deploy_type and shard assignment.
  - `_do_update(name)`:
    - Calls `TFSMonitor.get_model_status(name)`.
    - If error: set stat UNKNOWN and update/create znode.
    - Else: select latest AVAILABLE version (or latest version); if error_code != OK, raise.
    - If state changed, update znode.
  - `_updater()`:
    - Loops every 1s; skips if `_should_update` is False.
    - Updates all `model_names`; logs exceptions with traceback.
  - `_check_version()`:
    - Emits metrics:
      - `serving_model.latest_version`
      - `serving_model.since_last_update` (global metric)
      - `serving_model.update_ts`
    - Tags include model_name, idc/cluster, replica_id, shard_id, base_name.
  - `_watch_update()` runs `_check_version()` every 60s.
  - `_reregister()` every 10s: if `_should_reregister` True, calls `register()` and sets `_should_update=True`.
  - `start()` starts TFSMonitor and threads; `stop()` joins threads and clears meta.
- `ZKListener`:
  - On `LOST`: disables watcher polling + updater updates, sets `_has_lost`.
  - On reconnect after LOST: sets `_should_reregister=True`, sleeps 5s, re-enables polling.
- `ReplicaManager`:
  - Wires watcher + updater and registers ZKListener.
  - `start()`: `updater.register()`, `watcher.watch_data()`, `updater.start()`.
  - `stop()`: stops updater then watcher.
  - `is_ps_set_started()` ensures each PS task has at least one AVAILABLE replica; `is_dense_set_started()` checks dense replicas when enabled.
- `SyncBackendWrapper`:
  - `subscribe_model` stores model name.
  - `get_sync_targets("ps_i")` returns `(sub_graph, watcher.get_replicas_with_extra_info(...))`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/src/replica_manager.rs`.
- Rust public API surface: `ReplicaWatcher`, `ReplicaUpdater`, `ReplicaManager`, `SyncBackendWrapper`.
- Data model mapping: `ReplicaMeta`, `ModelState`, `TFSServerType`, `ServerType`, `ClientConfig.TargetExtraInfo`.
- Integration points: ZK client, TFSMonitor, metrics client.

**Implementation Steps (Detailed)**
1. Implement ZK watchers and periodic polling reconciliation.
2. Port registration/update logic with identical address selection and replica_id sequencing.
3. Port metrics emission with same prefixes/tags and cadence.
4. Implement ZK LOST handling and re-registration semantics.
5. Provide sync backend wrapper returning extra-info targets.

**Tests (Detailed)**
- Python tests: `monolith/agent_service/replica_manager_test.py`
- Rust tests: fake ZK + fake TFS server to validate registration, updates, and watcher maps.

**Gaps / Notes**
- `get_replicas_with_extra_info` returns a dict despite type hint List[str].
- Uses environment variables (`MY_HOST_IP`, `MY_HOST_IPV6`, `MONOLITH_METRIC_PREFIX`, `TCE_PSM`) for address/metrics.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/src/replica_manager.rs`.
- Rust public API surface: `ReplicaManager`, `ReplicaWatcher`, `ReplicaUpdater`, `SyncBackendWrapper`.
- Data model mapping: `ReplicaMeta`, `ModelState`, `TFSServerType`, `ServerType`.
- Integration points: ZK client + TFSMonitor + metrics client.

**Implementation Steps (Detailed)**
1. Implement watcher with ZK watches and periodic polling for reconciliation.
2. Implement updater with registration, status updates, and metrics.
3. Port dc-aware path parsing (`ZKPath`) and address family handling.
4. Port connection loss handling and re-registration semantics.
5. Provide SyncBackend wrapper for parameter sync features.

**Tests (Detailed)**
- Python tests: `monolith/agent_service/replica_manager_test.py`
- Rust tests: fake ZK + fake TFS server to validate registration and updates.

**Gaps / Notes**
- This module depends on TFSMonitor (gRPC) and ZK; ensure fakes are available for tests.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/replica_manager_test.py`
<a id="monolith-agent-service-replica-manager-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 126
- Purpose/role: Sets up FakeTFServing and helper registration for ReplicaManager tests; no actual test cases.
- Key symbols/classes/functions: `ReplicaMgrTest.setUpClass`, `register`.
- External dependencies: `FakeTFServing`, `FakeKazooClient`, `ReplicaMeta`, `AgentConfig`.
- Side effects: starts FakeTFServing servers for entry/ps in background threads.

**Required Behavior (Detailed)**
- `setUpClass`:
  - Sets env vars:
    - `MONOLITH_HOST_SHARD_N=5`, `SHARD_ID=1`, `REPLICA_ID=2`,
      `TCE_INTERNAL_IDC=lf`, `TCE_LOGICAL_CLUSTER=default`.
  - Builds `AgentConfig` with:
    - `bzid='bzid'`, `base_name=MODEL_NAME`, `deploy_type='mixed'`,
      `base_path=BASE_PATH`, `num_ps=20`, `num_shard=5`, `dc_aware=True`.
  - Extracts `model_config_file` path from `agent_conf.get_cmd(...)` for ENTRY and PS.
  - Starts two `FakeTFServing` instances (entry + ps) in background threads.
- `tearDownClass`:
  - Stops FakeTFServing servers and joins threads.
- `register(zk)` helper:
  - Creates ReplicaMeta entries for all shard/replica combos except current shard/replica.
  - Adds ps task replicas (`ps:{task_id}/{replica_id}`) and entry replicas (`entry:0/{replica_id}`).
  - Uses `find_free_port()` for each address and creates ephemeral nodes in ZK.
- Note: No actual `test_*` methods are implemented beyond helper setup.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/tests/replica_manager.rs`.
- Rust public API surface: FakeTFServing + ReplicaManager registration utilities.
- Data model mapping: same path formatting and ReplicaMeta JSON encoding.

**Implementation Steps (Detailed)**
1. Port setup/teardown as a test fixture in Rust.
2. Implement `register` helper to populate fake ZK.
3. Add real assertions in Rust tests (missing in Python).

**Tests (Detailed)**
- Python tests: none (setup only).
- Rust tests: verify watcher/updater interactions with fake TFS and ZK nodes.

**Gaps / Notes**
- Python test file lacks assertions; Rust should add coverage for lookups and updates.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/resource_utils.py`
<a id="monolith-agent-service-resource-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 269
- Purpose/role: Resource and model-size utilities (memory, CPU, model size computation, HDFS helpers).
- Key symbols/classes/functions: `cal_model_info_v2`, `total_memory`, `cal_available_memory`, `CPU`, `num_cpu`, `cal_cpu_usage`.
- External dependencies: `tensorflow.io.gfile`, `psutil`, `subprocess`, `export_state_utils`.
- Side effects: reads cgroup files, runs shell commands, reads filesystem/HDFS.

**Required Behavior (Detailed)**
- `_get_pod_cgroup_path()`:
  - Runs `cat /proc/1/cgroup`, finds line containing `:memory:`, returns trailing path without slashes.
  - On error returns `None`.
  - `_POD_CGROUP_PATH` computed at import time.
- `exists(dirname)` -> `tf.io.gfile.isdir(dirname) or tf.io.gfile.exists(dirname)`.
- `open_hdfs(fname)`:
  - Builds cmd `[_HADOOP_BIN, 'fs', '-text', ...]` (global `_HADOOP_BIN` expected).
  - Accepts string or list/tuple of paths.
  - Retries up to 3 times; logs exceptions; asserts output list is not None.
  - Yields non-empty stripped lines.
- `cal_model_info_v2(exported_models_path, ckpt=None, version=None)`:
  - Normalizes path to absolute (rstrip `/`), requires `tf.io.gfile.exists`.
  - Initializes `model_info` with each sub_model under export dir (excluding dotfiles) size 0.
  - If `ckpt` None: uses `tf.train.get_checkpoint_state(ckpt_base_path)` and basename of `model_checkpoint_path`.
  - `global_step = -1` if ckpt None else `int(ckpt.split('-')[-1])`.
  - If `version` None:
    - For each sub_model, attempt `export_state_utils.get_export_saver_listener_state(tfs_base_path)`; if state and `global_step>=0`, collect versions from entries and pick matching `global_step`.
    - Else list numeric subdirs under sub_model.
    - Intersect versions across sub_models; pick max if `version` still None.
  - Else cast `version` to int.
  - For each sub_model: sum all file sizes under `exported_models_path/sub_model/version` via `tf.io.gfile.walk`.
  - If assets dir `ckpt_base_path/{ckpt}.assets` exists:
    - For files matching `ROW` regex, add size to `model_info['ps_{index}']`.
  - Returns `{sub_model_name: (size, version_path)}`.
- Memory helpers:
  - `total_memory` reads `/sys/fs/cgroup/memory/{_POD_CGROUP_PATH}/memory.limit_in_bytes`; if 0 uses `MY_MEM_LIMIT`.
  - `total_memory_v2` uses `psutil.virtual_memory().total`.
  - `cal_available_memory` reads cgroup `memory.usage_in_bytes` and `memory.limit_in_bytes`, returns limit - usage.
  - `cal_available_memory_v2` uses `psutil.virtual_memory().available`.
- CPU helpers:
  - `CPU.wall_clock()` uses `time.time_ns()` (fallback to `date +%s%N` subprocess).
  - `CPU.cpu_clock()` reads cpuacct usage from file.
  - `CPU.cpu_usage()` returns delta_cpu/delta_wall and updates stored clocks.
  - `num_cpu()` reads `cpu.cfs_quota_us` and `cpu.cfs_period_us`, fallback to `MY_CPU_LIMIT` if period 0.
  - `cal_cpu_usage()` samples 5 times, 1s sleep, averages `round(cpu_usage*100,2)`.
  - `cal_cpu_usage_v2()` uses `psutil.cpu_percent()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/src/resource_utils.rs`.
- Rust public API surface: functions for model size + resource metrics.
- Data model mapping: `SubModelName`, `SubModelSize`, `VersionPath` equivalents.

**Implementation Steps (Detailed)**
1. Implement cgroup parsing for memory/cpu or provide platform-specific fallbacks.
2. Port `cal_model_info_v2` using filesystem traversal and checkpoint state parsing.
3. Replace `tf.io.gfile` with Rust FS/HDFS abstraction as needed.
4. Provide `psutil`-equivalent data via `sysinfo` or `/proc` parsing.

**Tests (Detailed)**
- Python tests: `monolith/agent_service/resource_utils_test.py`
- Rust tests: validate memory/cpu functions and model info on fixture dirs.

**Gaps / Notes**
- Requires HDFS support or a stub for `open_hdfs`.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/resource_utils_test.py`
<a id="monolith-agent-service-resource-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 36
- Purpose/role: Tests resource utility functions (memory/cpu).
- Key symbols/classes/functions: `UtilTest.test_cal_avaiable_memory_v2`, `.test_cal_cpu_usage_v2`
- External dependencies: `resource_utils`.
- Side effects: reads system stats.

**Required Behavior (Detailed)**
- `test_cal_avaiable_memory_v2`:
  - Calls `total_memory_v2()` and `cal_available_memory_v2()`; asserts `0 < available < total`.
- `test_cal_cpu_usage_v2`:
  - Calls `cal_cpu_usage_v2()`; asserts `0 <= usage <= 100`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/tests/resource_utils.rs`.
- Rust public API surface: resource utility functions.

**Implementation Steps (Detailed)**
1. Port tests with `sysinfo` or `/proc` sources.
2. Ensure numeric bounds match Python behavior.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: same assertions.

**Gaps / Notes**
- Tests may be flaky on constrained CI; consider tolerances.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/run.py`
<a id="monolith-agent-service-run-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 39
- Purpose/role: Entry-point multiplexer for `agent`, `agent_client`, and `tfs_client` binaries.
- Key symbols/classes/functions: `main`
- External dependencies: `absl.app`, `flags`, agent and client entrypoints.
- Side effects: dispatches into selected CLI.

**Required Behavior (Detailed)**
- Flag `bin_name` selects:
  - `agent` -> `monolith.agent_service.agent.main`
  - `agent_client` -> `monolith.agent_service.agent_client.main`
  - `tfs_client` -> `monolith.agent_service.tfs_client.main`
- Unknown value raises `ValueError`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-cli/src/bin/monolith.rs` (dispatcher) or separate binaries.
- Rust public API surface: CLI dispatch or multi-bin setup.

**Implementation Steps (Detailed)**
1. Decide whether to provide a dispatcher binary or separate `cargo` bins.
2. If dispatcher, implement `bin_name` flag with same choices.
3. Forward args to target subcommand.

**Tests (Detailed)**
- Python tests: none
- Rust tests: optional CLI smoke test.

**Gaps / Notes**
- Rust may prefer multiple bins; document any deviations.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/svr_client.py`
<a id="monolith-agent-service-svr-client-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 70
- Purpose/role: Thin gRPC client wrapper for AgentService (heart_beat and get_replicas).
- Key symbols/classes/functions: `SvrClient`, `heart_beat`, `get_replicas`
- External dependencies: `grpc`, `AgentServiceStub`, `AgentConfig`.
- Side effects: gRPC calls, prints responses.

**Required Behavior (Detailed)**
- `__init__`: accepts config path or `AgentConfig` object; defers stub creation.
- `stub` property:
  - Uses `MY_HOST_IP` env or local hostname; connects to `{host}:{agent_port}`.
- `get_server_type`:
  - If input is string, maps `ps/entry/dense` to enum using `FLAGS.server_type` (note: uses global flags).
- `heart_beat`: sends `HeartBeatRequest` and prints addresses.
- `get_replicas`: sends `GetReplicasRequest` and prints address list.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-cli/src/agent_svr_client.rs` or library module.
- Rust public API surface: `SvrClient` with `heart_beat` and `get_replicas`.

**Implementation Steps (Detailed)**
1. Implement gRPC stub creation with env host selection.
2. Port enum mapping for server types.
3. Preserve stdout printing behavior (for CLI usage).

**Tests (Detailed)**
- Python tests: none
- Rust tests: add gRPC stub tests using a fake AgentService.

**Gaps / Notes**
- The string mapping uses global FLAGS; ensure Rust CLI has equivalent.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/tfs_client.py`
<a id="monolith-agent-service-tfs-client-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 503
- Purpose/role: CLI for TensorFlow Serving status/metadata/load/predict/profile; includes data generation and format conversion.
- Key symbols/classes/functions: `read_header`, `read_data`, `generate_random_instance`, `generate_random_example_batch`, `get_instance_proto`, `get_example_batch_proto`, `gen_random_file`, `get_example_batch_proto_v2`, `get_example_batch_to_instance`, `ProfileThread`, `main`.
- External dependencies: TF Serving protos, matrix `ExampleBatch`/`Instance`, `FeatureList`, `data_gen_utils`.
- Side effects: reads/writes files, makes gRPC requests, spawns threads, prints output.

**Required Behavior (Detailed)**
- Flags/global constants:
  - Flags: `signature_name="serving_default"`, `feature_list=None`, `file_type="pb"`, `batch_size=8`, `lagrangex_header=False`, `has_sort_id=False`, `kafka_dump=False`, `kafka_dump_prefix=False`, `parallel_num=1`, `profile_duration=600`, `profile_data_dir=None`.
  - Globals: `VALID_SLOTS=[]`, `_NUM_SLOTS=6`, `_VOCAB_SIZES=[5,5,5,5,5,5]`, `SKIP_LIST` set.
- Input parsing helpers:
  - `read_header(stream)`:
    - `int_size=8`.
    - If `FLAGS.lagrangex_header`, read 8 bytes and return.
    - Else:
      - `aggregate_page_sortid_size=0`.
      - If `FLAGS.kafka_dump_prefix`:
        - Read `<Q` size; if size==0 read another `<Q`; else set `aggregate_page_sortid_size=size`.
      - If `FLAGS.has_sort_id`:
        - If `aggregate_page_sortid_size==0`, read `<Q` size; else use `aggregate_page_sortid_size`.
        - Read `size` bytes (sort_id payload).
      - If `FLAGS.kafka_dump`, read 8 bytes.
  - `read_data(stream)`: calls `read_header`, reads `<Q` size (8 bytes) then reads and returns that many bytes.
- Data generation:
  - `generate_random_instance(slots=None, vocab_sizes=_VOCAB_SIZES)`:
    - If slots None: `slots = [1..len(_VOCAB_SIZES)]`.
    - `max_vocab = max(vocab_sizes)`.
    - Build `fids = (slot << 54) | (i * max_vocab + rand(1, vocab_sizes[i]))` for each `i, slot` and repeat `vocab_sizes[i]` times.
    - Return serialized `Instance` with `fid` populated.
  - `generate_random_example_batch(feature_list, batch_size=256)`:
    - Creates `ExampleBatch` with `batch_size`.
    - Skips features whose name contains any token in `SKIP_LIST`.
    - Only processes features with `"_id"` or `"_name"` in name.
    - For each kept feature:
      - Adds `named_feature_list` with `name=feature.name`.
      - For each example in batch:
        - If `feature.method` startswith `vectortop` and `feature.args[0]` is numeric:
          - `num=int(args[0])`; if `num>0`, replace with `randint(1, num)`.
          - Create `num` fids `(feature.slot << 48) | rand(1, sys.maxsize-1)` into `fid_v2_list.value`.
        - Else: single fid `(feature.slot << 48) | rand(1, (1<<48)-1)` into `fid_v2_list.value`.
    - Adds `__LINE_ID__` feature list:
      - For each example: `LineId(sample_rate=0.001, req_time=now_ts - rand(1,1000), actions=[rand(1,3), rand(3,5)])` serialized into `bytes_list.value`.
    - Returns serialized `ExampleBatch`.
  - `gen_random_file(input_file, variant_type="example_batch")`:
    - Asserts `input_file` not None and `VALID_SLOTS` non-empty.
    - Builds `ParserArgs` with sparse_features from `VALID_SLOTS`, extra_features list, shapes, `batch_size=FLAGS.batch_size`, `variant_type`.
    - Uses `data_gen_utils.gen_random_data_file(..., sort_id=FLAGS.has_sort_id, kafka_dump=FLAGS.kafka_dump, num_batch=1, actions=[1..12])`.
- Tensor proto conversion:
  - `get_instance_proto(input_file=None, batch_size=256)`:
    - If `input_file` None: generate `batch_size` random instances.
    - Else: read `batch_size` instances via `read_data` and parse `Instance` per record.
    - Return `utils.make_tensor_proto(instances)` (list of serialized bytes).
  - `get_example_batch_proto(input_file=None, feature_list=None, batch_size=256, file_type='pb')`:
    - If no file: generate random ExampleBatch.
    - Else parse ExampleBatch from pb (`read_data`) or pbtxt (`text_format.Parse`).
    - Return `utils.make_tensor_proto([example_batch])`.
  - `get_example_batch_proto_v2(input_file)`:
    - If file missing, call `gen_random_file`.
    - Parse ExampleBatch from `read_data`.
    - `user_fname_set = {get_feature_name_and_slot(slot)[0] for slot in user_features}`.
    - For each `named_feature_list` with name in set:
      - Set `type = FeatureListType.SHARED`.
      - Replace feature list with `batch_size` copies of the first feature.
    - Return `utils.make_tensor_proto([serialized ExampleBatch])`.
  - `get_example_batch_to_instance(input_file, file_type)`:
    - Parse ExampleBatch from pb or pbtxt.
    - For each example index, create `Instance`:
      - Use shared feature if `named_feature_list.type == SHARED`.
      - Map `__LABEL__` to `inst.label` (float_list).
      - Map `__LINE_ID__` bytes to `inst.line_id.ParseFromString`.
      - For other features, select first non-empty value list in order:
        - `fid_v1_list`: compute `slot_id = fid >> 54`; convert each fid to v2 with `(slot_id << 48) | (mask & v)` where `mask=(1<<48)-1`.
        - `fid_v2_list`: copy.
        - `float_list` or `double_list`: copy into `float_value`.
        - `int64_list`: copy into `int64_value`.
        - `bytes_list`: copy into `bytes_value`.
    - Return `utils.make_tensor_proto(inst_list)` (serialized instances).
- Profiling thread:
  - `ProfileThread.run()` loops until `int(time.time()) - run_st < repeat_time`:
    - Job 0 logs progress every 60 seconds (`show_count` gate).
    - Builds PredictRequest with signature name flag, picks random cached example_batch and random stub.
    - Predict timeout = 30s.
    - Records per-request latency (ms) into list, increments count.
    - On exception: logs warning and continues.
  - `get_result()` joins thread and returns latency list.
- CLI (`main`):
  - Calls `enable_tob_env()` and `env_utils.setup_host_ip()`.
  - Loads `agent_conf = AgentConfig.from_file(FLAGS.conf)`.
  - `host = env["MY_HOST_IP"]` or `socket.gethostbyname(socket.gethostname())`.
  - Default `model_name` if flag unset:
    - PS: `ps_{shard_id}`; DENSE: `TFSServerType.DENSE`; else ENTRY.
  - `target` by deploy_type: entry/ps/dense port; allows override `FLAGS.target` and comma-separated targets.
  - Creates `channel_list` from targets, `channel` for first target.
  - `cmd_type` behaviors:
    - `status`: `ModelServiceStub.GetModelStatus` for model name/signature; print response.
    - `meta`: `PredictionServiceStub.GetModelMetadata`, metadata fields `base_path`, `num_versions`, `signature_name`; print response.
    - `load`: parse `ModelServerConfig` from pbtxt `FLAGS.input_file`, `HandleReloadConfigRequest`, log "load done", return status.
    - `profile`:
      - Assert `VALID_SLOTS` non-empty; ensure `profile_data_dir` exists.
      - Build list of up to 500 data files; generate missing via `gen_random_file` (uuid filenames).
      - Load each file with `get_example_batch_proto_v2`.
      - Spawn `parallel_num` `ProfileThread`s; gather latency list.
      - Compute avg latency, p99 (index `round((n-1)*0.99)`), QPS; log summary.
    - Default (`get`): build PredictRequest; choose inputs:
      - `input_type == "instance"`: if file missing, `gen_random_file(..., "instance")`; `get_instance_proto`.
      - `input_type == "example_batch"`: use provided file or new uuid, `get_example_batch_proto_v2`.
      - Else: `get_example_batch_to_instance`.
      - Call `Predict(timeout=30)` and print response.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-cli/src/bin/tfs_client.rs`.
- Rust public API surface: CLI entrypoint + helper functions for data parsing and proto conversion.
- Data model mapping: matrix ExampleBatch/Instance protos + TF Serving PredictRequest.
- Feature gating: `tf-runtime` and `matrix-protos` features.

**Implementation Steps (Detailed)**
1. Port file header parsing and size-prefixed record reading.
2. Implement ExampleBatch/Instance generation and conversion logic.
3. Implement gRPC clients for ModelService and PredictionService.
4. Port profiling logic with multi-threaded request loop and latency stats.
5. Mirror CLI flags and defaults exactly.

**Tests (Detailed)**
- Python tests: `monolith/agent_service/tfs_client_test.py`
- Rust tests: port get_instance_proto and get_example_batch_to_instance tests.

**Gaps / Notes**
- `user_features` referenced in `get_example_batch_proto_v2` is not defined in this module; Rust port must identify the intended source or expose configuration.
- `VALID_SLOTS` defaults empty; profile path asserts it is non-empty (likely set via external config/feature list).
- `_NUM_SLOTS` is unused in this module; `_VOCAB_SIZES` governs random instance generation.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/tfs_client_test.py`
<a id="monolith-agent-service-tfs-client-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 50
- Purpose/role: Tests tensor proto generation and ExampleBatch-to-Instance conversion.
- Key symbols/classes/functions: `TFSClientTest.test_get_instance_proto`, `.test_get_example_batch_to_instance_*`
- External dependencies: `tfs_client` helpers.
- Side effects: reads test data files.

**Required Behavior (Detailed)**
- `test_get_instance_proto`:
  - Calls `get_instance_proto()` with default `batch_size=256`.
  - Asserts `tensor_proto.dtype == 7` and `tensor_shape.dim[0].size == 256`.
- `test_get_example_batch_to_instance_from_pb`:
  - Uses file `monolith/native_training/data/training_instance/examplebatch.data`.
  - Sets `FLAGS.lagrangex_header = True`.
  - Calls `get_example_batch_to_instance(file, 'pb')`.
- `test_get_example_batch_to_instance_from_pbtxt`:
  - Uses file `monolith/agent_service/example_batch.pbtxt`.
  - Sets `FLAGS.lagrangex_header = True`.
  - Calls `get_example_batch_to_instance(file, 'pbtxt')`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-cli/tests/tfs_client.rs`.
- Rust public API surface: helper functions for instance/example batch parsing.

**Implementation Steps (Detailed)**
1. Port tests using fixture files (`examplebatch.data`, `example_batch.pbtxt`).
2. Assert dtype and shapes match Python behavior.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: parity tests for parsing and tensor construction.

**Gaps / Notes**
- Ensure Rust uses the same byte-order and header handling as Python.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/tfs_monitor.py`
<a id="monolith-agent-service-tfs-monitor-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 303
- Purpose/role: gRPC monitor for TensorFlow Serving model status and config reload.
- Key symbols/classes/functions: `TFSMonitor`, `get_model_status` (singledispatch), `gen_model_config`, `handle_reload_config_request`.
- External dependencies: TF Serving gRPC stubs, `ModelServerConfig`, `PublishMeta`, `StatusProto`.
- Side effects: gRPC calls to TFS servers; opens/closes channels.

**Required Behavior (Detailed)**
- Host selection:
  - `host` property: if unset or localhost, resolves to `get_local_ip()`.
- Stubs:
  - `connect()` creates `grpc.insecure_channel` + `ModelServiceStub` and `PredictionServiceStub` per deploy_type:
    - ENTRY: `tfs_entry_port`
    - PS: `tfs_ps_port`
    - DENSE: `tfs_dense_port` (only if dense_alone).
  - `start()` resets stubs and calls `connect()`.
  - `stop()` closes channels and clears stubs.
- Address selection:
  - `get_addr(sub_model_name)` chooses port based on deploy_type and sub_model.
  - `get_service_type(sub_model_name)` maps to `TFSServerType` or None.
- `get_model_status(PublishMeta, fix_dense_version=False)`:
  - For each sub_model in `pm.sub_models`:
    - Builds `tfs_model_name = f"{model}:{sub_model}"`.
    - Dense node detection:
      - If not dense_alone and entry → dense node.
      - If dense_alone and dense → dense node.
    - If dense node and `fix_dense_version=False`, request **no version** (latest).
    - Else request specific version = basename of version path.
  - If no statuses returned: create `ModelVersionStatus` with `state=UNKNOWN` and `StatusProto(error_code=NOT_FOUND)`.
  - If multiple statuses: sort by version and select latest (or AVAILABLE if present).
  - On `_InactiveRpcError`: returns UNKNOWN with StatusProto error_code from `e.code().value[0]`.
  - Returns `{tfs_model_name: (version_path, ModelVersionStatus)}`.
- `get_model_status(name, version=None, signature_name=None)` overload:
  - Determines service_type via `get_service_type`.
  - If None, returns empty list.
  - Else sends `GetModelStatusRequest` and returns list of ModelVersionStatus.
- `gen_model_config(pms, fix_dense_version=False)`:
  - Builds `ModelServerConfig` per service type.
  - Skips `PublishMeta` where `ptype == UNLOAD`.
  - Dense nodes:
    - version_policy = `latest` if not fix_dense_version; else `specific`.
    - version_data = `1` if latest, else version basename.
  - Non-dense nodes: `specific` version policy with basename.
  - Appends configs into appropriate service type list.
- `handle_reload_config_request(service_type, model_configs)`:
  - Ensures `DEFAULT_MODEL_CONFIG` is present if missing.
  - Sends `HandleReloadConfigRequest` to appropriate stub; returns `StatusProto`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/src/tfs_monitor.rs`.
- Rust public API surface: `TfsMonitor` with status query + reload APIs.
- Data model mapping: TF Serving protos, `PublishMeta`, `ModelServerConfig`.

**Implementation Steps (Detailed)**
1. Port gRPC client setup for entry/ps/dense.
2. Implement overload semantics for `get_model_status`.
3. Port dense-node version policy logic (latest vs specific).
4. Add default model config injection on reload.

**Tests (Detailed)**
- Python tests: `monolith/agent_service/tfs_monitor_test.py`
- Rust tests: fake TFS servers; verify reload responses + status selection.

**Gaps / Notes**
- Error code mapping uses `e.code().value[0]` (non-obvious; must preserve).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/tfs_monitor_test.py`
<a id="monolith-agent-service-tfs-monitor-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 182
- Purpose/role: Tests TFSMonitor reload and remove config with FakeTFServing.
- Key symbols/classes/functions: `TFSMonitorTest.test_reload_config`, `.test_remove_config`
- External dependencies: FakeTFServing, ModelServerConfig, PublishMeta.
- Side effects: starts fake TF serving servers.

**Required Behavior (Detailed)**
- Class setup:
  - Set env vars: `HOST_SHARD_ENV=10`, `SHARD_ID=1`, `REPLICA_ID=2`.
  - Build `AgentConfig(bzid='bzid', deploy_type='mixed')`.
  - Start `FakeTFServing` for entry and ps with `num_versions=2`, ports from config, `ModelServerConfig()`.
  - Start servers in threads, `sleep(2)` for readiness.
  - Create `TFSMonitor`, call `connect()`, init `data = {}`.
- Class teardown: `monitor.stop()`, stop both fake servers.
- `setUp`:
  - Build `sub_models` dict with `entry`, `ps_0`, `ps_3`, `ps_5` pointing at `path.format(sub_model, version)`.
  - `PublishMeta(model_name='test_1', num_ps=5, shard_id/replica_id from config)`.
  - Save `data['setUp'] = monitor.get_model_status(pm)`.
- `tearDown`:
  - Rebuild same `PublishMeta`, `sleep(1)`.
  - Compare `before_status` vs `after_status = monitor.get_model_status(pm)`; assert same length.
  - If `data['execute'] == 'reload_config'`:
    - For each model, ensure version path unchanged.
    - `before` status must be `version == -1` and `error_code == 5` (NOT_FOUND).
    - `after` status cases:
      - `version == -1`: allowed (no extra check).
      - `version == 1`: only for `entry` model names.
      - else `version == int(os.path.basename(version_path))`.
  - Else (remove_config):
    - For each model, `after.version == -1`, version path unchanged.
    - If `before.version == -1`, `before.status.error_code == 5` (NOT_FOUND).
    - Else `before.version > 0`.
- `test_reload_config`:
  - For `i in range(10)`:
    - `num_ps = random.randint(5, 20)`.
    - `sub_models` includes `ps_{j}` for `j in range(num_ps)` where `j % 3 == 0`.
    - Always include `entry` submodel.
    - Build `PublishMeta(model_name=f'test_{i}', num_ps=num_ps)` and append.
  - `model_configs = monitor.gen_model_config(pms)`.
  - For each service_type, if config list non-empty: `handle_reload_config_request`.
  - Set `data['execute'] = 'reload_config'`.
- `test_remove_config`:
  - Same as reload but `i in range(5, 10)` (different model names), set `data['execute'] = 'remove_config'`.
- Randomness: no seed set; tests rely on structural invariants rather than specific PS counts.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/tests/tfs_monitor.rs`.
- Rust public API surface: TFSMonitor + FakeTFServing.

**Implementation Steps (Detailed)**
1. Port fake TFS server setup.
2. Port PublishMeta-based config generation and reload requests.
3. Assert status responses match Python expectations (NOT_FOUND or version numbers).

**Tests (Detailed)**
- Python tests: this file
- Rust tests: parity test for reload/remove behavior.

**Gaps / Notes**
- Requires deterministic fake TFS server behavior for version states.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/tfs_wrapper.py`
<a id="monolith-agent-service-tfs-wrapper-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 202
- Purpose/role: Wraps TensorFlow Serving process launch, config file handling, and model status queries; provides a fake wrapper for tests.
- Key symbols/classes/functions: `TFSWrapper`, `FakeTFSWrapper`.
- External dependencies: `subprocess`, `grpc`, TF Serving protos, `monolith.utils.find_main`.
- Side effects: launches external process, writes logs, opens gRPC channel, kills process.

**Required Behavior (Detailed)**
- Globals:
  - `TFS_BINARY = os.environ.get("MONOLITH_TFS_BINARY", None)`.
  - `State = ModelVersionStatus.State`.
- `TFSWrapper.__init__`:
  - Stores ports, model config path, binary config, log file.
  - Runs `strings $TFS_BINARY | grep PredictionServiceGrpc` (shell=True) to detect gRPC remote op support.
  - Sets `_is_grpc_remote_op` based on `returncode == 0`.
- `_prepare_cmd()`:
  - Base flags:
    - `--model_config_file=...`
    - `--port=<grpc_port>`
    - `--rest_api_port=<http_port>`
    - `--model_config_file_poll_wait_seconds=60`
    - Archon flags: `--archon_port`, `--archon_rpc_psm`, `--archon_rpc_cluster`
    - `--metrics_namespace_prefix=<psm>`
  - If not gRPC remote op: add `--archon_entry_to_ps_rpc_timeout=<fetch_ps_timeout_ms>`.
  - Always adds `--conf_file=conf/service.conf` and `--log_conf=conf/log4j.properties`.
  - For each field in `TfServingConfig` type hints:
    - Uses default vs current; skips if equal.
    - For `platform_config_file`: if None, set `--platform_config_file=conf/platform_config_file.cfg`.
    - For bool: lower-case value.
- `start()`:
  - `os.chdir(find_main())`.
  - Launches process via `subprocess.Popen(tfs_cmd.split(), shell=False, stderr=STDOUT, stdout=log_file, env=os.environ)`.
  - Opens gRPC channel to `localhost:<grpc_port>` and `ModelServiceStub`.
- `stop()`:
  - Closes channel; closes stdout if exists; kills process.
- `poll()`:
  - Calls `proc.poll()` and returns `returncode`.
- `model_config_text()`:
  - Reads config file content.
- `list_saved_models()`:
  - Parses `ModelServerConfig` from text and returns `config.name` list.
- `list_saved_models_status()`:
  - For each saved model:
    - Calls `GetModelStatus`.
    - If multiple versions: picks latest AVAILABLE else latest by version.
    - On RPC error: returns `ModelVersionStatus(state=UNKNOWN, status=StatusProto(error_code=e.code().value[0], error_message=e.details()))`.
  - Returns `{model_name: ModelVersionStatus}`.
- `FakeTFSWrapper`:
  - No process; reads config file and returns AVAILABLE for all models.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/src/tfs_wrapper.rs`.
- Rust public API surface: `TfsWrapper` + `FakeTfsWrapper`.
- Feature gating: `grpc` for ModelService client; optional process spawning in non-test builds.

**Implementation Steps (Detailed)**
1. Port command builder with exact flags and `TfServingConfig` mapping.
2. Implement process spawn with log redirection and working dir `find_main()`.
3. Implement gRPC GetModelStatus handling and version selection.
4. Implement FakeTFSWrapper for tests.

**Tests (Detailed)**
- Python tests: used indirectly by `agent_v3_test`.
- Rust tests: `FakeTfsWrapper.list_saved_models` + `list_saved_models_status` correctness.

**Gaps / Notes**
- `TFS_BINARY` must be set; Python will crash if missing.
- Error_code mapping uses `e.code().value[0]` (non-obvious; must mirror).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/utils.py`
<a id="monolith-agent-service-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: ~1000+
- Purpose/role: Core config + helper utilities for agent service, TF Serving configs, TensorProto creation, network utilities, and config file generation.
- Key symbols/classes/functions: `TfServingConfig`, `AgentConfig`, `DeployType`, `TFSServerType`, `RoughSortModel*`, `conf_parser`, `find_free_port`, `gen_model_spec`, `gen_model_config`, `make_tensor_proto`, `InstanceFormater`, `ZKPath`, `get_local_ip`, many helpers.
- External dependencies: `tensorflow`, `tensorflow_serving` protos, `protobuf.text_format`, `json`, `socket`, `os`.
- Side effects: overrides `os.path.isabs`; writes platform config files; reads/writes files; inspects env; opens sockets.

**Required Behavior (Detailed)**
- Module globals:
  - Defines `flags.DEFINE_string("conf", "", "agent conf file")`.
  - `TFS_HOME="/opt/tiger/monolith_serving"`.
  - `DEFAULT_PLATFORM_CONFIG_FILE = "{TFS_HOME}/conf/platform_config_file.cfg"`.
  - Overrides `os.path.isabs` to treat paths starting with `hdfs:/` as absolute.
  - `DEFAULT_MODEL_CONFIG` computed via `gen_model_config` with name `default` and base_path `${TFS_HOME}/dat/saved_models/entry`.
  - `FeatureKeys` set used by `InstanceFormater.from_dump`.
- Small helpers:
  - `conf_parser(file_name, args)`:
    - Ignores missing file.
    - Strips comments/blank lines.
    - Supports `include <path>` directive (recursive parse).
    - Parses `key=value` or `key value` via regex split `SEQ`.
    - If key repeats: first value becomes list; subsequent appended.
  - `find_free_port()` binds `('localhost', 0)` and returns ephemeral port.
  - `check_port_open(port)` connects to `127.0.0.1:port`; logs and returns bool.
  - `write_to_tmp_file(content)` writes `str(content)` to tempfile and returns path.
  - `replica_id_from_pod_name()`:
    - If `MY_POD_NAME` set: MD5 hash, take hex `[10:20]` slice, parse base16.
    - Else returns `-1`; any exception returns `-1`.
- Type helpers and enums:
  - `TFSServerType` constants: `ps`, `entry`, `dense`, `unified`.
  - `DeployType` wraps server type; validates type; equality works vs string or DeployType.
  - `DeployType.compat_server_type(server_type)`:
    - If `server_type` None or `mixed`:
      - If self is `mixed`, raises RuntimeError.
      - Else returns self type.
    - If self is `mixed`, returns server_type.
    - Else asserts equality and returns server_type.
  - Rough sort enums: `RoughSortModelLoadedServer` and `RoughSortModelPrefix`.
- `TfServingConfig` dataclass defaults:
  - `enable_batching=False`, `allow_version_labels_for_unavailable_models=False`, `batching_parameters_file=None`.
  - `num_load_threads=0`, `num_unload_threads=0`, `max_num_load_retries=5`, `load_retry_interval_micros=60*1000*1000`.
  - `file_system_poll_wait_seconds=1`, `flush_filesystem_caches=True`.
  - `tensorflow_session_parallelism=0`, `tensorflow_intra_op_parallelism=0`, `tensorflow_inter_op_parallelism=0`.
  - `ssl_config_file=None`, `platform_config_file=None`.
  - `per_process_gpu_memory_fraction=0`, `allow_growth=True`, `saved_model_tags=None`.
  - `grpc_channel_arguments=None`, `grpc_max_threads=0`, `enable_model_warmup=True`, `version=None`.
  - `remove_unused_fields_from_bundle_metagraph=True`, `enable_signature_method_name_check=False`.
  - `xla_cpu_compilation_enabled=False`, `enable_profiler=True`.
- `AgentConfig` dataclass defaults (in addition to `TfServingConfig`):
  - `bzid=None`, `base_name=None`, `base_path=None`, `num_ps=1`, `num_shard=None`, `deploy_type=None`.
  - `replica_id=None`, `stand_alone_serving=False`, `zk_servers=None`.
  - `proxy_port=None`, `tfs_entry_port=None`, `tfs_entry_http_port=None`, `tfs_entry_archon_port=None`.
  - `tfs_ps_port=None`, `tfs_ps_http_port=None`, `tfs_ps_archon_port=None`.
  - `dense_alone=False`, `dense_service_num=3`, `tfs_dense_port=None`, `tfs_dense_http_port=None`, `tfs_dense_archon_port=None`.
  - `agent_port=None`, `update_model_status_interval=1`, `model_config_file=None`, `agent_version=1`.
  - `max_waiting_sec=1200`, `preload_jemalloc=True`.
  - `version_policy='latest'`, `version_data=1`.
  - `fetch_ps_timeout_ms=200`, `fetch_ps_long_conn_num=100`, `fetch_ps_long_conn_enable=True`, `fetch_ps_retry=2`, `aio_thread_num=30`.
  - `file_system_poll_wait_seconds_ps=0`.
  - Rough sort: `rough_sort_model_name=None`, `rough_sort_model_local_path=None`, `rough_sort_model_loaded_server=entry`, `rough_sort_model_p2p_path=None`, `rough_sort_resource_constrained=False`.
  - `dc_aware=False`.
  - Unified: `layout_pattern=None`, `layout_filters=None`, `tfs_port_archon=None`, `tfs_port_grpc=None`, `tfs_port_http=None`, `use_metrics=True`.
- `AgentConfig.__post_init__`:
  - Updates `zk_servers` via `_update_zk_servers(self.zk_servers, is_ipv6_only())`.
  - If `stand_alone_serving`: `deploy_type = DeployType(mixed)`; else requires `deploy_type` and wraps in `DeployType`.
  - `num_shard` defaults to `num_tce_shard`; otherwise asserts equal to `num_tce_shard`.
  - Port allocation (uses `find_free_port` and env overrides):
    - Mixed: proxy + entry/ps ports from `PORT`, `PORT3..PORT7`; dense ports from `PORT8..PORT10` if `dense_alone` and `DENSE_SERVICE_IDX==0`, else free ports.
    - Entry: proxy + ps ports free; entry ports from `PORT`, `PORT3`, `PORT4`; optional dense free ports.
    - PS: proxy + entry ports free; ps ports from `PORT`, `PORT3`, `PORT4`; optional dense free ports.
    - Dense: requires `dense_alone=True`; entry/ps ports free; dense ports from `PORT`, `PORT3`, `PORT4` if `DENSE_SERVICE_IDX==0`, else free.
    - Unified: `tfs_port_archon/ grpc / http` from `PORT`, `PORT3`, `PORT4`.
  - `agent_port` from `PORT2` or free.
  - `replica_id`: if `agent_version==1`, use `replica_id_from_pod_name`; else env `REPLICA_ID` or fallback to pod hash.
  - If `platform_config_file` unset, use `DEFAULT_PLATFORM_CONFIG_FILE`.
  - Calls `generate_platform_config_file()`.
- `generate_platform_config_file()`:
  - Builds `ConfigProto` with `intra_op_parallelism_threads` and `inter_op_parallelism_threads`:
    - Use configured values or `MY_CPU_LIMIT` or default `16`.
  - `allow_soft_placement=True`, `gpu_options.allow_growth=allow_growth`.
  - If `dense_alone` and `enable_batching`:
    - Build `BatchingParameters` with `max_batch_size=1024`, `batch_timeout_micros=800`, `max_enqueued_batches=100000`, `num_batch_threads=8`, `support_diff_dim_size_inputs=True`.
    - Build `SessionBundleConfig` with session+batching.
  - Else `SessionBundleConfig` with session only.
  - Set `enable_model_warmup`.
  - Wrap in `SavedModelBundleSourceAdapterConfig`, pack into `PlatformConfigMap` under key `tensorflow`.
  - Serialize to text and write to `platform_config_file`.
  - On exception: logs and attempts to remove file if it exists.
- AgentConfig properties:
  - `num_tce_shard`: `HOST_SHARD_ENV` or `1`.
  - `shard_id`: env `SHARD_ID` or `-1`.
  - `idc`: env `TCE_INTERNAL_IDC` lowercased.
  - `cluster`: env `TCE_LOGICAL_CLUSTER` or `TCE_CLUSTER` or `TCE_PHYSICAL_CLUSTER` lowercased.
  - `location`: `"{idc}:{cluster}"` if both; else None.
  - `path_prefix`: `/bzid/service/base_name[/idc:cluster]` if `dc_aware` else without location.
  - `layout_path`: if `layout_pattern` absolute, use it; else `/{bzid}/layouts/{layout_pattern}`.
  - `container_cluster`: `"{TCE_PSM};{idc};{cluster}"`.
  - `container_id`: env `MY_POD_NAME` or `get_local_ip()`.
- Command helpers:
  - `get_cmd_and_port(binary, server_type=None, config_file=None)`:
    - Normalizes `server_type` via `compat_server_type`.
    - If `config_file` None, generates model server config and writes to temp file.
    - Adds flags: `--model_config_file`, archon psm/cluster, metrics prefix, log_conf.
    - For mixed and non-entry: suffix psm with `_ps`/`_dense`, change log conf.
    - Adds port/rest/archon flags per server type.
    - ENTRY/DENSE add `archon_entry_to_ps_*` flags and `archon_async_dispatcher_threads`.
    - DENSE adds `--enable_batching=true` if enabled.
    - `agent_version != 1` adds `--model_config_file_poll_wait_seconds=0`.
    - Iterates `TfServingConfig` fields: if value differs from default, emit flag; special case `file_system_poll_wait_seconds`:
      - If `agent_version==1` and server_type==PS, use `file_system_poll_wait_seconds_ps`.
      - Else use configured value if not default.
    - Returns command string and chosen gRPC port.
  - `get_cmd` returns command only.
  - `get_server_schedule_iter(server_type)`:
    - Mixed/PS: for PS yields indices where `i % num_shard == shard_id`; else yields None.
    - Dense: if server_type dense, yields `replica_id` (commented TODO about bug).
    - Else yields None.
- Model config generation:
  - `_gen_model_server_config(server_type)`:
    - Uses `version_policy` and `version_data`, `compat_server_type`.
    - PS: create config per schedule index `ps_i`, base_path = `base_path/ps_i`.
      - If `rough_sort_model_name` and loaded_server=PS: add `ps_item_embedding_i` with base_path `${rough_sort_model_local_path}/${rough_sort_model_name}/${name}`.
    - Entry/Dense: add `entry` or `dense_0` model with base_path `${base_path}/{name}`.
      - If `rough_sort_resource_constrained` and loaded_server=ENTRY: add `entry_item_embedding_0` with base_path `${base_path}/{name}`.
      - Else if `rough_sort_model_name` and loaded_server in ENTRY/DENSE: add `{entry|dense}_item_embedding_0` with base_path `${rough_sort_model_local_path}/${rough_sort_model_name}/${name}`.
- Config parsing:
  - `AgentConfig.from_file(fname)`:
    - Calls `conf_parser` to build `kwarg`.
    - For each annotated field: convert strings to bool/int/float/str/List; `str == "none"` -> `None`; `int/float` uses `eval`.
    - If missing `deploy_type`, uses legacy `server_type`.
    - Returns `AgentConfig(**args)` (triggers `__post_init__`).
  - `_update_zk_servers(zk_servers, use_ipv6)`:
    - If `use_ipv6` and `zk_servers` contains IPv4 hosts:
      - If equals `default_zk_servers_ipv4` (constructed from `_HOSTS` + `_PORT`), return `default_zk_servers(True)` with warning.
      - Else raise exception.
    - Else returns original `zk_servers`.
- ZK path parser:
  - `ZKPath.PAT` regex supports `/bzid/service/base[/idc:cluster]/server_type:index[/replica_id]`.
  - `__init__`: if `path is None or len(path) != 0` then attempts match; sets `_group_dict` or logs "path not matched".
  - `__getattr__` returns captured group or None.
  - `task`: `"server_type:index"` if both present.
  - `location`: `"idc:cluster"` if both present.
  - `ship_in(idc, cluster)`: if either arg None, returns True; else compares to parsed idc/cluster.
- Pattern helpers:
  - `parse_pattern` splits string by `{}` (or `[]` in `expand_pattern`) and combines via `comb_fn`.
  - `normalize_regex` replaces `{name}` with named capture group `(?P<name>\\d+)`.
  - `expand_pattern` expands bracket ranges like `[1-3]` or `[1,2]` into list; uses `range(int(s), int(e))` for dash.
- Model spec/config helpers:
  - `gen_model_spec(name, version=None, signature_name=None)`:
    - Sets `version.value` if int; else `version_label` if str.
    - Adds `signature_name` if provided.
  - `gen_model_config(name, base_path, version_policy='latest', version_data=1, model_platform='tensorflow', version_labels=None)`:
    - `latest`, `latest_once` set `num_versions`.
    - `all` sets `ServableVersionPolicy.All()`.
    - `specific` accepts int or list and extends versions.
    - Else raises ValueError.
    - `version_labels` applied if provided.
  - `gen_status_proto` and `gen_model_version_status` wrap TF Serving status proto fields.
  - `make_tensor_proto(instances)` builds DT_STRING TensorProto with `dim.size=len(instances)` and `string_val` set.
- InstanceFormater:
  - `to_tensor_proto(batch_size)` repeats serialized instance `batch_size` times.
  - `to_pb(fname=None)` writes serialized bytes to temp file or given path.
  - `to_json(fname=None)` uses `MessageToJson` and `write_to_tmp_file` or writes to `fname`.
  - `to_pb_text(fname=None)` writes text format via `write_to_tmp_file` or to `fname`.
  - `from_json(fname)` reads JSON and `ParseDict` to Instance.
  - `from_pb_text(fname)` reads file, strips lines, `text_format.Parse`.
  - `from_dump(fname)`:
    - Parses key/value lines into nested dict/list structure (`kwargs`) with `stack` control.
    - Interprets numeric keys as list indices.
    - Handles feature-level merging using `FeatureKeys` and stack lookback.
    - Converts numeric values to int.
    - Parses resulting dict into Instance via `ParseDict`.
- Misc:
  - `pasre_sub_model_name(sub_model_name)` (typo in name) splits by `_`, returns `(lower, index)` with default `0`.
  - `get_local_ip()`:
    - First tries `MY_HOST_IP` or `socket.gethostbyname(hostname)`.
    - Falls back to UDP socket connect to `8.8.8.8:80` to infer local IP.
    - Returns `'localhost'` if no usable IP found.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/src/config.rs` + new `utils.rs`.
- Rust public API surface: `AgentConfig` struct + helpers for model spec and TensorProto assembly.
- Data model mapping: use `monolith_proto::tensorflow_serving::apis` for ModelSpec and `tensorflow_core::TensorProto`.

**Implementation Steps (Detailed)**
1. Port `AgentConfig` with all fields + defaults + env overrides.
2. Recreate port allocation logic, deploy type handling, and platform config file generation.
3. Port `gen_model_spec` and `gen_model_config` helpers with identical proto fields.
4. Implement `make_tensor_proto` for DT_STRING using TF Serving proto types.
5. Port network/IP helper methods (`get_local_ip`, `find_free_port`, etc.).
6. Mirror all file I/O (platform config) and text_format behavior.

**Tests (Detailed)**
- Python tests: `monolith/agent_service/utils_test.py`
- Rust tests: add unit tests for config defaults, model spec generation, and TensorProto creation.

**Gaps / Notes**
- This file is high-risk; many behaviors are implicit and must be traced manually.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/utils_test.py`
<a id="monolith-agent-service-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 170
- Purpose/role: Tests utility helpers (model spec/config, status proto, AgentConfig parsing, instance parsing, ZKPath parsing).
- Key symbols/classes/functions: `ServingUtilsTest` methods.
- External dependencies: `monolith.agent_service.utils`.
- Side effects: reads config and test data files.

**Required Behavior (Detailed)**
- `setUpClass`: sets `MY_HOST_IP=127.0.0.1`.
- `test_gen_model_spec`:
  - `gen_model_spec('model', 1, 'predict')` -> name, version.value, signature_name set.
- `test_gen_model_config`:
  - `gen_model_config` with `version_data=2` and version_labels.
  - Asserts `name`, `base_path`, and `latest.num_versions == 2`.
- `test_gen_status_proto`:
  - `gen_status_proto(ErrorCode.CANCELLED, 'CANCELLED')` sets fields.
- `test_gen_model_version_status`:
  - `gen_model_version_status(version=1, state=START, error_code=NOT_FOUND, error_message="NOT_FOUND")`.
  - Asserts version and state match.
- `test_gen_from_file`:
  - `AgentConfig.from_file('monolith/agent_service/agent.conf')` sets `stand_alone_serving=True`.
- `test_list_field`:
  - Same config file; asserts `layout_filters == ['ps_0', 'ps_1']`.
- `test_instance_wrapper_from_json`:
  - `InstanceFormater.from_json('monolith/agent_service/test_data/inst.json')`.
  - `to_tensor_proto(5)` yields dtype 7, dim[0].size 5.
- `test_instance_wrapper_from_pbtext`:
  - `from_pb_text('monolith/agent_service/test_data/inst.pbtext')`; same tensor checks.
- `test_instance_wrapper_from_dump`:
  - `from_dump('monolith/agent_service/test_data/inst.dump')`; same tensor checks.
- `test_get_cmd_and_port`:
  - `AgentConfig.from_file(...); conf.agent_version=2; conf.get_cmd_and_port(binary='tensorflow_model_server', server_type='ps')`.
  - Asserts `'model_config_file_poll_wait_seconds'` is in cmd string.
- `ZKPath` tests:
  - Full dc-aware: `/bzid/service/base_name/idc:cluster/server_type:0/1` -> bzid/base_name/idc/cluster/server_type/index/replica_id; `location='idc:cluster'`, `task='server_type:0'`, `ship_in(None,None)` True.
  - Partial dc-aware: `/bzid/service/base_name/idc:cluster/server_type:0` -> replica_id None, `ship_in('idc','cluster')` True.
  - Old full: `/bzid/service/base_name/server_type:0/1` -> idc/cluster None, replica_id `1`, `ship_in(None,None)` True.
  - Old partial: `/bzid/service/base_name/server_type:0` -> replica_id None.
  - Old partial 2: `/1_20001223_.../service/20001223_.../ps:1` -> bzid/base_name parsed, server_type `ps`, index `1`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/tests/utils.rs`.
- Rust public API surface: utils module equivalents in Rust.

**Implementation Steps (Detailed)**
1. Port utilities and add tests for every helper above.
2. Ensure AgentConfig parsing matches Python (including list parsing for layout filters).
3. Port InstanceFormater-like parsing for JSON/pbtext/dump inputs.
4. Implement ZKPath parser with dc-aware and legacy formats.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: parity tests with same fixtures.

**Gaps / Notes**
- InstanceFormater depends on test_data fixtures; ensure Rust can read them.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/zk_mirror.py`
<a id="monolith-agent-service-zk-mirror-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 672
- Purpose/role: ZK mirror/cache with watches for portal/publish/resource/service paths; coordinates scheduling and replica updates.
- Key symbols/classes/functions: `ZKMirror` and methods (watch_portal, watch_publish, expected_loading, update_service, election).
- External dependencies: `MonolithKazooClient`, `kazoo` watchers/election, `PublishMeta`, `ReplicaMeta`, `ResourceSpec`.
- Side effects: ZK watch registration, ZK CRUD, background threads, queue events.

**Required Behavior (Detailed)**
- Maintains in-memory `_data` cache of ZK paths to bytes.
- Defines base paths: `resource`, `portal`, `publish`, `service`, `locks`, `election`.
- CRUD helpers (`create`, `set`, `delete`, `exists`, `ensure_path`) wrap ZK and fall back to cache on errors.
- `report_resource`: writes ResourceSpec as ephemeral node.
- `resources` property: returns list of ResourceSpec from cached paths.
- `num_tce_replica`: waits until every replica id appears for all shards; returns count.
- `tce_replica_id`: uses env `REPLICA_ID` or derives from pod name.
- `publish_loadding`: writes PublishMeta entries to publish path; updates cache.
- `expected_loading`:
  - Groups PublishMeta by model_name; selects when all publish nodes have arrived.
  - Adjusts shard_id/replica_id for autoscaler and entry cases; filters sub_models to entry when needed.
- `update_service(replicas)`:
  - Computes paths for local replicas; removes outdated nodes; creates/updates current replicas.
- Replica queries: `get_all_replicas`, `get_model_replicas`, `get_task_replicas`, `get_replica` (AVAILABLE only).
- Watchers:
  - `watch_portal`: ensures portal/publish consistency; installs DataWatch for model meta; emits Event(PORTAL).
  - `watch_publish`: installs watches on publish nodes; emits Event(PUBLISH) when all publish nodes arrive.
  - `watch_resource`: watches resource nodes into cache.
  - `watch_service`: watches service hierarchy into cache.
- `election`: uses Kazoo Election to run leader function; handles reconnects.
- `start(is_client=False)`: starts ZK, watches service, and optionally publish.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/src/zk_mirror.rs`.
- Rust public API surface: `ZKMirror` with same watch APIs and cache.
- Data model mapping: `PublishMeta`, `ResourceSpec`, `ReplicaMeta`, `Event`.

**Implementation Steps (Detailed)**
1. Implement ZK cache and watch registration with thread-safe locks.
2. Port expected_loading logic (publish count and shard/replica overrides).
3. Port update_service and replica query helpers.
4. Implement leader election and reconnect behavior.
5. Provide Queue/Event mechanism for scheduler integration.

**Tests (Detailed)**
- Python tests: `monolith/agent_service/zk_mirror_test.py`
- Rust tests: use Fake ZK to validate watches, publish flow, and replica queries.

**Gaps / Notes**
- Requires a robust fake ZK in Rust to test watch behavior.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/agent_service/zk_mirror_test.py`
<a id="monolith-agent-service-zk-mirror-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 229
- Purpose/role: End-to-end test of ZKMirror portal/publish/service/resource flows using FakeKazooClient.
- Key symbols/classes/functions: `ZKMirrorTest.test_crud`, `.test_zk_mirror`
- External dependencies: FakeZK, FakeTFServing, data_def types.
- Side effects: ZK node creation, event queue handling.

**Required Behavior (Detailed)**
- `test_crud`:
  - `ensure_path`, `exists`, `create`, `get/set`, `delete` operations.
  - Checks derived properties: `num_tce_shard`, `tce_replica_id`, `tce_shard_id`.
- `test_zk_mirror`:
  - `watch_portal` + `watch_resource`.
  - Portal event should be emitted for new ModelMeta.
  - Scheduler simulation publishes PublishMeta to all shards/replicas.
  - `expected_loading` selects correct PublishMeta for current shard.
  - `update_service` writes ReplicaMeta nodes; verify replica query APIs.
  - `report_resource` and `resources` should roundtrip ResourceSpec.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-serving/tests/zk_mirror.rs`.
- Rust public API surface: ZKMirror with fake ZK.

**Implementation Steps (Detailed)**
1. Port fake ZK + ZKMirror to Rust.
2. Implement scheduler simulation and verify queue events.
3. Validate replica query helpers and resource roundtrip.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: direct parity tests.

**Gaps / Notes**
- Requires deterministic ordering of publish nodes and queue events.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/base_runner.py`
<a id="monolith-base-runner-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 46
- Purpose/role: Base runner class with TensorFlow summary writing helper.
- Key symbols/classes/functions: `BaseRunner`, `write_summary`.
- External dependencies: `tensorflow`.
- Side effects: writes TensorFlow summary events to a writer.

**Required Behavior (Detailed)**
- `BaseRunner.__init__` accepts `*args, **kwargs` (no stored fields in base).
- `run()` is abstract and raises `NotImplementedError`.
- `write_summary(logs, summary_writer, current_step)`:
  - Creates a new TF v1 Graph context.
  - Builds `tf.compat.v1.Summary.Value` entries from `logs` dict.
  - Writes to `summary_writer.add_summary(tf_summary, current_step)`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/base_runner.rs`.
- Rust public API surface: `BaseRunner` trait/struct with `run` and `write_summary` helper.
- Data model mapping: summary writer abstraction (likely TB event writer).

**Implementation Steps (Detailed)**
1. Implement a minimal runner trait with `run()`.
2. Provide a summary writer abstraction compatible with TF event files (or document absence).
3. Mirror log-to-summary conversion semantics.

**Tests (Detailed)**
- Python tests: none specific.
- Rust tests: unit test for summary serialization if supported.

**Gaps / Notes**
- Rust likely needs a TensorBoard writer crate to match TF summary outputs.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/common/python/mem_profiling.py`
<a id="monolith-common-python-mem-profiling-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 51
- Purpose/role: Configure tcmalloc heap profiling via environment variables.
- Key symbols/classes/functions: `enable_tcmalloc`, `setup_heap_profile`.
- External dependencies: `MLPEnv`, `monolith.utils`.
- Side effects: modifies `LD_PRELOAD` and `HEAP_PROFILE_*` env vars.

**Required Behavior (Detailed)**
- `enable_tcmalloc()`:
  - Appends `../gperftools/libtcmalloc/lib/libtcmalloc.so` (resolved via `utils.get_libops_path`) to `LD_PRELOAD`.
- `setup_heap_profile(...)`:
  - Calls `enable_tcmalloc()`.
  - Uses `MLPEnv().index` to name heap profile file `hprof_<index>` in `heap_pro_file` or `utils.find_main()`.
  - Sets env vars:
    - `HEAPPROFILE` path
    - `HEAP_PROFILE_INUSE_INTERVAL` and `HEAP_PROFILE_ALLOCATION_INTERVAL` scaled by `1/sample_ratio`
    - `HEAP_PROFILE_SAMPLE_RATIO`, `HEAP_PROFILE_TIME_INTERVAL`, `HEAP_PROFILE_MMAP`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/mem_profiling.rs`.
- Rust public API surface: functions to set environment variables and preload tcmalloc.

**Implementation Steps (Detailed)**
1. Implement environment variable setup with the same names and scaling.
2. Provide an MLPEnv equivalent or configuration injection for index.
3. Ensure LD_PRELOAD modification is appended (not overwritten).

**Tests (Detailed)**
- Python tests: none
- Rust tests: unit tests for env var values given sample_ratio.

**Gaps / Notes**
- Rust cannot preload shared libs at runtime on all platforms; document OS-specific behavior.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/__init__.py`
<a id="monolith-core-init-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 0
- Purpose/role: Empty initializer for `monolith.core`.
- Key symbols/classes/functions: none
- External dependencies: none
- Side effects: none

**Required Behavior (Detailed)**
- Importing this module must not execute any side effects.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/lib.rs`.
- Rust public API surface: module boundary only.

**Implementation Steps (Detailed)**
1. Ensure the Rust crate exposes `monolith-core` without implicit side effects.

**Tests (Detailed)**
- Python tests: none
- Rust tests: none

**Gaps / Notes**
- None.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/auto_checkpoint_feed_hook.py`
<a id="monolith-core-auto-checkpoint-feed-hook-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 376
- Purpose/role: TPU infeed/outfeed SessionRunHook with thread-managed queues, TPU init/shutdown, and end-of-stream stopping signals.
- Key symbols/classes/functions: `PeriodicLogger`, `_SIGNAL`, `_OpQueueContext`, `_OpSignalOnceQueueContext`, `TPUInfeedOutfeedSessionWithEndOfStreamHandlingHook`.
- External dependencies: `threading`, `time`, `os`, `six.moves.queue/xrange`, `tensorflow.compat.v1`, `config_pb2`, `tpu_compilation_result`, `summary_ops_v2` (plus implicit `ops`, `tpu_config`, `session_support`).
- Side effects: spawns threads, initializes/shuts down TPU, runs TF sessions, reads env vars, logs via TF logging.

**Required Behavior (Detailed)**
- `PeriodicLogger(seconds)`:
  - `log()` emits TF log only if elapsed time exceeds `seconds`.
- `_SIGNAL`:
  - `NEXT_BATCH = -1`, `STOP = -2` (negative values reserved for control).
- `_OpQueueContext`:
  - Starts a daemon thread running `target(self, *args)`.
  - `send_next_batch_signal(iterations)` enqueues integer iterations.
  - `read_iteration_counts()` yields iterations until `_SIGNAL.STOP` then returns.
  - `join()` logs, sends STOP, joins thread.
- `_OpSignalOnceQueueContext`:
  - Only allows the first `send_next_batch_signal`; subsequent calls ignored.
- `TPUInfeedOutfeedSessionWithEndOfStreamHandlingHook`:
  - `__init__`:
    - Stores enqueue/dequeue ops, rendezvous, master, session config, init ops, outfeed cadence.
    - Reads embedding config from `ctx` if present.
    - Sets `_should_initialize_tpu=False` when model-parallel + per-host input broadcast; else true.
    - Sets `stopping_signal=False`.
  - `_create_or_get_iterations_per_loop()`:
    - Uses graph collection `tpu_estimator_iterations_per_loop`.
    - If >1 existing var → `RuntimeError("Multiple iterations_per_loop_var in collection.")`.
    - Else creates resource variable in scope `tpu_estimator`, int32 scalar, non-trainable, in LOCAL_VARIABLES, colocated with global step.
  - `begin()`:
    - Records `_iterations_per_loop_var`.
    - Adds TPU shutdown op to `_finalize_ops` if `_should_initialize_tpu`.
    - Adds summary writer init ops to `_init_ops` and flush ops to `_finalize_ops`.
  - `_run_infeed(queue_ctx, session)`:
    - Optional sleep (`initial_infeed_sleep_secs`).
    - If `run_infeed_loop_on_coordinator`: for each iteration signal, runs `enqueue_ops` `steps` times.
    - Else runs `enqueue_ops` once per signal.
  - `_run_outfeed(queue_ctx, session)`:
    - Runs `dequeue_ops` every `outfeed_every_n_steps`.
    - If output includes `_USER_PROVIDED_SIGNAL_NAME`, expects a dict containing `stopping`.
    - When first stopping signal seen, sets `stopping_signals=True` and later flips `self.stopping_signal=True`.
  - `_assertCompilationSucceeded(result, coord)`:
    - Parses TPU compilation proto; if `status_error_message` set → log error + `coord.request_stop()`.
  - `after_create_session()`:
    - If `_should_initialize_tpu`, runs `tf.tpu.initialize_system` in a fresh graph/session.
    - Runs `_init_ops` with 30 minute timeout.
    - If `TPU_SPLIT_COMPILE_AND_EXECUTE=1`, runs `tpu_compile_op` and asserts compilation success.
    - Starts infeed/outfeed controller threads.
    - If `TF_TPU_WATCHDOG_TIMEOUT>0`, starts worker watchdog.
  - `before_run()`:
    - If `stopping_signal` is set, raises `tf.errors.OutOfRangeError`.
    - Reads `iterations_per_loop`, sends signals to infeed/outfeed controllers.
  - `end()`:
    - Joins infeed/outfeed threads; calls rendezvous `record_done`; runs finalize ops (flush + shutdown).
  - `get_stopping_signals_and_name(features)`:
    - If `_USER_PROVIDED_SIGNAL_NAME` in `features`, uses `tf.tpu.cross_replica_sum` to compute `stopping` boolean.
    - Returns `(stopping_signals, _USER_PROVIDED_SIGNAL_NAME)`.
- Env vars:
  - `TPU_SPLIT_COMPILE_AND_EXECUTE=1` triggers separate compile step.
  - `TF_TPU_WATCHDOG_TIMEOUT` enables worker watchdog.
- Threading: two daemon threads (infeed/outfeed) controlled via queues.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/src` (TPU runtime integration).
- Rust public API surface: no equivalent; would need a TPU session hook abstraction.
- Data model mapping: TF session/run hooks have no direct Rust equivalent.
- Feature gating: only relevant if TF-runtime backend is enabled.
- Integration points: training loop / input pipeline in `monolith-training`.

**Implementation Steps (Detailed)**
1. Decide if TPU SessionRunHook is in-scope for Rust TF backend.
2. If in-scope, implement infeed/outfeed controller threads and queue signaling.
3. Expose env-var toggles for compile/execute split and watchdog timeout.
4. Provide cross-replica stopping signal handling for outfeed.

**Tests (Detailed)**
- Python tests: none dedicated (covered indirectly by TPU training flows).
- Rust tests: none yet; would need integration tests with TF runtime.
- Cross-language parity test: not applicable unless TF runtime implemented in Rust.

**Gaps / Notes**
- Missing imports in Python (`ops`, `tpu_config`, `session_support`) are referenced but not imported.
- Rust has no TPU SessionRunHook analog; full parity requires TF runtime + hook infrastructure.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/base_embedding_host_call.py`
<a id="monolith-core-base-embedding-host-call-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 643
- Purpose/role: Host call logic for embedding tasks, including TPU variable caching and DeepInsight metrics.
- Key symbols/classes/functions: `BaseEmbeddingHostCall`, `update_tpu_variables_ops`, `generate_host_call_hook`.
- External dependencies: `tensorflow.compat.v1`, `tensorflow`, `BaseHostCall`, `ReplicatedVariable`.
- Side effects: creates TPU variables on all replicas; writes TF summaries; computes AUC.

**Required Behavior (Detailed)**
- Constants define metric names and TPU variable names.
- `TPUVariableRestoreHook` runs assign op after session creation.
- `BaseEmbeddingHostCall.__init__`:
  - Stores flags for host call, deepinsight, scalar metrics, caching mode.
  - Extracts `context` unless `cpu_test`.
  - Creates TPU variables if `enable_host_call` and caching enabled.
- `_create_tpu_var`:
  - Creates per-replica TPU variables across hosts with zeros, adds to `TPU_VAR` collection.
  - Wraps in `ReplicatedVariable`; registers restore hooks.
- `_compute_new_value(base, delta, update_offset)`:
  - Pads `delta` to base length, then `tf.roll` by offset, then adds to base.
- `update_tpu_variables_ops(...)`:
  - Clears TPU vars when `global_step % host_call_every_n_steps == 1`.
  - Writes labels/preds/uid_buckets and req_times/sample_rates into TPU vars at computed offsets.
  - Updates accumulated counter only after tensor updates.
- `record_summary_tpu_variables()` collects TPU vars into host call tensors.
- `record_summary_tensor` filters based on `enable_host_call_scalar_metrics` and deprecated metric names.
- `_write_summary_ops`:
  - Filters by uid_bucket and stopping_signals; computes AUC; writes scalar summaries and serialized tensors.
- `generate_host_call_hook()`:
  - Calls `compress_tensors()` and returns either `_host_call` or `_host_call_with_tpu` plus tensor list; or `None` if disabled.
  - Host call writes summaries under `output_dir/host_call` with `tf2.summary`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/base_embedding_host_call.rs`.
- Rust public API surface: host call builder with optional TPU caching mode.
- Data model mapping: summary writer and AUC metric implementation.

**Implementation Steps (Detailed)**
1. Recreate TPU variable caching logic (or provide equivalent buffer) with host-call step windowing.
2. Implement summary writing and AUC computation matching TF semantics.
3. Mirror filtering by UID buckets and stopping signals.
4. Provide compatibility with TPU/CPU test modes.

**Tests (Detailed)**
- Python tests: `monolith/core/base_embedding_host_call_test.py`
- Rust tests: unit tests for `_compute_new_value` and host call output shapes.

**Gaps / Notes**
- Full parity requires TF TPU runtime; Rust may need to stub or bridge this functionality.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/base_embedding_host_call_test.py`
<a id="monolith-core-base-embedding-host-call-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 77
- Purpose/role: Tests `_compute_new_value` in BaseEmbeddingHostCall.
- Key symbols/classes/functions: `BaseEmbeddingHostCallTest.test_compute_new_value`.
- External dependencies: `tensorflow.compat.v1`.
- Side effects: none.

**Required Behavior (Detailed)**
- Constructs `BaseEmbeddingHostCall` with `enable_host_call=False` and `context=None`.
- Verifies `_compute_new_value` behavior with different offsets.
- Uses TF session to evaluate results and compares to expected tensors.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/tests/base_embedding_host_call.rs`.
- Rust public API surface: `_compute_new_value` equivalent.

**Implementation Steps (Detailed)**
1. Port `_compute_new_value` logic in Rust and add unit tests for offsets.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: same assertions.

**Gaps / Notes**
- Needs tensor ops; may require a small tensor wrapper or TF binding in Rust tests.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/base_embedding_task.py`
<a id="monolith-core-base-embedding-task-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 611
- Purpose/role: Base class for TPU embedding tasks; builds input pipeline, vocab sizing, and TPU embedding configs.
- Key symbols/classes/functions: `BaseEmbeddingTask.params`, `create_input_fn`, `_post_process_example`, `create_feature_and_table_config_dict`, `process_features_for_cpu_test`.
- External dependencies: `tensorflow.compat.v1`, `tpu_embedding`, `FeatureSlot/FeatureColumn`, `auto_checkpoint_feed_hook`, `util`.
- Side effects: reads vocab file (possibly from HDFS), constructs TF datasets and embedding configs.

**Required Behavior (Detailed)**
- `params()` defines many embedding-related flags, including vocab sizing, QR hashing, deepinsight, host call metrics, file input ranges, and stopping signals.
- `__init__`:
  - Sets flags, builds vocab dict (from file or downloaded from HDFS), constructs `Env`.
- `download_vocab_size_file_from_hdfs()`:
  - Downloads a single `part*.csv` from HDFS into temp folder; updates `p.vocab_file_path` if successful.
- `_create_vocab_dict()`:
  - Reads vocab file (tsv slot_id -> count), applies overrides and offsets, returns dict.
- `create_input_fn(mode)`:
  - Only supports TRAIN.
  - If `file_pattern` provided, uses `tf.data.Dataset.list_files` (no shuffle); else uses `file_folder` + `date_and_file_name_format` and `util.range_dateset` with `start_date/end_date`.
  - Shards files per TPU host call index.
  - Interleaves files with `cycle_length` and parses examples via `_get_feature_map` and `_post_process_example`.
  - If `enable_stopping_signals`, appends a final batch with stop signal flag via `auto_checkpoint_feed_hook`.
  - Prefetches with AUTOTUNE.
- `_post_process_example`:
  - Converts embedding tensors to SparseTensor; applies vocab size mods, QR hashing, and FeatureColumn3D row_lengths.
  - Adds UID bucket for AUC sampling if `_UID` exists.
- `create_feature_and_table_config_dict()`:
  - Builds `tpu_embedding.TableConfig` and `FeatureConfig` per slot/feature slice, including QR hashing tables.
- `process_features_for_cpu_test()`:
  - Creates embedding variables with random init and uses `safe_embedding_lookup_sparse`.
  - Clears internal feature/table config dicts after processing.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/base_embedding_task.rs`.
- Rust public API surface: base embedding task trait/struct with dataset and embedding config helpers.
- Data model mapping: TF dataset pipeline and TPU embedding configs (likely via TF runtime bridge).

**Implementation Steps (Detailed)**
1. Port parameter schema and vocab file parsing.
2. Implement dataset pipeline generation or equivalent data loader.
3. Recreate embedding config generation and QR hashing logic.
4. Implement CPU test path for embeddings.

**Tests (Detailed)**
- Python tests: none direct; used by TPU runner tests.
- Rust tests: unit tests for vocab dict creation and QR hashing config creation.

**Gaps / Notes**
- Full parity depends on TF TPU embedding APIs; Rust likely needs a bridge or compatibility layer.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/base_host_call.py`
<a id="monolith-core-base-host-call-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 145
- Purpose/role: Collects tensors for TPU host calls, with compression/decompression by dtype.
- Key symbols/classes/functions: `BaseHostCall.record_summary_tensor`, `compress_tensors`, `decompress_tensors`.
- External dependencies: `tensorflow`, `absl.logging`.
- Side effects: builds internal tensor lists; uses global_step tensor.

**Required Behavior (Detailed)**
- `__init__(output_dir, enable_host_call)`:
  - Initializes `_tensor_names` with `"global_step"` and `_tensors` with reshaped global step.
  - `_lists_tensor_sizes` tracks original sizes for decompression.
- `record_summary_tensor(name, tensor)`:
  - No-op if host call disabled.
  - Asserts unique name, asserts tensor rank <= 1, reshapes to `[-1]`, appends.
- `compress_tensors()`:
  - Groups tensors by dtype; concatenates each dtype list along axis 0, then `expand_dims` to add batch dimension.
  - Stores per-group tensor sizes in `_lists_tensor_sizes`.
  - Replaces `_tensor_names` and `_tensors` with compressed versions.
- `decompress_tensors(tensors)`:
  - Splits each compressed tensor by recorded sizes; squeezes to 1D.
  - Asserts first tensor name is `global_step` (error message uses first character only).
  - Returns `(global_step_scalar, decompressed_tensor_list)` where global step is `decompressed_tensor_list[0][0]`.
- `generate_host_call_hook()` default returns `None` (to be overridden).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/base_host_call.rs`.
- Rust public API surface: `BaseHostCall` struct with tensor collection and (de)compression.

**Implementation Steps (Detailed)**
1. Implement tensor collection and dtype grouping logic.
2. Preserve compression/decompression shapes and per-dtype grouping semantics.
3. Mirror global_step positioning and return shape.

**Tests (Detailed)**
- Python tests: none specific (used indirectly by embedding host call tests).
- Rust tests: unit tests for compress/decompress roundtrip.

**Gaps / Notes**
- Error message in assert uses `self._tensor_names[0][0]` (first char); keep for parity.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/base_layer.py`
<a id="monolith-core-base-layer-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 161
- Purpose/role: Base class for layers with child management, name assignment, and per-graph layer loss tracking.
- Key symbols/classes/functions: `BaseLayer`, `get_uname`, `add_layer_loss`, `get_layer_loss`.
- External dependencies: `tensorflow`, `InstantiableParams`, `NestedMap`.
- Side effects: global registries `_layer_loss` and `_name_inuse` are mutated.

**Required Behavior (Detailed)**
- `params()`:
  - Returns `InstantiableParams(cls)` and defines `name` using `get_uname(cls.__name__)`.
- `__init__(params)`:
  - Asserts `params.name` is set.
  - Initializes `_private_children` as `NestedMap`.
- `children` property returns `_private_children`.
- `__getattr__`:
  - Raises AttributeError if `_private_children` not created.
  - If `name` in children, returns it.
  - If class has a property with that name, calls the property's getter to surface the same AttributeError.
  - Else raises `"<name> is not a sub-layer of <self>."`.
- `__call__` forwards to `fprop()`.
- `fprop()` is abstract: raises `NotImplementedError('Abstract method of %s' % self)`.
- `create_child(name, params)`:
  - If `params.name` empty, assigns `self.p.name` (assumes BaseLayer has `p` attribute set by InstantiableParams).
  - Instantiates child via `params.instantiate()` and stores under `_private_children[name]`.
- `create_children(name, params_list)`:
  - Creates list; for each param, sets `param.name = f"{name}_{index}"` if missing; instantiates and appends.
- `get_uname(name)`:
  - Uses `_name_inuse` defaultdict; **note**: code checks membership but never inserts, so it currently always returns `name` (no suffix) unless keys are inserted elsewhere.
- `add_layer_loss(name, loss)`:
  - Adds loss into `_layer_loss` keyed by default graph and layer name.
- `get_layer_loss()`:
  - Returns the dict for the current default graph.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/base_layer.rs`.
- Rust public API surface: `BaseLayer` trait/struct, child map, `get_uname`, layer loss registry.
- Data model mapping: `NestedMap` equivalent and parameter instantiation mechanism.

**Implementation Steps (Detailed)**
1. Implement a base layer trait with child management and `fprop` entrypoint.
2. Provide a `NestedMap` equivalent with attribute-like access.
3. Mirror `get_uname` behavior (including current no-op uniqueness unless explicitly fixed).
4. Implement per-graph loss registry or document deviation if Rust lacks TF graphs.

**Tests (Detailed)**
- Python tests: `monolith/core/base_layer_test.py`
- Rust tests: verify create_child/create_children and `__getattr__`-like behavior.

**Gaps / Notes**
- BaseLayer relies on `self.p` being set by hyperparams instantiation; mirror this in Rust or document.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/base_layer_test.py`
<a id="monolith-core-base-layer-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 41
- Purpose/role: Tests child creation APIs in `BaseLayer`.
- Key symbols/classes/functions: `BaseLayerTest.test_create_child`, `.test_create_children`.
- External dependencies: `base_layer`.
- Side effects: none.

**Required Behavior (Detailed)**
- `test_create_child`:
  - Instantiates BaseLayer params, sets name, creates layer, calls `create_child`, and asserts child exists.
- `test_create_children`:
  - Creates two child layers and asserts list length is 2.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/tests/base_layer.rs`.
- Rust public API surface: BaseLayer child creation.

**Implementation Steps (Detailed)**
1. Port tests using Rust BaseLayer + params instantiation.
2. Assert child map/list presence and lengths.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: same assertions.

**Gaps / Notes**
- Python tests access `_disable_create_child` which is not defined in BaseLayer; confirm if Rust needs similar flag (likely no-op).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/base_model_params.py`
<a id="monolith-core-base-model-params-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 25
- Purpose/role: Defines abstract params holder for single-task models.
- Key symbols/classes/functions: `SingleTaskModelParams.task`.
- External dependencies: none.
- Side effects: none.

**Required Behavior (Detailed)**
- `SingleTaskModelParams.task()` is abstract and must be overridden to return task params; raises `NotImplementedError('Abstract method')` by default.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/base_model_params.rs`.
- Rust public API surface: trait or struct with `task()`/`task_params()` abstract method.

**Implementation Steps (Detailed)**
1. Create a Rust trait for model params with `task()` returning task config.
2. Ensure errors are explicit when called on base type.

**Tests (Detailed)**
- Python tests: none specific.
- Rust tests: optional compile-time trait enforcement or runtime panic test.

**Gaps / Notes**
- None.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/base_task.py`
<a id="monolith-core-base-task-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 95
- Purpose/role: Base class for a single training task, defining standard hyperparams and abstract input/model functions.
- Key symbols/classes/functions: `BaseTask.params`, `create_input_fn`, `create_model_fn`.
- External dependencies: `base_layer.BaseLayer`, `hyperparams.Params`.
- Side effects: none.

**Required Behavior (Detailed)**
- `params()`:
  - Extends `BaseLayer.params()` with:
    - `accelerator` (None/"tpu"/"horovod")
    - `input.eval_examples`, `input.train_examples`
    - `eval.per_replica_batch_size`, `eval.steps_per_eval`, `eval.steps`
    - `train.steps`, `train.max_steps`, `train.per_replica_batch_size`, `train.file_pattern`, `train.repeat`, `train.label_key`, `train.save_checkpoints_steps`, `train.save_checkpoints_secs`, `train.dense_only_save_checkpoints_secs`, `train.dense_only_save_checkpoints_steps`
- `create_input_fn(mode)` and `create_model_fn()` are abstract and must be overridden.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/base_task.rs`.
- Rust public API surface: base task trait with hyperparams schema and abstract hooks.
- Data model mapping: `Params` equivalent (typed config or map).

**Implementation Steps (Detailed)**
1. Implement base task trait/struct with default parameter schema.
2. Provide typed config structures with the same field names.
3. Ensure overridden methods are required (trait methods).

**Tests (Detailed)**
- Python tests: none specific.
- Rust tests: verify default params include the expected keys.

**Gaps / Notes**
- This depends on the Rust `Params`/hyperparams implementation to mirror Python semantics.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/base_tpu_test.py`
<a id="monolith-core-base-tpu-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 73
- Purpose/role: Base TPU tests for running TPU tasks on CPU and validating merged vector behavior.
- Key symbols/classes/functions: `BaseTPUTest.runWithCPU`, `runMergeVectorTestOnCPU`.
- External dependencies: `model_registry`, `TPURunner`.
- Side effects: runs TPU runner in CPU test mode.

**Required Behavior (Detailed)**
- `runWithCPU(task_name)`:
  - Retrieves task params, instantiates TPURunner, sets `_cpu_test=True` and `_host_call_every_n_steps=0`, runs.
- `runMergeVectorTestOnCPU(task_name)`:
  - Enables `merge_vector` on task params; runs in CPU test mode.
  - Validates merged slot dims and embedding dims in runner task env.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/base_tpu.rs`.
- Rust public API surface: TPU runner CPU-test mode.

**Implementation Steps (Detailed)**
1. Provide CPU test mode in Rust TPU runner.
2. Expose task env state for merge_vector assertions.

**Tests (Detailed)**
- Python tests: used as base class in TPU-related tests.
- Rust tests: implement equivalent helper assertions.

**Gaps / Notes**
- Requires task env structures in Rust to match slot/dim semantics.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/core_test_suite.py`
<a id="monolith-core-core-test-suite-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 35
- Purpose/role: Aggregates core unit tests into a suite.
- Key symbols/classes/functions: `suite()`.
- External dependencies: `unittest`, `ParamsTest`, `BaseLayerTest`, `BaseEmbeddingHostCallTest`, `UtilTest`.
- Side effects: runs test suite when invoked as main.

**Required Behavior (Detailed)**
- `suite()` creates a `unittest.TestSuite` containing the four test classes.
- `__main__` runs suite with `TextTestRunner(verbosity=2)`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (Rust test harness handles suites).
- Rust public API surface: none; map to Rust test module organization.

**Implementation Steps (Detailed)**
1. Ensure Rust tests cover the same components.
2. Document that Python-style suite is not required in Rust.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: already covered by individual test modules.

**Gaps / Notes**
- None.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/dense.py`
<a id="monolith-core-dense-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 179
- Purpose/role: Custom Dense layer combining TF Keras Dense with BaseLayer and optional kernel normalization.
- Key symbols/classes/functions: `Dense.params`, `build`, `fprop`.
- External dependencies: `tensorflow`, `VarianceScaling`.
- Side effects: creates TF variables (kernel/bias/trainable norms).

**Required Behavior (Detailed)**
- `params()`:
  - Defines units, activation, use_bias, kernel/bias initializers, kernel norm options, and partitioner.
- `__init__`:
  - Initializes BaseLayer and tf.keras.layers.Dense with given params.
  - Sets attributes: `allow_kernel_norm`, `kernel_norm_trainable`, `var_name_prefix`, `partitioner`.
- `build(input_shape)`:
  - Validates dtype (float/complex).
  - Uses `VarianceScaling` initializer to create kernel; uses `tf.compat.v1.get_variable` (partitioner optional).
  - If `allow_kernel_norm`: L2-normalizes kernel and optionally multiplies by trainable norm variable.
  - Creates bias if `use_bias`.
- `get_config()` merges base config with custom fields.
- `fprop(inputs)` calls `self.call(inputs)`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/src/dense.rs`.
- Rust public API surface: Dense layer with optional kernel normalization and partitioner hooks.

**Implementation Steps (Detailed)**
1. Implement Dense forward pass and parameter initializers.
2. Add optional kernel normalization and trainable scaling.
3. Preserve config serialization fields.

**Tests (Detailed)**
- Python tests: `monolith/core/dense_test.py`
- Rust tests: layer instantiation, dtype handling, partitioner behavior.

**Gaps / Notes**
- Partitioned variables are TF-specific; Rust may need abstraction or ignore.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/dense_test.py`
<a id="monolith-core-dense-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 108
- Purpose/role: Tests Dense layer instantiation, dtype handling, and partitioner.
- Key symbols/classes/functions: `DenseTest` methods.
- External dependencies: `testing_utils.layer_test`, `Dense`.
- Side effects: creates TF variables and runs sessions.

**Required Behavior (Detailed)**
- `test_dense_instantiate`: runs `layer_test` for different input shapes.
- `test_dense_dtype`: ensures output dtype is float32 when specified.
- `test_dense`: checks output shape and runs session to initialize vars.
- `test_dense_with_partitioner`: ensures Dense works with variable partitioner.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/dense.rs`.
- Rust public API surface: Dense layer tests.

**Implementation Steps (Detailed)**
1. Port tests to Rust layer API.
2. Validate shapes and dtype outputs.
3. Provide a mock partitioner or skip if unsupported (document).

**Tests (Detailed)**
- Python tests: this file
- Rust tests: parity tests.

**Gaps / Notes**
- Some TF-specific behaviors may require stubbing in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/feature.py`
<a id="monolith-core-feature-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 611
- Purpose/role: Sail-like feature API for defining feature slots/slices/columns and embedding lookups with optional merged vector handling.
- Key symbols/classes/functions: `FeatureSlice`, `FeatureSlot`, `FeatureColumnV1`, `FeatureColumn3D`, `Env`.
- External dependencies: `tensorflow`, `absl.logging`, `collections.namedtuple` (imported; unused).
- Side effects: creates TF placeholders, mutates shared `Env` state, writes to `_tpu_features`, logs via `absl.logging`.

**Required Behavior (Detailed)**
- `FeatureSlice`:
  - Constructor stores `feature_slot`, `dim`, `slice_index`, `optimizer`, `initializer`, `learning_rate_fn`.
  - `__repr__` returns `[FeatureSlice][slot_{slot_id}][{slice_index}]` (used as dict key).
  - `__hash__` uses `(feature_slot.slot_id(), slice_index)`.
  - Read-only properties: `dim`, `slice_index`, `optimizer`, `initializer`, `learning_rate_fn`.
- `FeatureSlot`:
  - Constructor registers itself into `Env` via `env.set_feature_slot(slot_id, self)`.
  - If `has_bias=True`, creates bias `FeatureSlice` with `dim=1`, `slice_index=0` using bias optimizer/initializer/lr and appends to `_feature_slices`.
  - `add_feature_slice(dim, optimizer=None, initializer=None, learning_rate_fn=None)`:
    - Defaults to `default_vec_*` settings when args are `None`.
    - Creates `FeatureSlice` with `slice_index=len(_feature_slices)`; appends and returns it.
  - `add_merged_feature_slice(...)`: same as `add_feature_slice` but appends to `_merged_feature_slices`.
  - `_add_feature_column(feature_column)`:
    - Adds to `_feature_columns`.
    - If `has_bias=True`, sets `_bias` for **all** feature columns by calling `feature_column.embedding_lookup(feature_slices[0])`.
  - Properties expose `bias_*`, `default_vec_*`, `feature_slices`, `merged_feature_slices`, `feature_columns`.
- `FeatureColumnV1`:
  - Constructor stores `feature_slot`, `fc_name`; initializes placeholder dicts and `_bias=None`; registers with `FeatureSlot`.
  - `embedding_lookup(feature_slice, init_minval_for_oov=None, init_maxval_for_oov=None)`:
    - Delegates to `Env._embedding_lookup`.
  - `get_bias()` asserts `_bias is not None`.
  - `feature_slice_to_tf_placeholder`:
    - Asserts `env.is_finalized` (note: Python has a bug, method recurses).
    - Returns merged or non-merged placeholder map based on `env._merge_vector`.
- `FeatureColumn3D`:
  - Constructor sets `max_seq_length`, logs it, registers with slot.
  - `embedding_lookup(...)` delegates to `Env._seq_embedding_lookup`.
  - `size_tensor_lookup()` delegates to `Env._size_tensor_lookup`.
  - `feature_slice_to_tf_placeholder` returns 3D placeholder map.
- `Env`:
  - Constructor initializes `vocab_size_dict`, `_slot_id_to_feature_slot`, `_tpu_features=None`, `_is_finalized=False`, then calls `set_params(params)`.
  - `set_params(params)` reads:
    - `qr_multi_hashing`, `qr_hashing_threshold`, `qr_collision_rate`,
      `use_random_init_embedding_for_oov`, `merge_vector`.
  - `set_feature_slot(slot_id, feature_slot)`:
    - If already finalized, returns immediately.
    - Asserts `slot_id` uniqueness.
  - `set_tpu_features(tpu_features)`:
    - Assigns `_tpu_features`; if `_merge_vector` is true, calls `_split_merged_embedding` on each feature slot.
  - `_embedding_lookup(feature_column, feature_slice, init_minval_for_oov=None, init_maxval_for_oov=None)`:
    - Asserts slot IDs match.
    - If `_tpu_features` exists:
      - If `qr_multi_hashing` and `vocab_size_dict[slot_id] > qr_hashing_threshold`, uses quotient/remainder features (`fc_name_slice_0`, `fc_name_slice_1`) and returns their sum.
      - Otherwise uses key `"{fc_name}_{slice_index}"`.
      - If `use_random_init_embedding_for_oov` and `init_minval_for_oov` provided:
        - Computes `norm` across axis=1, replaces near-zero rows with `tf.random.uniform` values.
    - If no `_tpu_features`:
      - Creates `tf.compat.v1.placeholder(tf.float32, [None, dim])` keyed by `FeatureSlice` object.
  - `_seq_embedding_lookup(...)`:
    - Same structure as `_embedding_lookup` but uses placeholder shape `[None, max_seq_length, dim]`.
    - Random OOV initialization uses `tf.random.uniform` and `tf.norm(axis=1)` (3D norms).
  - `_size_tensor_lookup(feature_column)`:
    - If `_tpu_features` exist, reads `"{fc_name}_0_row_lengths"` and builds `[B, max_seq_length]` boolean mask (cast to `int32`).
    - Else returns placeholder `tf.float32` with shape `[None, max_seq_length]` named `"{fc_name}_size"`.
  - `finalize()`:
    - Asserts not already finalized; sets `_is_finalized=True`.
    - If `_merge_vector`, calls `_merge_vector_in_same_slot()`.
  - `_merge_vector_in_same_slot()`:
    - For each slot, keeps bias slice separate (assert `dim==1`).
    - Merges remaining slices into a single merged `FeatureSlice` whose dim is sum of vector dims.
    - For each feature column, creates placeholder for merged slice and updates `_merged_feature_slice_to_tf_placeholder`.
  - `_split_merged_embedding(feature_slot)`:
    - For each feature column, finds merged embedding from `_tpu_features`, splits by per-slice dims, and writes back individual embeddings into `_tpu_features` by `"{fc_name}_{slice_index}"`.
  - Properties:
    - `vocab_size_dict` returns stored dict.
    - `slot_id_to_feature_slot` asserts `is_finalized` (buggy in Python) then returns map.
    - `features` returns `_tpu_features`.
- Determinism: random OOV embeddings depend on TF RNG.
- Logging: `logging.info` in `FeatureColumn3D.__init__`, `logging.vlog` in lookup methods.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/feature.rs`.
- Rust public API surface: `FeatureSlot`, `FeatureSlice`, `SparseFeatureColumn`, `DenseFeatureColumn`, `FeatureColumn` trait.
- Data model mapping: Python Sail-like slot/slice/column + `Env` **do not exist** in Rust; current Rust types represent data containers, not TF graph wiring.
- Feature gating: TF placeholder/graph APIs are Python-only; Rust needs an explicit TF backend or a compat layer.
- Integration points: Python `Env` expected by embedding tasks; Rust `feature.rs` is not integrated with embedding task flow.

**Implementation Steps (Detailed)**
1. Decide whether to port the Sail-like API or provide a shim around existing Rust feature structs.
2. If porting, add `Env`, `FeatureSlot`, `FeatureSlice`, `FeatureColumnV1/3D` in Rust with TF backend hooks.
3. Implement placeholder / feature tensor registry for training/serving use cases.
4. Add merge/split vector logic with deterministic slice ordering.
5. Match OOV random initialization (norm threshold 1e-10) and QR hashing behavior.
6. Capture and fix Python bugs when porting (e.g., `is_finalized` recursion, undefined `slot_id`).

**Tests (Detailed)**
- Python tests: `monolith/core/feature_test.py`.
- Rust tests: add `monolith-rs/crates/monolith-core/tests/feature.rs` mirroring merge/split behavior and bias handling.
- Cross-language parity test: compare split/merge outputs and placeholder shapes.

**Gaps / Notes**
- Python `Env.is_finalized` method recurses (`return self.is_finalized`) instead of returning `_is_finalized`.
- `_embedding_lookup` uses undefined `slot_id` in QR branch; should likely be `feature_column._feature_slot.slot_id()`.
- `_seq_embedding_lookup` references `feature_slice.init_minval_for_oov`/`init_maxval_for_oov` which are not defined.
- Rust `feature.rs` implements data structures, not the TF embedding/placeholder semantics used in Python.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/feature_test.py`
<a id="monolith-core-feature-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 178
- Purpose/role: Tests bias slice creation, feature slice indexing, feature column registration, and merged vector split/merge behavior.
- Key symbols/classes/functions: `FeatureSlotTest`, `FeatureColumnV1Test`.
- External dependencies: `tensorflow`, `monolith.core.hyperparams`, `FeatureSlot`, `FeatureColumnV1`, `Env`.
- Side effects: builds TF placeholders/tensors; uses TF session for split verification.

**Required Behavior (Detailed)**
- `FeatureSlotTest.test_has_bias`:
  - `FeatureSlot(has_bias=True)` creates one bias slice with `dim=1`, `slice_index=0`.
- `FeatureSlotTest.test_add_feature_slice`:
  - Additional slices get incrementing `slice_index` and correct dims.
- `FeatureColumnV1Test.test_add_feature_column`:
  - Creating a feature column appends to `FeatureSlot._feature_columns`.
- `FeatureColumnV1Test.test_merge_split_vector_in_same_slot`:
  - With `merge_vector=True`:
    - `_merge_vector_in_same_slot()` populates `_merged_feature_slices` and merged placeholder maps per slot and column.
    - Expected merged dims:
      - slot 1 (bias+2) -> merged dims `[1,2]`
      - slot 2 (bias only) -> `[1]`
      - slot 3 (no bias, 2+3) -> `[5]`
      - slot 4 (bias+2+3+4) -> `[1,9]`
    - `_split_merged_embedding()` splits merged embeddings into per-slice tensors with exact contents as asserted.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/feature.rs`.
- Rust public API surface: currently missing Env + merge/split logic.
- Data model mapping: add tests once feature API exists in Rust.
- Feature gating: TF backend likely required for placeholder/graph ops.
- Integration points: embedding tasks and feature pipeline.

**Implementation Steps (Detailed)**
1. Implement Rust equivalents for `Env`, `FeatureSlot`, `FeatureColumnV1`.
2. Add merge/split vector unit tests mirroring Python.
3. Verify placeholder/tensor semantics for no-feature and TPU-feature cases.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: `monolith-rs/crates/monolith-core/tests/feature.rs`.
- Cross-language parity test: compare split/merge outputs and bias dims.

**Gaps / Notes**
- Rust feature module is currently a different abstraction; merge/split tests are not implemented.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/host_call.py`
<a id="monolith-core-host-call-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 248
- Purpose/role: Host call implementation for non-TPU-variable caching; collects tensors and writes summaries (AUC, deepinsight).
- Key symbols/classes/functions: `HostCall.record_summary_tensor`, `compress_tensors`, `generate_host_call_hook`.
- External dependencies: `tensorflow`, `absl.logging`.
- Side effects: creates summary writers and writes scalar/text summaries.

**Required Behavior (Detailed)**
- Similar tensor collection/compression logic to `BaseHostCall`:
  - Global step tensor is always first.
  - Tensors grouped by dtype, concatenated, expanded to batch dimension.
- `generate_host_call_hook()`:
  - If disabled, returns None.
  - Otherwise returns `_host_call` and compressed tensors.
  - `_host_call` decompresses tensors, writes scalar summaries and AUC; optional deepinsight text summaries via `_serialize_messages`.
- `_serialize_messages`:
  - Verifies shapes/dtypes, flattens tensors, writes serialized tensors as text summaries.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/host_call.rs`.
- Rust public API surface: host call builder with summary writer integration.

**Implementation Steps (Detailed)**
1. Port tensor compression/decompression semantics.
2. Implement summary writer outputs (scalar + text) and AUC calculation.
3. Support optional deepinsight serialization.

**Tests (Detailed)**
- Python tests: none specific.
- Rust tests: unit tests for compress/decompress and summary output formatting.

**Gaps / Notes**
- Reuses logic duplicated in BaseHostCall; consider shared helper in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/hyperparams.py`
<a id="monolith-core-hyperparams-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 439
- Purpose/role: Dynamic hyperparameter container with attribute + dotted-path access, nested parameter trees, immutability, stringification, and class instantiation support.
- Key symbols/classes/functions: `_is_named_tuple`, `_SortedDict`, `_Param`, `Params`, `InstantiableParams`, `copy_params_to`, `update_params`, `allowed_kwargs`.
- External dependencies: `copy`, `re`, `six`, `inspect.signature/Parameter`, `tensorflow` (Tensor detection for deepcopy).
- Side effects: error messages include similar-key hints; deepcopy of `tf.Tensor` intentionally keeps a reference (no deep copy).

**Required Behavior (Detailed)**
- `_is_named_tuple(x)`:
  - Returns true if `x` is a `tuple` and has `_fields` attribute.
- `_SortedDict.__repr__()`:
  - Always renders dict entries sorted by key.
- `_Param`:
  - Stores `name`, `value`, `description`.
  - `__eq__` compares only name + value (description ignored).
  - `__deepcopy__`: if value is a `tf.Tensor`, **do not copy** (keep ref); else deep-copy; memo updated.
  - `to_string(nested_depth)`:
    - For `Params`: delegate to nested `_to_string`.
    - For `dict`: produce `_SortedDict` with nested `GetRepr` on values.
    - For `list/tuple` (non-namedtuple): reconstruct same type from `GetRepr` values.
    - If value has `Repr` method, call it (used for function-like objects).
    - If value is `str`, wrap in double quotes.
    - Indent with 2 spaces per nesting level.
  - `set` stores value without copying.
  - `get` returns stored value.
- `Params`:
  - Internal storage: `_params` dict mapping name → `_Param`; `_immutable` bool.
  - `__setattr__` / `__setitem__`: if immutable -> `TypeError("This Params instance is immutable.")`. Otherwise set existing param; missing name -> `AttributeError` with `_key_error_string`.
  - `__getattr__` / `__getitem__`: missing name -> `AttributeError` with `_key_error_string`.
  - `__dir__` returns sorted param names. `__contains__`, `__len__` work on `_params`.
  - `__eq__`: only equal to another `Params` with equal `_params` (uses `_Param.__eq__`).
  - `__str__` / `_to_string`: emits multi-line block with params sorted by name; nested `Params` rendered recursively.
  - `_similar_keys(name)`:
    - Builds list of keys with >0.5 overlap of 3-char substrings from `name`.
  - `_key_error_string(name)`:
    - If similar keys exist: `"name (did you mean: [k1,k2])"` with keys sorted.
  - `define(name, default_value, description)`:
    - Reject if immutable → `TypeError("This Params instance is immutable.")`.
    - Assert `name` is `str` matching `^[a-z][a-z0-9_]*$` (raises `AssertionError`).
    - If already defined, `AttributeError("Parameter %s is already defined" % name)`.
  - `freeze()` sets `_immutable=True`; `is_immutable()` returns bool.
  - `_get_nested("a.b[0].c")`:
    - Supports dot navigation into nested `Params`.
    - Supports list indexing in a segment via `name[index]`.
    - If missing segment: `AttributeError` with partial path.
    - If intermediate is not `Params`: `AssertionError("Cannot introspect <type> for <path>")`.
  - `set(**kwargs)`:
    - If immutable -> `TypeError("This Params instance is immutable: %s" % self)`.
    - Dotted names traverse `_get_nested`; missing keys -> `AttributeError` with similar key hints.
    - Returns `self`.
  - `get(name)`:
    - Dotted names traverse `_get_nested`; missing -> `AttributeError` with similar key hints.
  - `delete(*names)`:
    - If immutable -> `TypeError("This Params instance is immutable.")`.
    - Dotted names traverse `_get_nested`; missing -> `AttributeError` with similar key hints.
    - Returns `self`.
  - `iter_params()` yields `(name, value)` for all params (dict iteration order).
  - `copy()` deep-copies `_params` and preserves `_immutable`.
- `copy_params_to(from_p, to_p, skip=None)`:
  - For each param in `from_p`, sets into `to_p`.
  - Nested `Params` are copied via `p.copy()`.
  - `skip` list omits names.
- `InstantiableParams(Params)`:
  - Defines param `cls`.
  - `instantiate()`:
    - If `cls.__init__` has exactly 2 parameters and `cls` has attribute `params` and `'params'` is a parameter: call `cls(self)`.
    - Otherwise, build inverted index of all leaf (non-Params) values; pass matching `__init__` parameter names (excluding `self`, `cls`, varargs/kwargs).
    - Additionally pass any `allowed_kwargs` present and not `None`.
- `update_params(params, args)`:
  - Recursively traverses params; if leaf key exists in `args`, sets and removes from `args`.
  - Unmatched keys remain in `args`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/hyperparams.rs`.
- Rust public API surface: `Params`, `ParamValue`, `InstantiableParams`, `copy_params_to`, `update_params`, `ParamsFactory`.
- Data model mapping:
  - Python `_Param` → Rust `Param` (name/value/description).
  - Python `Params` → Rust `Params` with `BTreeMap` storage.
  - Python raw values → Rust `ParamValue` (including `External` for TF tensors / opaque handles).
- Feature gating: none currently; TF handle storage likely lives in `ParamValue::External` (tie into `monolith-tf` if needed).
- Integration points: `base_layer.rs`, `base_task.rs`, `base_model_params.rs`, `model_registry.rs`.

**Implementation Steps (Detailed)**
1. Align Rust `Params` error messages with Python (TypeError/AttributeError strings).
2. Match `__str__` formatting (sorted keys, indent, list/dict rendering, quoting).
3. Implement `_similar_keys` overlap logic and error hints exactly.
4. Support list indexing in dotted paths with Python-compatible errors.
5. Preserve Tensor/opaque value deepcopy semantics (reference copy).
6. Implement `InstantiableParams.instantiate` reflection-style path or provide compatible factory shim.
7. Ensure `copy_params_to` surfaces missing keys (don’t silently drop errors).
8. Add tests mirroring `hyperparams_test.py` including exact error strings.

**Tests (Detailed)**
- Python tests: `monolith/core/hyperparams_test.py`.
- Rust tests: add `monolith-rs/crates/monolith-core/tests/hyperparams.rs`.
- Cross-language parity test: add fixture-driven JSON snapshot compare of `to_string`, errors, and nested access.

**Gaps / Notes**
- Rust currently raises `MonolithError::ConfigError` rather than `TypeError`/`AttributeError` equivalents; messages differ.
- Rust `Params` does not implement equality; Python `Params.__eq__` is relied on in tests.
- Rust `InstantiableParams` uses `ParamsFactory` (no reflection or `allowed_kwargs` handling).
- `copy_params_to` in Rust ignores `set` errors (should propagate to match Python exceptions).
- String rendering differs (Python uses sorted `_SortedDict` repr and special `Repr()` hook).
- Rust treats list navigation errors differently (`Expected list index` vs Python `Cannot introspect` assertion).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/hyperparams_test.py`
<a id="monolith-core-hyperparams-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 277
- Purpose/role: Unit tests defining required semantics for `Params` behavior, error messages, and string rendering.
- Key symbols/classes/functions: `ParamsTest`, `TestEnum`.
- External dependencies: `unittest`, `tensorflow`, `absl.flags/logging`, `monolith.core.hyperparams`.
- Side effects: none (tests only).

**Required Behavior (Detailed)**
- `test_equals`:
  - `Params.__eq__` compares only name+value; nested `Params` must compare deeply; non-Params should not be equal.
- `test_deep_copy`:
  - `Params.copy()` deep-copies nested `Params`.
  - `tf.Tensor` values are **shared** (same object identity) in copied params.
- `test_copy_params_to`:
  - Copy respects `skip` list; `dest` only contains copied keys.
- `test_define_existing`:
  - Duplicate `define` raises `AttributeError` with "already defined".
- `test_legal_param_names`:
  - Invalid names (`None`, empty, `_foo`, `Foo`, `1foo`, `foo$`) raise `AssertionError`.
  - Valid examples: `foo_bar`, `foo9`.
- `test_set_and_get`:
  - Setting undefined param via `.set` or `setattr` raises `AttributeError` mentioning name.
  - `set/get` works; `delete` removes; subsequent access raises `AttributeError`.
- `test_set_and_get_nested_param`:
  - Dotted traversal with nested `Params` works for `get`, `set`, `delete`.
  - `set` on missing path raises `AttributeError` with dotted name.
  - Attempting to navigate into non-Params (e.g., dict) raises `AssertionError("Cannot introspect ...")`.
- `test_freeze`:
  - Frozen params reject `set`, `setattr`, `delete`, `define` with `TypeError`.
  - `get` on missing still raises `AttributeError`.
  - Nested params remain mutable after parent frozen.
  - Attempt to set `_immutable` attribute raises `TypeError`.
  - `copy()` of frozen params is still immutable.
- `test_to_string`:
  - `str(params)` yields exact multi-line formatting:
    - Sorted keys.
    - Strings quoted.
    - Dicts rendered with sorted keys and nested `Params` converted to dicts.
    - Lists render nested `Params` as dicts.
- `test_iter_params`:
  - Iteration yields all keys/values (order not asserted).
- `test_similar_keys`:
  - Error message for misspelled attribute includes sorted similar keys:
    - `actuvation (did you mean: [activation,activations])`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/hyperparams.rs`.
- Rust public API surface: `Params`, `ParamValue`, `InstantiableParams`, `copy_params_to`, `update_params`.
- Data model mapping: implement Rust tests mirroring Python assertions and error text.
- Feature gating: none.
- Integration points: Rust tests in `monolith-rs/crates/monolith-core/tests`.

**Implementation Steps (Detailed)**
1. Add Rust unit tests that mirror each Python test case.
2. Build helper to compare error strings for invalid operations.
3. Ensure `Params` equality is implemented for Rust tests.
4. Add string formatting parity checks using exact expected text.
5. Validate frozen behavior and nested mutability.

**Tests (Detailed)**
- Python tests: TODO (manual)
- Rust tests: TODO (manual)
- Cross-language parity test: TODO (manual)

**Gaps / Notes**
- TODO (manual)

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/mixed_emb_op_comb_nws.py`
<a id="monolith-core-mixed-emb-op-comb-nws-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 421
- Purpose/role: Keras layer for neural width search (NWS) over embedding sizes, with optional teacher distillation transform.
- Key symbols/classes/functions: `TeacherEmbeddingTransform`, `MixedEmbedOpComb`.
- External dependencies: `numpy`, `tensorflow`, `tensorflow.keras.layers.Layer/InputSpec`.
- Side effects: creates trainable TF variables, prints to stdout, uses TF random sampling.

**Required Behavior (Detailed)**
- `TeacherEmbeddingTransform(max_choice_per_embedding, teacher_embedding_sizes_list)`:
  - Stores arrays; asserts same length.
  - `build(input_shape)`:
    - Input is 2D; last dim equals sum(teacher sizes).
    - Creates `teacher_embedding_transform_weight` with shape `[sum(max_choice * size), 1]` initialized with `TruncatedNormal(stddev=0.15)`.
    - Creates `teacher_embedding_transform_bias` with shape `[sum(max_choice)]` initialized with zeros.
    - Calls `_snapshot_for_serving` on both (method not defined in this class).
  - `call(inputs)`:
    - Slices teacher embedding per size; for each slice, reshapes weight to `[size, max_choice]` and matmul.
    - Concats results along axis 1 and adds bias.
  - `compute_output_shape` raises `NotImplementedError`.
  - `get_config` returns max choices + teacher sizes.
- `MixedEmbedOpComb(slot_names, embedding_size_choices_list, warmup_steps, pretraining_steps, teacher_embedding_sizes_list=None, distillation_mask=False)`:
  - Asserts slot names length matches choices list; prints lengths.
  - Computes per-slot num choices, per-slot max embedding choice (sum of sizes), total embedding size, max num choices.
  - **Note:** `teacher_embedding_sizes_list` is ignored (set to `None`), so teacher path is disabled.
  - `build(input_shape)`:
    - Asserts input is 2D; verifies total input dim matches total embedding size.
    - Creates `arch_embedding_weights` variable of length sum(num_choices), init uniform(-1e-3, 1e-3).
    - For each slot:
      - Softmax over weights slice; compute entropy (softmax_cross_entropy_with_logits_v2).
      - Compute expected embedding dims as weighted sum of choice sizes.
      - If first choice size is 0, scale its prob by warmup schedule based on global step.
      - Create per-choice masks (ranges over max_emb_choice), pad to max_num_choices.
      - Pretraining: if global_step < pretraining_steps, probability hard-coded to `[0.5, 0.5]` (assumes 2 choices).
      - Sample choice via `tf.random.categorical`, one-hot, select mask.
      - Apply straight-through trick: mask * (1 + w - stop_gradient(w)).
    - Concatenates per-slot masks into `_arch_embedding_masks_multipler`.
    - Stores `_arch_entropy`, `_expected_emb_dims`, `_expected_zero_embedding_size_weights`,
      `_arch_embedding_weights_after_softmax_list`.
  - `call(inputs)`:
    - Computes `masked_embedding = embedding * _arch_embedding_masks_multipler`.
    - If teacher path active:
      - Builds `TeacherEmbeddingTransform`, transforms teacher embedding.
      - Computes distillation MSE against masked embedding (optionally with mask).
      - Returns `(mixed_embedding, distillation_loss, teacher_embedding_transform.name)` but `mixed_embedding` is undefined (bug).
    - Else returns `masked_embedding`.
  - `get_config` includes `slot_names`, `embedding_size_choices_list`, `warmup_steps`, `teacher_embedding_sizes_list` (pretraining_steps and distillation flag omitted).
  - `get_arch_embedding_weights()` returns variable; `get_summaries()` returns entropy, expected dims, and weight list.
- Determinism: sampling uses TF RNG; dependent on global step and random categorical.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/src` (no existing equivalent).
- Rust public API surface: none; would need a layer module for NWS + distillation.
- Data model mapping: Keras layers + TF ops → Rust tensor ops (Candle/TF runtime).
- Feature gating: requires TF runtime or compatible tensor ops.
- Integration points: embedding layer selection / search in training pipeline.

**Implementation Steps (Detailed)**
1. Decide whether to support NWS (sampling-based) in Rust backend.
2. If yes, implement mask sampling, expected dims, and entropy summaries.
3. Add teacher distillation transform and MSE loss path.
4. Ensure global step and warmup/pretraining schedules are wired.
5. Fix Python bugs when porting (teacher path, mixed_embedding).

**Tests (Detailed)**
- Python tests: none in repo.
- Rust tests: add unit tests for mask construction, sampling shapes, and expected dims.
- Cross-language parity test: compare mask selection behavior under fixed RNG seeds.

**Gaps / Notes**
- `teacher_embedding_sizes_list` is ignored in `__init__` (always `None`).
- `call()` returns `mixed_embedding` in teacher path, but it is never defined.
- `pretraining_steps` only supports 2 choices (`[0.5, 0.5]` hard-coded).
- `_snapshot_for_serving` is called but not defined on `Layer` (likely missing mixin).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/model.py`
<a id="monolith-core-model-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 320
- Purpose/role: Deprecated Sail-like TPU Model wrapper for configuring TPU embedding tables, input pipelines, and pooling.
- Key symbols/classes/functions: `Model`, `create_input_fn`, `create_feature_and_table_config_dict`, `sum_pooling`, `init_slot_to_dims`.
- External dependencies: `tensorflow`, `tpu_embedding`, `absl.logging`, `math`, `FeatureSlot/FeatureColumnV1/Env`.
- Side effects: reads vocab file, logs size info, builds TF datasets/TPU table configs.

**Required Behavior (Detailed)**
- `Model.__init__(params)`:
  - Reads `vocab_size_per_slot`, logs if fixed.
  - Builds vocab dict from file; constructs `Env` and runs `init_slot_to_dims()`.
- `_create_vocab_dict(file_path, vocab_size_per_slot=None)`:
  - Reads TSV with `slot_id<TAB>count`.
  - Skips non-digit slot IDs; uses fixed vocab size if provided.
  - Returns `{slot_id: vocab_size}`.
- `_get_feature_map()`: abstract; must return TF parse feature map.
- `_post_process_example(example)`:
  - For each slot in `env.slot_to_dims`:
    - If `vocab_size_per_slot` set, mod values in `slot_{id}_0`.
    - Duplicates `slot_{id}_0` into `slot_{id}_{i}` for each additional dim.
- `create_input_fn(file_pattern, repeat=True)`:
  - Returns `input_fn(params)` using TF Dataset:
    - `list_files(shuffle=False)`, shard by `context.current_input_fn_deployment()`.
    - Optional per-shard skip from `params["shard_skip_file_number"]`.
    - `interleave(TFRecordDataset, cycle_length, num_parallel_calls, deterministic=False)`.
    - `batch(drop_remainder=True).map(parse_example, num_parallel_calls=AUTOTUNE, deterministic=False)`.
    - `repeat()` if requested; `prefetch(AUTOTUNE)`.
- `_padding_8(dim)`:
  - Rounds up to multiple of 8.
- `_get_slot_number(optimizer, use_gradient_accumulation)`:
  - Maps TPU optimizer class → slot count:
    - FTRL: 3, Adagrad: 2, Adam: 3, SGD: 1; adds 1 if gradient accumulation.
  - Else asserts unsupported optimizer (assert uses truthy string; ineffective).
- `_get_max_slot_number()`:
  - Iterates env slots and dims; chooses bias vs vec optimizer per dim; returns max slot count.
- `create_feature_and_table_config_dict()`:
  - Requires `env.is_finalized()`.
  - For each slot/dim: builds `tpu_embedding.TableConfig` and `FeatureConfig`.
  - Computes and logs embedding table sizes (raw, padded-to-8, padded+max-slot).
  - Returns `(feature_to_config_dict, table_to_config_dict)`.
- `sum_pooling(fc_dict, input_map, features, dim, total_embeddings, add_into_embeddings=True)`:
  - For each slot in `features`, calls `fc_dict[slot].add_vector(dim)`, appends to totals, populates `input_map` with unique keys.
  - Returns single embedding if one feature; else `tf.add_n`.
- `logits_fn()`, `create_model_fn()` are abstract.
- `init_slot_to_dims()` calls `logits_fn()`, `env.finalize()`, logs slot dims.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` (no direct equivalent).
- Rust public API surface: none; core training uses different abstractions.
- Data model mapping: TF TPU embedding configs have no Rust analog.
- Feature gating: requires TF runtime backend.
- Integration points: input pipelines, embedding table config, TPU training loop.

**Implementation Steps (Detailed)**
1. Decide whether to port deprecated Model API or replace with `base_embedding_task` parity.
2. If ported, implement vocab dict parsing, dataset sharding, and TFRecord pipeline equivalents.
3. Add TPU embedding config generator or document unsupported feature in Rust.
4. Implement pooling helper and slot/embedding table sizing logs.

**Tests (Detailed)**
- Python tests: none in repo.
- Rust tests: none (integration tests would be required with TF runtime).
- Cross-language parity test: compare table config dicts and size calculations.

**Gaps / Notes**
- File is marked deprecated; likely superseded by `base_embedding_task.py`.
- `Env` constructor here omits `params` argument (signature mismatch with current `Env`).
- `_get_slot_number` uses `assert("message")` which never fails; should raise.
- Depends on `env.slot_to_dims` and `env.slot_to_config` which do not exist in current `Env` implementation.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/model_imports.py`
<a id="monolith-core-model-imports-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 104
- Purpose/role: Dynamic import helpers for task/model parameter modules.
- Key symbols/classes/functions: `_Import`, `ImportAllParams`, `ImportParams`.
- External dependencies: `importlib`, `absl.logging`.
- Side effects: imports Python modules dynamically; logs import results.

**Required Behavior (Detailed)**
- `_Import(name)`:
  - Logs attempt; imports module; logs success; returns True on success.
  - On ImportError, logs error and returns False.
- `ImportAllParams(task_root=_ROOT, task_dirs=_DIRS, require_success=False)`:
  - For each `task` in `task_dirs`, attempts to import `{task_root}.{task}.params.params`.
  - If `require_success` and nothing imported, raises `LookupError`.
  - Note: code defines `module_str` with `path` but `path` is undefined (bug); actual import uses `.params.params`.
- `ImportParams(model_name, task_root, task_dirs, require_success=True)`:
  - Expects `model_name` to contain a dot; else ValueError.
  - Extracts `model_module` and attempts to import it directly.
  - For built-in tasks, if `model_module` starts with `{task}.`, builds module path `{task_root}.{task}.params.{path}` and attempts import.
  - If `require_success` and no import succeeded, raises LookupError with guidance.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/model_imports.rs`.
- Rust public API surface: helper functions to load/register model param types (likely via registry rather than dynamic import).

**Implementation Steps (Detailed)**
1. Provide a Rust registry-based import mechanism (explicit registration, no dynamic import).
2. Mirror error messages and logging at call sites.
3. Document that Python dynamic import is replaced by static registration.

**Tests (Detailed)**
- Python tests: none specific.
- Rust tests: verify registry lookup error messages when missing.

**Gaps / Notes**
- Python uses dynamic import; Rust should use explicit registration or plugin loading if required.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/model_registry.py`
<a id="monolith-core-model-registry-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 174
- Purpose/role: Global registry for `SingleTaskModelParams` classes with dynamic import helpers.
- Key symbols/classes/functions: `_ModelRegistryHelper`, `RegisterSingleTaskModel`, `GetAllRegisteredClasses`, `GetClass`, `GetParams`.
- External dependencies: `absl.logging`, `model_imports`, `SingleTaskModelParams`.
- Side effects: stores classes in global dict; logs registration; raises on duplicates.

**Required Behavior (Detailed)**
- Registry maps keys to param classes. Key format: `<module>.<ClassName>`; also a shortcut key with `monolith.tasks.` prefix stripped and `params.` removed.
- `_RegisterModel(src_cls)`:
  - Registers both full and shortcut keys.
  - Raises ValueError on duplicate key.
  - Logs module registration once per module.
- `RegisterSingleTaskModel` decorator:
  - Validates subclass of `SingleTaskModelParams` else TypeError.
  - Registers class and returns it.
- `GetAllRegisteredClasses()`:
  - Logs warning if none registered.
- `GetClass(class_key)`:
  - Raises LookupError if missing; logs known models.
- `GetParams(class_key)`:
  - Instantiates model params class, calls `.task()` to get task config.
- Module-level helpers call `model_imports.ImportAllParams` or `ImportParams` before returning registry results.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/model_registry.rs`.
- Rust public API surface: registry with registration macro/trait and lookup by string key.

**Implementation Steps (Detailed)**
1. Implement a global registry (static map) for model param factories.
2. Provide a registration macro that enforces trait bounds.
3. Implement lookup with detailed error listing known keys.
4. Replace dynamic import with explicit registration in Rust.

**Tests (Detailed)**
- Python tests: none specific.
- Rust tests: register dummy models, verify duplicate detection and lookup errors.

**Gaps / Notes**
- Shortcut key behavior must be preserved for compatibility with existing names.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/optimizers.py`
<a id="monolith-core-optimizers-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 25
- Purpose/role: Maps string names to TensorFlow v1 optimizer classes.
- Key symbols/classes/functions: `optimizers` dict.
- External dependencies: `tf.compat.v1.train` optimizers.
- Side effects: none.

**Required Behavior (Detailed)**
- `optimizers` contains:
  - `'adagrad'`: `AdagradOptimizer`
  - `'momentum'`: `MomentumOptimizer`
  - `'rmsprop'`: `RMSPropOptimizer`
  - `'adam'`: `AdamOptimizer`

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-optimizer/src/lib.rs`.
- Rust public API surface: map from string to optimizer factory or enum.

**Implementation Steps (Detailed)**
1. Define optimizer registry in Rust with equivalent names.
2. Map to native implementations or TF bindings if used.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: verify lookup table matches expected keys.

**Gaps / Notes**
- Rust may not support all TF optimizers natively; document fallbacks.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/py_utils.py`
<a id="monolith-core-py-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 313
- Purpose/role: NestedMap container with attribute access, flatten/pack utilities, and validation.
- Key symbols/classes/functions: `NestedMap` and its methods.
- External dependencies: `six`, `re`, `numpy` (imported but unused here).
- Side effects: enforces key validation on set.

**Required Behavior (Detailed)**
- Keys must be valid identifiers (regex `[A-Za-z_][A-Za-z0-9_]*`) and not reserved dict attributes.
- Attribute access maps to items; missing keys raise AttributeError listing available attributes.
- `GetItem` and `Get` support dotted key paths; no list indexing.
- `Set` creates nested maps along dotted path; raises if intermediate is not map/dict.
- `Flatten`, `FlattenItems`, `Pack`, `Transform`, `IsCompatible`, `Filter`, `FilterKeyVal`, `DebugString`, `VLog` mirror Lingvo-style NestedMap.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/py_utils.rs`.
- Rust public API surface: `NestedMap` with typed key validation and traversal helpers.

**Implementation Steps (Detailed)**
1. Implement NestedMap with key validation and attribute-like access (maybe via methods).
2. Provide flatten/pack utilities preserving order (sorted keys).
3. Implement filter logic with delete sentinel behavior.

**Tests (Detailed)**
- Python tests: indirect via core layers.
- Rust tests: unit tests for key validation, Get/Set, Flatten/Pack compatibility.

**Gaps / Notes**
- Python uses dynamic attributes; Rust should expose explicit methods.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/testing_utils.py`
<a id="monolith-core-testing-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 203
- Purpose/role: Test helper for Keras/BaseLayer compatibility; adapted from TF Keras testing_utils.
- Key symbols/classes/functions: `layer_test`.
- External dependencies: TF internal Keras/test utilities.
- Side effects: builds models, runs predict, validates shapes/dtypes.

**Required Behavior (Detailed)**
- `layer_test`:
  - Generates `input_data` if not provided; uses `input_shape` and `input_dtype` defaults.
  - Instantiates layer with `kwargs`; optionally calls `adapt`.
  - Tests `get_weights`/`set_weights` and re-instantiation with `weights` kwarg.
  - Builds functional model `y = layer(x)` and checks output dtype.
  - Checks expected output shape and computed output signature.
  - Runs `model.predict` and compares output to expected if provided.
  - Chooses assertion method based on dtype (string vs numeric).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/tests/testing_utils.rs`.
- Rust public API surface: test helper for layer validation (if Rust layers exist).

**Implementation Steps (Detailed)**
1. Implement a minimal test harness for Rust layers with shape/dtype checks.
2. If Rust uses different layer API, map semantics accordingly.

**Tests (Detailed)**
- Python tests: used by layer tests in core.
- Rust tests: provide helper functions for layer unit tests.

**Gaps / Notes**
- TF internal APIs used here are not available in Rust; might require simplified harness.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/tpu_variable.py`
<a id="monolith-core-tpu-variable-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 214
- Purpose/role: ReplicatedVariable wrapper for TPU context handling and resource variable ops.
- Key symbols/classes/functions: `ReplicatedVariable`, `_enclosing_tpu_context`, `_tensor_conversion`.
- External dependencies: TF internal ops.
- Side effects: registers tensor conversion function globally.

**Required Behavior (Detailed)**
- `_enclosing_tpu_context()` walks control flow contexts to find XLA TPU context.
- `ReplicatedVariable`:
  - Wraps list of per-replica variables; exposes `handle` that is replicated in TPU context.
  - Implements `assign`, `assign_add`, `assign_sub`, `read_value` using resource ops.
  - Provides `initializer`, `dtype`, `shape`, `get_shape`, `to_proto` by delegating to primary var.
  - `_should_act_as_resource_variable` is a no-op placeholder.
- Registers tensor conversion function and dense tensor like type (pre-TF2.3).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/src/tpu_variable.rs`.
- Rust public API surface: replicated variable wrapper if TF TPU is supported.

**Implementation Steps (Detailed)**
1. Provide wrapper around per-replica variables with TPU context awareness.
2. Implement assign/read operations using TF runtime or stub.
3. Ensure tensor conversion registration or equivalent behavior in Rust.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: optional integration tests if TPU runtime is available.

**Gaps / Notes**
- TF TPU context is highly specific; Rust likely needs a bridge or stubs.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/util.py`
<a id="monolith-core-util-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 269
- Purpose/role: GCS utilities, checkpoint reload helpers, and dataset date filtering.
- Key symbols/classes/functions: `download_gcs_file`, `update_params`, `range_dateset`.
- External dependencies: `google.cloud.storage`, `tensorflow.compat.v1`, `gsutil` via subprocess.
- Side effects: network calls to GCS, subprocess execution, logging.

**Required Behavior (Detailed)**
- `get_bucket_name_and_relavite_path(gs_file_path)` parses `gs://bucket/path` into bucket + relative path.
- `download_gcs_file` and `download_gcs_file_with_relative_path` fetch blobs to local filename.
- `list_gcs_files_with_prefix` returns bucket and blob iterator for prefix.
- `parse_example_number_meta_file` reads `file,count` lines (comma separated) and ensures file names are ascending.
- `calculate_shard_skip_file_number` computes per-shard skip counts based on completed steps and batch sizes.
- `get_checkpoint_completed_step_number` scans GCS checkpoint `.meta` files to find max step.
- `update_params`:
  - Computes batch sizes based on shard count (TPU workers) and validates consistency.
  - Uses checkpoint step to compute `shard_skip_file_number` based on meta file.
- `get_per_file_example_numbers_for_checkpoint_reload`:
  - Uses `gsutil ls` on dataset path, checks ordering, and matches against meta list.
- `range_dateset` filters dataset elements by date substring between start/end dates.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/util.rs`.
- Rust public API surface: GCS helpers and dataset date filtering.

**Implementation Steps (Detailed)**
1. Implement GCS helpers (use Google Cloud Storage SDK or `gsutil` fallback).
2. Port checkpoint reload logic and shard skip calculations.
3. Implement dataset date range filter (likely for TF dataset bridge).

**Tests (Detailed)**
- Python tests: `monolith/core/util_test.py`
- Rust tests: replicate range_dateset filtering behavior.

**Gaps / Notes**
- `gsutil` subprocess usage may need replacement or explicit dependency in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/util_test.py`
<a id="monolith-core-util-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 149
- Purpose/role: Tests `range_dateset` date filtering behavior.
- Key symbols/classes/functions: `UtilTest.test_range_dataset_*`.
- External dependencies: `tensorflow.compat.v1`.
- Side effects: none.

**Required Behavior (Detailed)**
- Tests that `range_dateset` filters dataset elements based on date substring in path.
- Covers single date, multiple dates, out-of-bound ranges, missing start or end date.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/tests/util.rs`.
- Rust public API surface: `range_dateset` equivalent.

**Implementation Steps (Detailed)**
1. Port tests using a mock dataset list and filter function.
2. Ensure output order and content match Python.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: same scenarios.

**Gaps / Notes**
- Requires a dataset abstraction; may be tested with plain lists in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/core/variance_scaling.py`
<a id="monolith-core-variance-scaling-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 188
- Purpose/role: Numpy-based variance scaling initializer (truncated/untruncated normal or uniform).
- Key symbols/classes/functions: `_compute_fans`, `VarianceScaling.__call__`, `get_config`.
- External dependencies: `numpy`, `scipy.stats.truncnorm`.
- Side effects: uses NumPy RNG (seeded per call).

**Required Behavior (Detailed)**
- `_compute_fans(shape, data_format)` computes fan_in/fan_out for dense or conv shapes; supports `channels_first/last`.
- `VarianceScaling.__init__` validates `scale > 0`, `mode` in `fan_in/fan_out/fan_avg`, distribution in `truncated_normal/untruncated_normal/uniform`.
- `__call__(shape, dtype)`:
  - Computes scaled variance based on mode.
  - Seeds NumPy RNG with `self.seed` each call.
  - For `truncated_normal`: uses scipy truncnorm with cutoff ±2 stddev, stddev adjusted by constant 0.87962566103423978.
  - For `untruncated_normal`: uses `np.random.normal`.
  - For `uniform`: uses `np.random.uniform` with limit `sqrt(3*scale)`.
- `get_config()` returns dict with scale/mode/distribution/seed.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/variance_scaling.rs`.
- Rust public API surface: `VarianceScaling` struct implementing initializer to produce arrays.

**Implementation Steps (Detailed)**
1. Port `_compute_fans` logic and mode handling.
2. Implement truncated normal sampling (use rand_distr or custom truncation) matching SciPy behavior.
3. Ensure seeding behavior matches (seed per call).

**Tests (Detailed)**
- Python tests: none.
- Rust tests: sample shape and distribution sanity checks; seed determinism.

**Gaps / Notes**
- Matching SciPy truncnorm exactly may require careful implementation.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/gpu_runner.py`
<a id="monolith-gpu-runner-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 226
- Purpose/role: GPU training/eval runner for TF Estimator, with optional Horovod multi-GPU support.
- Key symbols/classes/functions: `GPURunner`, `create_estimator`, `run`, CLI `main`.
- External dependencies: `tensorflow`, `horovod`, `mpi4py`, `absl.flags`, `model_registry`.
- Side effects: initializes Horovod, configures GPU visibility, trains/evaluates model, writes summaries.

**Required Behavior (Detailed)**
- CLI flags: `task`, `model_dir`, `save_checkpoints_steps`, `mode` (`train|eval|train_and_eval`).
- `GPURunner.__init__`:
  - Reads flags and task_param; sets `_mode`.
- `create_estimator(model_fn)`:
  - If `task_param.accelerator == 'horovod'`:
    - `hvd.rank()` controls checkpoint saving (only rank 0).
    - Configures `tf.compat.v1.ConfigProto` with XLA JIT ON and GPU memory growth.
    - Sets `visible_device_list` to local rank.
    - `num_gpus = hvd.size()`.
  - Else: `num_gpus = 1` and uses `tf.compat.v1.estimator.RunConfig`.
  - Returns `tf.compat.v1.estimator.Estimator` with params: train/eval batch sizes, accelerator, num_replicas, hvd_rank.
- `run()`:
  - Loads global step (or 0).
  - Instantiates task; builds input_fn_train/eval and model_fn.
  - If horovod: `hvd.init()`, sets visible GPU.
  - For `train`:
    - Horovod: uses `BroadcastGlobalVariablesHook(0)`.
    - Non-horovod: `est.train`.
  - For `eval`:
    - Computes `num_steps` from `eval_examples` and batch size.
    - Runs `est.evaluate` and writes summary under `model_dir/eval`.
  - For `train_and_eval`:
    - Loop train for `steps_per_eval` up to max_steps, evaluate and write summary.
    - Horovod uses MPI barrier after each eval cycle.
- `main`: fetches task params from registry and runs `GPURunner`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/gpu_runner.rs` (or CLI bin).
- Rust public API surface: runner struct + CLI entrypoint.
- Data model mapping: task params, estimator equivalent (likely still Python-backed or TensorFlow runtime binding).
- Feature gating: `tf-runtime` and `horovod` (if supported) features.

**Implementation Steps (Detailed)**
1. Recreate CLI flags and task registry lookup in Rust.
2. Decide how to execute training: native Rust pipeline or Python/TF bridge.
3. If bridging TF, maintain Horovod initialization and GPU pinning logic.
4. Mirror evaluation loop, summary writing, and MPI barrier behavior.

**Tests (Detailed)**
- Python tests: none specific.
- Rust tests: integration tests for CLI argument parsing and control flow (mocked task).

**Gaps / Notes**
- Full parity likely requires TF Estimator execution; consider keeping Python runner or embedding TF runtime.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/alert/alert_manager.py`
<a id="monolith-native-training-alert-alert-manager-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 31
- Purpose/role: Placeholder alert manager module defining a flag and stub accessor.
- Key symbols/classes/functions: `get_default_alert_manager()`.
- External dependencies: `absl.flags`, `absl.logging`, `google.protobuf.text_format`, `threading`, `time`, `traceback`.
- Side effects: defines `--monolith_alert_proto` flag at import time.

**Required Behavior (Detailed)**
- Flag definition:
  - `monolith_alert_proto` (string), default `""`, description: "The text format of alert proto."
- `get_default_alert_manager()`:
  - Returns `None`.
- No other behavior implemented.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` (if alerts are ported).
- Rust public API surface: optional alert manager accessor + config flag.
- Data model mapping: map CLI flag to config struct; no runtime behavior.
- Feature gating: none.
- Integration points: training runner or alert subsystem (not present).

**Implementation Steps (Detailed)**
1. Add a config flag or CLI option mirroring `monolith_alert_proto`.
2. Provide a stub `get_default_alert_manager` returning `None`/`Option::None`.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: none required (flag parsing only).
- Cross-language parity test: not applicable.

**Gaps / Notes**
- Module is largely empty; functionality likely lives elsewhere (or removed).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/alert/alert_manager_test.py`
<a id="monolith-native-training-alert-alert-manager-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 32
- Purpose/role: Placeholder test module; no actual tests defined.
- Key symbols/classes/functions: none (only `__main__` entrypoint).
- External dependencies: `absl.testing`, `absl.flags/app`, `google.protobuf.text_format`.
- Side effects: running as main executes `absltest.main()`.

**Required Behavior (Detailed)**
- No tests; `absltest.main()` called if executed.

**Rust Mapping (Detailed)**
- Target crate/module: none.
- Rust public API surface: none.
- Data model mapping: none.
- Feature gating: none.
- Integration points: none.

**Implementation Steps (Detailed)**
1. No Rust tests required unless alert manager gains functionality.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: none.
- Cross-language parity test: not applicable.

**Gaps / Notes**
- No tests defined; consider removing or adding real coverage if alerting is implemented.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/barrier_ops.py`
<a id="monolith-native-training-barrier-ops-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 158
- Purpose/role: TF-based barrier primitive to block non-chief workers until chief releases barrier; includes SessionRunHook integration.
- Key symbols/classes/functions: `BarrierOp`, `BarrierHook`, `BarrierAlreadyPlacedError`.
- External dependencies: `tensorflow`, `absl.logging`, `threading`, `time`, `basic_restore_hook` (imported, unused).
- Side effects: creates TF variables/placeholders, sleeps, logs every 60 seconds.

**Required Behavior (Detailed)**
- `BarrierAlreadyPlacedError`: raised when attempting to place a barrier twice.
- `BarrierOp(capacity, is_chief=True, wait_seconds=1, name_prefix="default", barrier_callbacks=None)`:
  - Creates `barrier_var` bool vector of length `capacity`:
    - If `is_chief`: `LOCAL_VARIABLES`; else `VARIABLES`.
  - Creates index placeholder `_idx_ph`.
  - `_place_op` sets `barrier_var[idx] = True`; `_remove_op` sets `False`.
  - `barrier_placed_tensor` references element 0.
  - `barrier_op_action` string variable + placeholder, assign op.
  - Uses a threading lock for thread safety.
- `place_barrier(session, action="")`:
  - If barrier already placed (index 0 True), raises `BarrierAlreadyPlacedError`.
  - Sets barrier at index 0 and assigns action; runs callbacks `(action, session)`.
- `remove_barrier(session)`:
  - Clears barrier at index 0; no checks.
- `is_barrier_placed(session)`:
  - Returns `session.run(barrier_placed_tensor)`.
- `wait_until_barrier_removed(session, index)`:
  - Validates `index` in `(0, capacity)` else `ValueError`.
  - Sets barrier at `index`, reads action, runs callbacks.
  - Loops until `barrier_placed_tensor` becomes False, sleeping `wait_seconds` and logging every 60s.
  - Removes its own barrier index after release.
- `is_all_blocked`/`is_none_blocked`:
  - Reads `barrier_var` and checks count equals capacity or 0.
- `get_unblocked_indices`/`get_blocked_indices`:
  - Returns indices with False/True in barrier vector.
- `BarrierHook(index, barrier_op)`:
  - `before_run` requests `barrier_placed_tensor`.
  - `after_run`: if `index > 0` and barrier placed, calls `wait_until_barrier_removed`.
- Threading: lock guards state changes; waiting uses sleep polling.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/barrier.rs`.
- Rust public API surface: `Barrier` trait, `InProcessBarrier`, `PsBarrier` (async).
- Data model mapping: TF variable barrier → async barrier/coordination via PS; no SessionRunHook equivalent.
- Feature gating: none (used in distributed training).
- Integration points: `runner.rs`, `distributed_ps.rs`.

**Implementation Steps (Detailed)**
1. Decide whether a TF-variable barrier is needed in Rust (likely no).
2. Map barrier semantics to async barrier APIs (waiting, timeouts).
3. Provide hook-like integration if training loop expects per-step blocking.

**Tests (Detailed)**
- Python tests: `monolith/native_training/barrier_ops_test.py`.
- Rust tests: add async barrier tests for arrival/release semantics.
- Cross-language parity test: not applicable unless TF runtime added.

**Gaps / Notes**
- TF `basic_restore_hook` import unused.
- Python barrier uses polling on tensor 0; Rust barrier is event-based and may not mirror per-step hook semantics.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/barrier_ops_test.py`
<a id="monolith-native-training-barrier-ops-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 104
- Purpose/role: Verifies BarrierOp placement/removal and BarrierHook blocking behavior.
- Key symbols/classes/functions: `BarrierOpsTest`.
- External dependencies: `tensorflow`, `monitored_session._HookedSession`.
- Side effects: spawns threads and TF sessions.

**Required Behavior (Detailed)**
- `test_basic`:
  - Place barrier; double place raises `BarrierAlreadyPlacedError`.
  - Remove barrier → `is_barrier_removed` true.
- `test_barrier_hook_not_blocked`:
  - Without barrier, hook does not block; global step reaches 5.
- `test_barrier_hook_blocked`:
  - Place barrier; worker thread blocks after 1 step.
  - Callback called with action, sets a variable to True.
  - Removing barrier allows training to finish; all barriers cleared.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/barrier.rs`.
- Rust public API surface: barrier trait/implementations.
- Data model mapping: no SessionRunHook analogue; tests should exercise async barrier directly.
- Feature gating: none.
- Integration points: training runner.

**Implementation Steps (Detailed)**
1. Add Rust tests for arrival/release semantics using in-process barrier.
2. Add integration test for PS barrier timeout if applicable.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: add async barrier tests for arrival/release semantics.
- Cross-language parity test: not applicable.

**Gaps / Notes**
- Rust barrier is async/event-based; no direct SessionRunHook equivalent.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/basic_restore_hook.py`
<a id="monolith-native-training-basic-restore-hook-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 72
- Purpose/role: SessionRunHook that invokes listener callbacks around checkpoint restore (no actual restore logic).
- Key symbols/classes/functions: `CheckpointRestorerListener`, `CheckpointRestorerHook`.
- External dependencies: `tensorflow.python.training.session_run_hook`, `absl.logging`.
- Side effects: logs on creation and restore calls.

**Required Behavior (Detailed)**
- `CheckpointRestorerListener`:
  - Interface with `begin`, `before_restore(session)`, `after_restore(session)`, `end(session)`; all no-ops by default.
- `CheckpointRestorerHook(listeners=None)`:
  - Logs "Create CheckpointRestorerHook."
  - `begin()` calls `listener.begin()` for each listener.
  - `after_create_session(session, coord)` calls `_restore(session)`.
  - `_restore(session)`:
    - Logs "Calling checkpoint restorer listeners."
    - Calls `before_restore(session)` then `after_restore(session)` on each listener.
    - **No actual restore actions performed in hook**.
- No `end()` implementation in hook; listener `end()` is never invoked.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/hooks.rs`.
- Rust public API surface: `Hook` trait + `CheckpointHook` (save only).
- Data model mapping: listener callbacks map to hook lifecycle events.
- Feature gating: none.
- Integration points: training loop hook list.

**Implementation Steps (Detailed)**
1. Add a Rust hook that runs listener callbacks at session start.
2. If restore is implemented in Rust, wire callbacks before/after restore.
3. Ensure lifecycle ordering matches Python (begin → after_create_session → before/after restore).

**Tests (Detailed)**
- Python tests: `monolith/native_training/basic_restore_hook_test.py`.
- Rust tests: add hook lifecycle test in `monolith-training`.
- Cross-language parity test: compare callback ordering under identical steps.

**Gaps / Notes**
- Hook does not implement `end()`, so listener `end()` is never called.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/basic_restore_hook_test.py`
<a id="monolith-native-training-basic-restore-hook-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 137
- Purpose/role: Verifies listener callbacks fire during `after_create_session` and do not repeat during training.
- Key symbols/classes/functions: `CountCheckpointRestorerListener`, `CountHook`, `CheckpointRestorerHookTest`.
- External dependencies: `tensorflow`, `session_run_hook`.
- Side effects: runs TF monitored sessions.

**Required Behavior (Detailed)**
- `test_restore_only_in_after_create_session`:
  - Listener `begin/before_restore/after_restore` called once at session creation.
  - Another hook receives `after_create_session` once before any `before_run/after_run`.
  - After training steps, listener counts remain unchanged; CountHook shows before_run/after_run/end increments.
- `test_two_listeners_with_restorer`:
  - Two listeners both receive begin/before/after restore once.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/hooks.rs`.
- Rust public API surface: hook lifecycle tests.
- Data model mapping: count callbacks during `on_start` or equivalent.
- Feature gating: none.
- Integration points: hook list execution order.

**Implementation Steps (Detailed)**
1. Add a Rust test that runs a hook list and validates callback order/counts.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: add CPU tensor tests for clipping behavior and immutability.
- Cross-language parity test: compare Rust outputs vs TF for fixed inputs.

**Gaps / Notes**
- GPU-only custom ops in Python; Rust needs equivalent or fallback path.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/clip_ops.py`
<a id="monolith-native-training-clip-ops-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 80
- Purpose/role: Custom gradient clipping with Monolith GPU ops and optional fused path.
- Key symbols/classes/functions: `_global_norm`, `clip_by_global_norm`.
- External dependencies: `tensorflow`, `device_utils`, `gen_monolith_ops` (custom ops).
- Side effects: none (pure tensor ops), but relies on custom TF ops.

**Required Behavior (Detailed)**
- `_global_norm(t_list)`:
  - Returns `None` if list empty.
  - Uses `gen_monolith_ops.global_l2_reduce` to compute L2 sum, then `sqrt`.
- `clip_by_global_norm(t_list, clip_norm, use_norm=None)`:
  - Requires `t_list` to be a list; else raises `TypeError("t_list should be a list")`.
  - If `t_list` empty, returns `(t_list, 0)`.
  - If `use_norm` provided: returns `monolith_clip_by_global_norm(t_list, use_norm, clip_norm)` and `use_norm`.
  - If in GPU placement context: uses fused op `monolith_clip_by_global_norm_fused(t_list, clip_norm)` (returns list, norm).
  - Otherwise computes `global_norm` via:
    - `_global_norm` if GPU context, else `tf.linalg.global_norm`.
  - Applies `monolith_clip_by_global_norm(t_list, global_norm, clip_norm)` and returns `(list_clipped, global_norm)`.
- Expected semantics match `tf.clip_by_global_norm` including NaN on inf norms.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-optimizer/src` (or `monolith-tensor`).
- Rust public API surface: clip-by-global-norm helper in optimizer utilities.
- Data model mapping: custom TF ops → native tensor ops (Candle/TF runtime).
- Feature gating: GPU vs CPU paths should be explicit.
- Integration points: optimizer step and gradient pre-processing.

**Implementation Steps (Detailed)**
1. Implement global norm computation and clipping on tensor lists.
2. Provide GPU-optimized path or document as CPU-only if missing.
3. Ensure NaN propagation on inf norms matches TF.
4. Add compatibility test vs `tf.clip_by_global_norm`.

**Tests (Detailed)**
- Python tests: `monolith/native_training/clip_ops_test.py`.
- Rust tests: add numeric tests for clip_by_global_norm including NaN/inf cases.
- Cross-language parity test: compare outputs against TF for fixed inputs.

**Gaps / Notes**
- Custom ops (`monolith_clip_by_global_norm_*`) are not available in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/clip_ops_test.py`
<a id="monolith-native-training-clip-ops-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 92
- Purpose/role: Validates custom clip-by-global-norm against TF behavior and input immutability.
- Key symbols/classes/functions: `ClipOpsTest`, `NormOpsTest`.
- External dependencies: `tensorflow`, `numpy`, `test_util`.
- Side effects: runs GPU-only tests if available.

**Required Behavior (Detailed)**
- `ClipOpsTest._test_clip_by_global_norm`:
  - Runs op on GPU; compares output to TF `clip_by_global_norm` (unless expected provided).
  - Asserts input tensors are not modified in-place.
- `test_clip_by_global_norm`:
  - Covers simple, uneven shapes, no clipping, zero norm, inf -> NaN, and large random grads.
- `NormOpsTest`:
  - `test_it` (GPU only) checks `_global_norm` for inf and known values.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-optimizer/tests`.
- Rust public API surface: clip-by-global-norm tests.
- Data model mapping: use CPU tensors unless GPU backend exists.
- Feature gating: GPU-only tests optional.
- Integration points: optimizer utilities.

**Implementation Steps (Detailed)**
1. Add CPU tests matching the Python expected outputs.
2. Add NaN/inf propagation test.
3. Add randomized large-shape test if memory allows.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: add temp-dir PS file round-trip test.
- Cross-language parity test: not applicable.

**Gaps / Notes**
- Rust uses different discovery stack; PS file persistence may not be needed.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/cluster_manager.py`
<a id="monolith-native-training-cluster-manager-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 184
- Purpose/role: Build TF cluster specs for distributed training and manage PS discovery persistence.
- Key symbols/classes/functions: `generate_session_config`, `get_training_cluster`, `_query_ps_cluster`, `_query_chief_addr`, `_save_ps_cluster_to_file`, `_fetch_ps_cluster_from_file`.
- External dependencies: `tensorflow`, `absl.logging`, `ServiceDiscovery`, `metric.cli`.
- Side effects: reads/writes PS cluster file via `tf.io.gfile`, sleeps on retries, emits metrics.

**Required Behavior (Detailed)**
- `emit_store(name, value, tagkv=None)`:
  - Delegates to `_MCLI.emit_store` (metrics).
- `generate_session_config(cluster_and_task=None)`:
  - If `None`, returns `ConfigProto(allow_soft_placement=True)`.
  - Else builds `ClusterSpec` and `ConfigProto(cluster_def=...)` with:
    - `device_filters` including `/job:ps` and `/job:chief`; non-chief adds its own job/task filter.
  - Sets `share_cluster_devices_in_session=True`.
  - Sets `experimental.share_session_state_in_clusterspec_propagation=True`.
  - Disables Grappler meta optimizer.
- `get_training_cluster(...)`:
  - If `index == 0` (chief):
    - If `num_redundant_ps`: try reading PS addrs from file (timeout=0); if insufficient, query discovery then save to file.
    - Else query discovery for PS addrs.
    - Builds cluster with `chief=[worker_addr]`, `worker=fake_worker_list`, `ps=ps_addrs`.
    - `task={"type":"chief","index":0}`.
  - Else (worker):
    - Gets `chief_addr` via `_query_chief_addr`.
    - Builds `fake_worker_list` of size `num_workers-1` and assigns `worker_addr` at `index-1`.
    - PS addrs from file (if redundant) or discovery.
    - `task={"type":"worker","index":index-1}`.
  - Asserts PS count equals `num_required_ps`.
- `_query_chief_addr(discovery)`:
  - Polls `discovery.query("worker")` until index 0 present; sleeps 5s between retries.
- `_query_ps_cluster(discovery, num_required_ps, model_name=None, cluster_type="stable")`:
  - Polls `discovery.query("ps")` until enough PS; logs count and emits metrics if model_name provided.
  - Returns sorted PS addresses by index, truncated to `num_required_ps`.
- `_save_ps_cluster_to_file(file_name, ps_addrs)`:
  - Writes comma-separated list to temp file and atomically renames.
- `_fetch_ps_cluster_from_file(file_name, timeout=1800)`:
  - Repeatedly attempts read; returns list if found, else empty after timeout.
- `_get_ps_cluster_file_name(model_dir, uuid)`:
  - Path: `<model_dir>/ps_cluster_dir/<uuid or "ps_info">`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/distributed.rs`, `py_discovery.rs`.
- Rust public API surface: `ClusterConfig`, `ServiceDiscovery` implementations.
- Data model mapping: TF `ClusterSpec` → Rust `ClusterConfig`; discovery polling → async discovery APIs.
- Feature gating: none.
- Integration points: distributed runner and service discovery.

**Implementation Steps (Detailed)**
1. Map discovery polling to Rust async discovery with retry/backoff.
2. Add PS cluster file persistence helpers (optional).
3. Provide cluster config builder mirroring fake worker list behavior if needed for TF_CONFIG parity.

**Tests (Detailed)**
- Python tests: `monolith/native_training/cluster_manager_test.py`.
- Rust tests: add unit test for PS file round-trip and cluster config validation.
- Cross-language parity test: compare generated cluster dictionaries for sample inputs.

**Gaps / Notes**
- Python uses fake worker list due to TF_CONFIG limitation; Rust cluster config may not need this.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/cluster_manager_test.py`
<a id="monolith-native-training-cluster-manager-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 35
- Purpose/role: Tests PS cluster file persistence helpers.
- Key symbols/classes/functions: `ClusterManagerTest.testBasic`.
- External dependencies: `os`, `cluster_manager`.
- Side effects: writes temp files under `TEST_TMPDIR`.

**Required Behavior (Detailed)**
- `testBasic`:
  - Writes PS addrs to file via `_save_ps_cluster_to_file`.
  - Reads via `_fetch_ps_cluster_from_file` and asserts equality.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests`.
- Rust public API surface: PS cluster file helper tests.
- Data model mapping: same string list round-trip.
- Feature gating: none.
- Integration points: optional in discovery integration.

**Implementation Steps (Detailed)**
1. Add Rust test that writes/reads PS cluster file in a temp dir.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: TODO (manual)
- Cross-language parity test: TODO (manual)

**Gaps / Notes**
- TODO (manual)

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/consul.py`
<a id="monolith-native-training-consul-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 149
- Purpose/role: Minimal Consul client for service lookup/register/deregister using HTTP or Unix socket.
- Key symbols/classes/functions: `Client`, `UnixHTTPConnection`, `ConsulException`.
- External dependencies: `six.moves.http_client.HTTPConnection`, `socket`, `threading`, `json`, `os`.
- Side effects: spawns a health-check thread on register; caches lookup results.

**Required Behavior (Detailed)**
- `UnixHTTPConnection(path)`:
  - Connects via UNIX domain socket to Consul.
- `Client.__init__()`:
  - Determines consul host:
    - `CONSUL_HTTP_HOST` or `TCE_HOST_IP` env vars.
    - Else uses `/opt/tmp/sock/consul.sock` if file exists.
    - Else defaults to `"127.0.0.1"`.
  - Port from `CONSUL_HTTP_PORT` or `2280`.
  - Initializes `_cache` and `_lock`.
- `lookup(name, timeout=3, cachetime=0)`:
  - If `cachetime>0` and cached entry is fresh, returns it.
  - Else uses `_lookup`, with longer timeout (30s) on cache miss.
  - Caches result with timestamp.
- `_lookup(name, timeout)`:
  - Uses Unix socket if host starts with `/`, else TCP.
  - GET `/v1/lookup/name?name=<name>&addr-family=dual-stack`.
  - If status != 200: logs error and returns `[]`.
  - Otherwise returns JSON-decoded list.
- `register(name, port, tags=None, check_script=None, host=None)`:
  - Builds payload with id `<name>-<port>`, TTL check (60s).
  - Adds tags as `["k:v"]`.
  - If `check_script` provided: replaces check with `interval=30s, script=...`.
  - Registers via PUT `/v1/agent/service/register`.
  - On non-200 → raises `ConsulException`.
  - Spawns daemon thread that periodically `GET /v1/agent/check/pass/service:<name>-<port>`; on socket error sleeps 2s and retries.
- `deregister(name, port, host=None)`:
  - PUT `/v1/agent/service/deregister/<name>-<port>`.
  - On non-200 → raises `ConsulException`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/discovery.rs`.
- Rust public API surface: `ConsulDiscovery` (stub), `ServiceDiscovery` traits.
- Data model mapping: Python client → Rust Consul discovery abstraction.
- Feature gating: `consul` feature.
- Integration points: distributed runner discovery.

**Implementation Steps (Detailed)**
1. Implement Consul HTTP client or keep stub and document missing features.
2. Add cache semantics and Unix socket support if parity required.
3. Add optional background health-check ticker.

**Tests (Detailed)**
- Python tests: `monolith/native_training/consul_test.py`.
- Rust tests: add mock HTTP tests for lookup/register/deregister (feature-gated).
- Cross-language parity test: compare HTTP request paths and payloads.

**Gaps / Notes**
- Python uses a ByteDance-specific `/v1/lookup/name` API, not stock Consul.
- Rust `ConsulDiscovery` targets standard Consul catalog APIs; endpoint mismatch.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/consul_test.py`
<a id="monolith-native-training-consul-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 59
- Purpose/role: Unit tests for Consul client lookup/register/deregister using mocked HTTPConnection.
- Key symbols/classes/functions: `ConsulTest`.
- External dependencies: `unittest.mock`, `six.moves.http_client.OK`.
- Side effects: none (network mocked).

**Required Behavior (Detailed)**
- `test_lookup`:
  - Mock HTTP 200 with JSON payload; `Client.lookup` returns decoded list.
- `test_register` / `test_deregister`:
  - Mock HTTP 200; call methods without error.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests`.
- Rust public API surface: Consul discovery tests.
- Data model mapping: mock HTTP client behavior.
- Feature gating: `consul`.
- Integration points: discovery subsystem.

**Implementation Steps (Detailed)**
1. Add mocked HTTP tests for lookup/register/deregister under `consul` feature.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: TODO (manual)
- Cross-language parity test: TODO (manual)

**Gaps / Notes**
- TODO (manual)

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/cpu_sync_training_test.py`
<a id="monolith-native-training-cpu-sync-training-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 360
- Purpose/role: End-to-end CPU sync training tests with Horovod for features, embeddings, sequence features, and distributed sync training.
- Key symbols/classes/functions: `FeatureTask`, `EmbeddingUpdateTask`, `FloatFeatureTask`, `SequenceFeatureTask`, `NonFeatureTask`, `CpuSyncTrainTest`, `DistributedSyncTrainTest`.
- External dependencies: `horovod.tensorflow`, `cpu_training`, `feature`, `embedding_combiners`, `device_utils`, `NativeTask`, `entry`, `advanced_parse`.
- Side effects: sets `MONOLITH_WITH_HOROVOD=True` env var; runs TF estimators and training loops.

**Required Behavior (Detailed)**
- Environment:
  - `MONOLITH_WITH_HOROVOD` must be set **before** importing `monolith.native_training`.
- `FeatureTask`:
  - Input: ragged int64 feature `[1,2,3,4]` repeated 5.
  - Model uses `FeatureSlotConfig`, one slice (dim=5), embedding lookup.
  - For TRAIN: computes loss on embedding, applies gradients via feature factory.
- `EmbeddingUpdateTask`:
  - Compares monolith embedding updates vs TF embedding lookup.
  - Uses `ConstantsInitializer(0)` and `AdagradOptimizer(0.1, accum=1)`.
  - Asserts equality between monolith embedding and TF embedding; increments global step.
- `FloatFeatureTask`:
  - Includes float feature; predictions from float feature sum.
  - Training uses ragged embedding for gradients; float feature only for predictions.
- `SequenceFeatureTask`:
  - Ragged sequence feature; uses `embedding_combiners.FirstN(2)`.
  - Loss from embeddings; predictions from sequence feature sum.
- `NonFeatureTask`:
  - Input dataset yields scalar; model returns constant loss and uses input as train op.
- `CpuSyncTrainTest`:
  - `test_cpu_training_feature/float_feature/sequence_feature/non_feature` run `CpuTraining` with `enable_sync_training=True`.
  - `test_embedding_update` trains 10 steps, compares embedding updates to TF.
- `DistributedSyncTrainTest`:
  - `test_basic` and `test_sparse_pipelining` invoke `distributed_sync_train` with config toggles (pipelined a2a, embedding_postpush).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests`.
- Rust public API surface: distributed training harness + Horovod equivalent (if any).
- Data model mapping: TF estimator + feature factory → Rust training loop abstractions.
- Feature gating: requires Horovod/TF runtime; Rust likely lacks direct support.
- Integration points: `CpuTraining`, feature pipeline, embedding update logic.

**Implementation Steps (Detailed)**
1. Determine whether Rust will support Horovod-like sync training.
2. If yes, add integration tests for feature pipeline and embedding updates.
3. If no, document tests as Python-only and provide alternative sync tests.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: TBD (requires distributed training support).
- Cross-language parity test: compare embedding update equivalence on a tiny synthetic dataset.

**Gaps / Notes**
- Tests assume Horovod is available and initialize `hvd` in-process.
- Uses `entry.ConstantsInitializer` and `entry.AdagradOptimizer` (must exist in Rust).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/cpu_training.py`
<a id="monolith-native-training-cpu-training-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 2449
- Purpose/role: Main CPU training orchestrator for `NativeTask` (local + distributed), covering feature table construction, embedding lookups/prefetch, checkpoint/restore, export, sync/async training, PS lifecycle, and metrics.
- Key symbols/classes/functions: `_combine_slices_as_table`, `_lookup_embedding_ids`, `_convert_parquets_to_instance`, `create_exporter`, `_CpuFeatureFactory`, `_FusedCpuFeatureFactory`, `_MetricsHeartBeatThread`, `CpuTrainingConfig`, `CpuTraining`, `DistributedCpuTrainingConfig`, `_prepare_server`, `_shutdown_ps`, `_join_ps`, `_do_worker_train`, `_do_worker_feature_engineering`, `distributed_train`, `distributed_sync_train`, `local_train_internal`, `local_feature_engineering_internal`, `local_train`.
- External dependencies: TensorFlow (graph/session/estimator, internal file_io/summary), absl flags/logging, numpy, gRPC-free but many Monolith modules (hash tables, export, sync hooks, dataset distribution, service discovery), and optional parquet/CityHash for roughsort.
- Side effects:
  - Sets/clears `TF_CONFIG` env var.
  - Starts TF servers (PS/worker), threads (metrics heartbeat, PS sync).
  - Writes files (`debugging_info.pb`, checkpoints, dense-only checkpoints, candidate item pb).
  - Alters global flags (`FLAGS.dataset_worker_idx`, `FLAGS.dataset_num_workers`, `FLAGS.monolith_alert_proto`).
  - Pushes tensors into TF collections for dequeued features and hooks.

**Required Behavior (Detailed)**
- Flags:
  - Defines `monolith_chief_alert_proto` flag; chief maps it to `FLAGS.monolith_alert_proto`.
- Utility helpers:
  - `_combine_slices_as_table(slices, hashtable_config)`:
    - Builds `EmbeddingHashTableConfig`, appends all slice segments.
    - If exporting, sets `EntryConfig.entry_type = SERVING`.
    - Collects `learning_rate_fn` per slice.
    - Calls `hashtable_config.mutate_table(table_config)`.
    - Returns `HashTableConfigInstance(table_config, learning_rate_fns)`.
  - `_lookup_embedding_ids(hash_table, name_to_embedding_ids)`:
    - Converts ragged tensors to `values` and calls `hash_table.lookup`.
  - `_convert_parquets_to_instance(parquets_path, instance_path)`:
    - Requires `parquets_path` to be a directory; selects **latest date subdir** (YYYYMMDD).
    - Requires `.snappy.parquet` files; reads `item_id` and `fids`.
    - Uses `CityHash64(str(item_id)) & ((1<<63)-1)` for item_id.
    - Writes `Instance` protobufs as `[u64_len][bytes]` records to `instance_path`.
  - `create_exporter(...)`:
    - `ExportMode.STANDALONE` → `StandaloneExporter`.
    - `ExportMode.DISTRIBUTED` → `DistributedExporter` with `dense_only`, `allow_gpu`, `clear_entry_devices`, `include_graphs`, `global_step_as_timestamp=config.enable_sync_training`, `with_remote_gpu`.
    - Else raises `ValueError("Invalid export_mode: ...")`.
- Feature factories:
  - `_CpuFeatureFactory.apply_gradients`:
    - `emb_grads = utils.propagate_back_gradients(grads_and_vars, embeddings.values())`.
    - Maps slot→ids and slot→grads; `global_step = tf.identity(get_or_create_global_step())`.
    - Enqueues `_push` via `AsyncFunctionMgr.add_async_function`, queue `postpush_queue`.
  - `_FusedCpuFeatureFactory.apply_gradients`:
    - Computes gradients on `/device:GPU:0`.
    - Calls `hash_table.apply_gradients(..., auxiliary_bundle, scale, skip_merge_id)` depending on `use_native_multi_hash_table`.
- Metrics:
  - `_MetricsHeartBeatThread` emits `training_heart_beat` with `{type: running/stopped}` every interval; flushes each time.
- `get_req_time(features)`:
  - Returns `features["req_time"][0]` if present; else `None`.
- `CpuTrainingConfig` (dataclass, gflags linked `use_dataservice`):
  - Fields/defaults (non-None):
    - `server_type="worker"`, `index=0`, `num_ps=0`, `num_workers=1`, `model_name=""`.
    - `filter_capacity=300000000`, `filter_split_num=7`, `filter_type=FilterType.SLIDING_HASH_FILTER`, `filter_equal_probability=True`, `hashtable_init_capacity=0`.
    - `embedding_prefetch_capacity=0`, `enable_embedding_postpush=False`, `enable_variable_prefetch=False`, `enable_variable_postpush=False`.
    - `enable_sync_training=False`, `enable_partial_sync_training=False`, `enable_gpu_training=False`, `processes_per_gpu=1`, `merge_sync_training_ckpt=True`.
    - `mode=tf.estimator.ModeKeys.TRAIN`, `enable_realtime_training=False`, `enable_async_optimize=False`.
    - `enable_pipelined_fwda2a=False`, `enable_pipelined_bwda2a=False`.
    - `profile_save_steps_interval=5000`, `reorder_fids_in_data_pipeline=False`.
    - `chief_timeout_secs=1800`, `dense_only_stop_training_when_save=False`.
    - `warmup_file="./warmup_file"`, `skip_zero_embedding_when_serving=False`, `max_rpc_deadline_millis=30000`.
    - `checkpoints_max_to_keep=10`, `cluster_type="stable"`, `max_slow_start_wait_minute=10`.
    - `enable_model_ckpt_info=False`, `feature_eviction_on_save=False`, `only_feature_engineering=False`.
    - `enable_variable_partition=True`, `enable_fused_layout=False`, `force_shutdown_ps=False`.
    - `clear_nn=False`, `continue_training=False`, `enable_model_dump=False`.
    - `enable_resource_constrained_roughsort=False`, `roughsort_items_use_parquet=False`.
    - `items_input_lagrangex_header=False`, `items_input_has_sort_id=False`.
    - `items_input_kafka_dump=False`, `items_input_kafka_dump_prefix=False`.
    - `num_extra_dsworker_on_gpu_worker=0`, `save_summary_steps=100`, `log_step_count_steps=100`.
  - Fields/defaults (None):
    - `use_native_multi_hash_table=None`, `partial_recovery=None`.
    - `tide_start_hour=None`, `tide_start_minute=None`, `tide_end_hour=None`, `tide_end_minute=None`, `tide_save_secs=None`.
    - `profile_some_steps_from=None`, `profile_with_nvprof_from_to=None`.
    - `save_checkpoints_secs=None`, `save_checkpoints_steps=None`, `dense_only_save_checkpoints_secs=None`, `dense_only_save_checkpoints_steps=None`.
    - `submit_time_secs=None`, `containers_ready_time_secs=None`.
    - `reload_alias_map=None`, `enable_alias_map_auto_gen=None`.
    - `roughsort_candidate_items_path=None`.
    - `device_fn=None`, `use_dataservice=None`.
  - `enable_full_sync_training` = `enable_sync_training and not enable_partial_sync_training`.
- Serving-config conversion:
  - `_make_serving_config_from_training_config` disables prefetch/postpush and sync flags; if sync training, disables sync and may set `num_ps = num_workers`.
  - `_make_serving_feature_configs_from_training_configs` sets entry_type to SERVING, sets `skip_zero_embedding`, and calls `cuckoo.SetInParent()`.
- Context helpers:
  - `make_native_task_context` builds `NativeTaskContext` with PS/worker indices and sync backend.
  - `is_chief`: MPI rank 0 for sync/partial sync; otherwise worker index 0.
- `CpuTraining.__init__`:
  - Requires `server_type == "worker"`.
  - Derives `model_name` from `task.p.metrics.deep_insight_name` or class name.
  - For realtime training: sets `partial_recovery=True` and `dense_only_save_checkpoints_secs=1800` if unset.
  - Defaults `use_native_multi_hash_table=True` if None.
  - Updates parser contexts and dataset flags.
  - Collects feature configs via `DumpUtils` (if dumped) or `_collect_feature_name_to_table_config`.
  - Builds serving feature configs and optional dummy merged table (if not native multi hash table).
  - Initializes fused-layout params and export context list.
- `CpuTraining.create_input_fn`:
  - Wraps task input_fn; optionally reorders fids via `distribution_ops.fused_reorder_by_indices`.
  - Uses `datasets.distribute` when `use_dataservice` and not exporting.
  - Always `prefetch(tf.data.AUTOTUNE)`.
- `CpuTraining.create_model_fn`:
  - Builds hash tables + hash filters (and sync clients).
  - Uses GPU device for hash filters if `use_gpu_emb_table`.
  - For exporting: returns hash_filters = `[None]`.
  - For sync training, uses in-worker hash table creation and queue configs.
  - Returns pipelined model_fn via `_get_pipelined_model_fn`.
- `CpuTraining._get_pipelined_model_fn`:
  - If no embedding features: disable feature_factory and return raw model_fn.
  - Defines restore hooks (`CheckpointRestorerHook`) for hash tables, filters, and `CustomRestoreListener`.
  - Save hooks:
    - `PartialRecoverySaver` with `max_to_keep`, `exempt_checkpoint_paths`, `skip_save` if not root node.
    - `HashTableCheckpointSaverListener`, `MultiHashTableCheckpointSaverListener`, `HashFilterCheckpointSaverListener`.
    - Optional `FidSlotCountSaverListener`, feature eviction listeners.
    - Dense-only saver hooks and optional `SyncTrainingSaverControlHook`.
    - Errors if both `save_checkpoints_secs` and `save_checkpoints_steps` are set.
  - Training hooks:
    - `SetCurrentSessionHook` first.
    - Barrier hooks + PS health check if async training.
    - Queue hooks, cached-variable hooks, async function hooks.
  - `model_fn` path:
    - Creates hash_table, hash_filters and `AsyncFunctionMgr`.
    - Handles EOF signal in features (keys `"2"` or `EofAwareTask.EOF_KEY`).
    - Fused layout path uses `hash_table.lookup(..., ret_lookup_callable_fn=True)` and sets `EmbeddingLayoutFactory`.
    - Non-fused path does embedding lookup + optional prefetch queue; records dequeued features into TF collections.
    - Builds `CpuFeatureFactory` or `FusedCpuFeatureFactory` depending on sync training.
    - If exporting with remote GPU, uses `RemotePredictHelper` to call a subgraph; builds prediction-only EstimatorSpec.
    - Adds restore/save/metrics hooks when not exporting.
    - For partial sync training and non-root worker, constructs custom Scaffold with local init ops.
    - Adds dequeued EOF to collection.
- `DistributedCpuTrainingConfig` extends `CpuTrainingConfig` with `model_dir`, thread counts, retry/timeout, redundant PS, uuid, and fountain config.
- PS/worker helpers:
  - `_prepare_server` initializes PS machine info via `logging_ops.machine_info`.
  - `_shutdown_ps` enqueues a shutdown token into each PS queue.
  - `_join_ps` blocks on PS queue; optionally runs a parameter sync thread using `ParameterSyncClient`.
  - `_get_blocked_addrs` extracts all cluster addresses except ignored jobs.
  - `NodeAliveCheckerError` raised when nodes unreachable.
- Worker execution:
  - `_do_worker_train`:
    - Validates task is `NativeTask`, forbids `enable_model_dump` with `with_remote_gpu`.
    - Builds `RunConfig` with `save_summary_steps/log_step_count_steps * num_workers`.
    - Optionally wraps with `EofAwareTask` if partial sync or dataservice.
    - Calls `estimator.train(..., max_steps=params.train.max_steps)`.
    - For `enable_resource_constrained_roughsort`: runs an extra training pass on candidate items.
  - `_do_worker_feature_engineering`:
    - Runs dataset iterator and `FeatureEngineeringSaveHook` in `MonitoredTrainingSession`.
  - `_run_ps_benchmark`: runs extra PS benchmark task and restores cluster.
  - `_save_debugging_info`: writes `debugging_info.pb` with cluster + feature configs.
- `distributed_train`:
  - Validates config; sets alert proto on chief.
  - Creates TF server, registers with discovery, starts PS or worker flow.
  - Worker flow handles retries, node health checks, metrics heartbeat thread, benchmark PS, and graceful shutdown (kill/finish application via yarn_runtime).
- `distributed_sync_train`:
  - MPI-based sync training; sets `use_gpu_emb_table` when GPU training enabled.
  - Adjusts session config (memory optimization, optional XLA).
  - Only rank 0 writes summaries; non-root uses NOP summary writer.
  - Adds sync hooks (`ParameterSyncHook`, `SyncTrainingInfoHook`).
- Local entry points:
  - `local_train_internal`:
    - Creates local PS servers if requested and sets `TF_CONFIG`.
    - Disables meta optimizer (ragged tensors).
    - Runs estimator.train for `steps`.
    - Optionally runs roughsort item pass.
  - `local_feature_engineering_internal`:
    - Similar to distributed FE path but for local graph; supports profiler start/stop.
  - `local_train`:
    - Builds `CpuTrainingConfig`, optionally removes model_dir, calls local_* based on `only_feature_engineering`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/cpu_training.rs` (new), `monolith-rs/crates/monolith-training/src/distributed.rs` (new), `monolith-rs/crates/monolith-training/src/local.rs` (new).
- Rust public API surface:
  - `CpuTrainingConfig`, `DistributedCpuTrainingConfig`, `CpuTraining` wrapper.
  - `distributed_train`, `distributed_sync_train`, `local_train`, `local_feature_engineering`.
  - Exporter selection and hook wiring.
- Data model mapping:
  - TF Estimator-based pipeline ↔ Rust training loop (Candle or TF runtime).
  - Hash table ops ↔ `monolith-hash-table` + distributed PS equivalents.
  - TF collections/hook APIs ↔ Rust hook/callback system.
- Feature gating:
  - TF runtime required for estimator compatibility, saved_model export, and custom ops.
  - MPI/horovod for sync training (`get_mpi_rank/size`).
  - gRPC + service discovery for distributed orchestration.
- Integration points:
  - Hash tables (`hash_table_ops`, `multi_hash_table_ops`), dataset distribution (`datasets`), exporter (`saved_model_exporters`), hooks (`session_run_hooks`, `sync_training_hooks`), metrics (`metric/cli`).

**Implementation Steps (Detailed)**
1. Define Rust config structs mirroring all `CpuTrainingConfig` and `DistributedCpuTrainingConfig` fields and defaults.
2. Implement feature config collection (`DummyFeatureFactory` analogue) or load from dump metadata.
3. Port exporter creation (standalone/distributed), including dense-only export path.
4. Implement hash table creation logic for async vs sync training and fused layout.
5. Recreate embedding prefetch queue + async postpush pipeline.
6. Implement hook system that mirrors restore/save/metrics/barrier hook ordering.
7. Port distributed orchestration (service discovery, PS lifecycle, retries, heartbeat).
8. Implement local training utilities and roughsort item conversion.
9. Provide TF runtime shims for cached-variable handling and partitioned variables if TF backend is used.

**Tests (Detailed)**
- Python tests: `cpu_training_test.py`, `cpu_training_distributed_test_binary.py` + integration harness.
- Rust tests:
  - Unit tests for config defaults and serving-config conversion.
  - Integration tests for local training and distributed orchestration once TF backend exists.
- Cross-language parity test:
  - Compare checkpoint layout, export artifacts, and hash table update semantics on a minimal model.

**Gaps / Notes**
- Uses several TF private APIs and graph collections; Rust will need a custom hook framework.
- `should_do_first_save` is forced `False`, diverging from intended partial recovery behavior.
- Relies on `TF_CONFIG` env var and estimator graph semantics; Candle backend will require a new control-plane.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/cpu_training_distributed_test_binary.py`
<a id="monolith-native-training-cpu-training-distributed-test-binary-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 226
- Purpose/role: Distributed CPU training integration test binary with host-based service discovery.
- Key symbols/classes/functions: `SyncHook`, `FeatureTask`, `HostServiceDiscovery`, `test0/1/2`, `test_run`.
- External dependencies: `absl.flags/app`, `tensorflow`, `cpu_training`, `cluster_manager`, `service_discovery`, `feature`.
- Side effects: overrides retry/backoff globals, sets `_shutdown_ps`, writes discovery files, spawns barrier sync.

**Required Behavior (Detailed)**
- Flags:
  - `test_case`, `test_dir`, `server_type` (`ps`/`worker`), `index`, `num_ps`, `num_workers`, `num_extra_ps`, `num_redundant_ps`, `uuid`, `use_native_multi_hash_table`.
- Overrides:
  - `cluster_manager._cluster_query_failure_handler = _sleep_short` (0.1s).
  - `cpu_training._EXTRA_PS_BENCHMARK_SECS = 0.5`.
- `SyncHook`:
  - Creates per-worker boolean var in local variables (chief) or global variables (workers).
  - After session creation, sets its index to True; chief waits until all workers set.
- `FeatureTask`:
  - Defines `training_hooks` param.
  - Model builds feature slot, embedding lookup, applies gradients.
  - Training hooks include `SyncHook` and any provided hooks.
- `HostServiceDiscovery`:
  - Registers by writing files `<base>/<name>/<index>` with address.
  - Query reads files into `{index: addr}` map.
- `test_run(params)`:
  - Builds `DistributedCpuTrainingConfig` using flags and a per-test model dir.
  - Sets `params.train.max_pending_seconds_for_barrier = 2`.
  - Uses `HostServiceDiscovery` and runs `cpu_training.distributed_train`.
- `test0`: normal run.
- `test1`: overrides `_shutdown_ps` to never exit.
- `test2`: adds `RaiseErrorHook` that throws `DeadlineExceededError` on first `before_run`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests`.
- Rust public API surface: distributed training integration tests + file-based discovery.
- Data model mapping: HostServiceDiscovery → file-backed discovery in Rust (not present).
- Feature gating: requires distributed training + PS/worker runner.
- Integration points: `distributed_train` analog, barrier sync.

**Implementation Steps (Detailed)**
1. Add file-backed discovery helper for integration tests.
2. Add hook to block chief until all workers register.
3. Add test cases that simulate non-shutdown and deadline errors.

**Tests (Detailed)**
- Python tests: this binary test (invoked by integration harness).
- Rust tests: integration tests if Rust distributed runner exists.
- Cross-language parity test: verify barrier synchronization semantics.

**Gaps / Notes**
- This script mutates module-level globals (`_EXTRA_PS_BENCHMARK_SECS`, `_shutdown_ps`).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/cpu_training_test.py`
<a id="monolith-native-training-cpu-training-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 597
- Purpose/role: Comprehensive CPU training tests covering feature slots, export modes, occurrence/expire configs, distributed training, debugging server, and local train.
- Key symbols/classes/functions: `FeatureTask`, `FloatFeatureTask`, `SequenceFeatureTask`, `FeatureWithSlotOccurrenceThresholdTask`, `FeatureWithExpireTimeTask`, `NonFeatureTask`, `CpuTrainTest`, `DistributedTrainTest`, `LocalTrainTest`.
- External dependencies: `tensorflow`, `cpu_training`, `entry`, `feature`, `utils`, `debugging_server`, `saved_model_exporters`, `ExportMode`, `debugging_info_pb2`, `embedding_hash_table_pb2`, `ServiceDiscovery`.
- Side effects: spawns subprocesses for distributed tests; writes checkpoints and export dirs; reads debugging info files.

**Required Behavior (Detailed)**
- Shared helpers:
  - `inc_global_step_op()` increments global step and returns grouped op.
  - `FLAGS.use_native_multi_hash_table` controls hash table implementation.
- `FeatureTask`:
  - Input: ragged feature tensor.
  - Model: create slot/slice, embedding lookup; grads applied via feature factory; predict returns sum.
  - Serving input receiver uses ragged constant + placeholder for serialized input.
- `FloatFeatureTask`:
  - Uses ragged embedding + float feature; predictions from float feature; training uses embedding grads.
- `SequenceFeatureTask`:
  - Uses combiner `FeatureColumnV1.first_n(2)`; predictions from sequence feature sum.
- `FeatureWithSlotOccurrenceThresholdTask`:
  - Creates slot with `slot_id=2021`, `occurrence_threshold=3`; asserts training captures threshold map.
- `FeatureWithExpireTimeTask`:
  - Two slots with `expire_time=0` and `1`; uses `ZerosInitializer`.
  - After training, checks `_slot_to_expire_time` map and prediction values.
- `NonFeatureTask`:
  - Input dataset yields scalar; train op uses input with global step increment.
- `CpuTrainTest`:
  - `test_cpu_training_feature` basic training.
  - `test_with_misc_features`: `feature_eviction_on_save=True`.
  - `test_with_export_when_saving`: `serving.export_when_saving=True`.
  - `test_dense_only_export`: export mode `DISTRIBUTED` + `dense_only_save_checkpoints_steps=10`.
  - `test_with_prefetch_postpush`: enables variable prefetch/postpush and embedding postpush.
  - `test_cpu_training_float_feature` and `test_cpu_training_sequence_feature` run those tasks.
  - `test_cpu_training_with_slot_occurrence_threshold` checks internal threshold map.
  - `test_cpu_training_with_expire_time` checks expire time map and prediction values.
  - `test_cpu_training_non_feature` runs non-feature task.
  - `test_gpu_export`: exports saved model with remote GPU.
- `DistributedTrainTest`:
  - Spawns `cpu_training_distributed_test_binary` processes for PS/worker.
  - Tests: basic, extra_ps, redundant_ps, debugging server (case=1), temporary error (case=2).
  - `test1_with_debugging_server` waits for checkpoints then reads debugging info proto; checks variable/feature fetch via debugging server.
- `LocalTrainTest`:
  - `local_train` with and without PS.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests`.
- Rust public API surface: training loop, export pipeline, debug info tooling.
- Data model mapping: TF Estimator/FeatureFactory → Rust training/feature abstractions.
- Feature gating: depends on TF runtime and distributed runner.
- Integration points: `TrainingConfig`, export utilities, discovery and debugging.

**Implementation Steps (Detailed)**
1. Build Rust integration tests that cover: basic training, feature thresholds, expire time, and export flow.
2. Add distributed runner test harness or mark as Python-only.
3. Provide debug info export and retrieval parity tests.

**Tests (Detailed)**
- Python tests: this file plus `cpu_training_distributed_test_binary`.
- Rust tests: integration tests for training/export/debug info.
- Cross-language parity test: compare embedding predictions and debug info outputs.

**Gaps / Notes**
- Uses private internals (`training._slot_to_occurrence_threshold`, `_slot_to_expire_time`).
- Debugging server depends on proto + embedding hash table dumps not present in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/__init__.py`
<a id="monolith-native-training-data-init-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 20
- Purpose/role: Package exports for dataset builders and feature utilities.
- Key symbols/classes/functions: re-exports `PBDataset`, `InstanceReweightDataset`, `NegativeGenDataset`, `PbType`, `parse_examples`, `parse_instances`, `parse_example_batch`, `filter_by_*`, `feature_combine`, `negative_sample`, `switch_slot`, `special_strategy`.
- External dependencies: internal modules `datasets`, `parsers`, `feature_utils`.
- Side effects: imports modules at package import time.

**Required Behavior (Detailed)**
- Importing package exposes symbols listed above at `monolith.native_training.data.*`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/src`.
- Rust public API surface: module re-exports for dataset and feature utils.
- Data model mapping: match Python export surface.
- Feature gating: none.
- Integration points: data pipeline and training input.

**Implementation Steps (Detailed)**
1. Re-export dataset and parser APIs in Rust modules.

**Tests (Detailed)**
- Python tests: none specific.
- Rust tests: module visibility tests.
- Cross-language parity test: not applicable.

**Gaps / Notes**
- None.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/data_ops_test.py`
<a id="monolith-native-training-data-data-ops-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 502
- Purpose/role: End-to-end tests for PB datasets, parsing, filtering, compression, and dataset variants.
- Key symbols/classes/functions: `DataOpsTest`, `CahceOneDatasetTest`, `DecompressTest`, helper parsers.
- External dependencies: `tensorflow`, `PBDataset`, `parse_examples/instances/example_batch`, `feature_utils` filters, `ExampleBatch`, `gen_random_data_file`, `session_hooks`.
- Side effects: creates temp files under `tmp_data`; invokes external `zstd` binary; writes compressed files.

**Required Behavior (Detailed)**
- `DataOpsTest.setUpClass`:
  - Generates random instance data files (3 parts) using `gen_random_data_file`.
- `pb_dataset_target(input_pb_type, output_pb_type, filter_fn=None)`:
  - Builds `PBDataset` for Instance/Example/ExampleBatch with appropriate flags.
  - For ExampleBatch, applies `instance_reweight`.
  - Applies optional `filter_fn`.
  - Batches and maps to parser (`parse_inst_exam`/`parse_eb`).
  - Iterates through dataset and counts elements (logs count).
- Tests validate permutations:
  - Instance→Instance/Example, Example→Example/Instance, ExampleBatch→Example/Instance.
  - PLAINTEXT output for Instance/Example.
  - `filter_by_fids`, `filter_by_value` (ge/in/eq/between/str/any/all/diff), `special_strategy`.
  - `parse_example_batch` for scalar and batch inputs.
  - `PBDataset` resolves to `FilePBDataset` or `KafkaDataset` based on flags/args.
  - `testCreateInstanceDatasetHdfs` reads generated files via `PBDataset`.
  - `PBDataset.gen_patterns` returns correct date range size.
- `CahceOneDatasetTest`:
  - `CacheOneDataset` wraps dataset; second element flagged `True`.
- `DecompressTest`:
  - Creates copies of examplebatch data and tests `CompressType.ZSTD/ZLIB/GZIP`.
  - Uses `parse_example_batch` to validate decompression.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests`.
- Rust public API surface: dataset parsing, compression, filter utils.
- Data model mapping: TF datasets → Rust data pipeline equivalents.
- Feature gating: compression codecs and Kafka support.
- Integration points: `monolith-data` crate and training input.

**Implementation Steps (Detailed)**
1. Add Rust tests for parsing Instance/Example/ExampleBatch formats.
2. Add filter/feature utility tests mirroring Python cases.
3. Add compression decode tests (zstd/zlib/gzip).

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: dataset parsing + compression + filter tests.
- Cross-language parity test: compare parsed feature dict shapes and counts.

**Gaps / Notes**
- Requires external `zstd` binary for decompression test.
- Relies on hard-coded proto files under `monolith/native_training/data/training_instance`.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/data_service_parquet_test.py`
<a id="monolith-native-training-data-data-service-parquet-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 145
- Purpose/role: Integration test for TF data service reading Parquet via `PBDataset`.
- Key symbols/classes/functions: `DataServiceTest2.testDataServiceWithParquetDataset`.
- External dependencies: `tensorflow.data.experimental.service`, `PBDataset`, `PbType`, `ExampleBatch`, `json`.
- Side effects: starts dispatcher/workers on port 7080, reads local files.

**Required Behavior (Detailed)**
- `setUpClass`/`tearDownClass`: create and destroy dispatcher + workers.
- `testDataServiceWithParquetDataset`:
  - Reads `META_JSON_PATH` and `PARQUET_DIR` env vars (defaults under `$HOME/temp`).
  - Loads meta JSON, builds column names/types mapping.
  - Creates `PBDataset` with `use_data_service=True`, `use_parquet=True`, `output_pb_type=PLAINTEXT`.
  - Registers dataset and reads from two consumers (distributed_epoch).
  - Parses `ExampleBatch` from bytes, accumulates `batch_size`.
  - Prints row count (assertion against parquet row count is commented out).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests`.
- Rust public API surface: data service + parquet ingestion tests (if supported).
- Data model mapping: `PBDataset` + Parquet pipeline.
- Feature gating: Parquet and data service support.
- Integration points: dataset registration/consumers.

**Implementation Steps (Detailed)**
1. Add Rust integration test only if parquet + data service are implemented.
2. Otherwise, document as Python-only system test.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: optional integration tests.
- Cross-language parity test: compare total row counts.

**Gaps / Notes**
- Test depends on local parquet files and meta JSON; not hermetic.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/data_service_test.py`
<a id="monolith-native-training-data-data-service-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 98
- Purpose/role: Tests data service split provider with `DynamicMatchingFilesDataset`.
- Key symbols/classes/functions: `DataServiceTest.testSplitProvider`.
- External dependencies: `tensorflow.data.experimental.service`, `DynamicMatchingFilesDataset`.
- Side effects: starts dispatcher/workers on port 7080.

**Required Behavior (Detailed)**
- Registers `DynamicMatchingFilesDataset` and consumes it via two distributed_epoch consumers.
- Alternates pulling items until both consumers are exhausted.
- Asserts total count equals 19.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests`.
- Rust public API surface: data service dataset tests.
- Data model mapping: dynamic file matching dataset.
- Feature gating: TF data service equivalent required.
- Integration points: dataset registry + consumer.

**Implementation Steps (Detailed)**
1. Only port if Rust data service is implemented.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: TODO (manual)
- Cross-language parity test: TODO (manual)

**Gaps / Notes**
- TODO (manual)

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/datasets.py`
<a id="monolith-native-training-data-datasets-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 1642
- Purpose/role: Dataset factory and dataset classes for PB files, Kafka streams, Parquet, TFRecord, transforms, and data service.
- Key symbols/classes/functions: `PBDataset`, `FilePBDataset`, `DistributedFilePBDataset`, `KafkaDataset`, `ParquetDataset`, `InstanceReweightDataset`, `NegativeGenDataset`, `SplitFlowDataset`, `MergeFlowDataset`, `TransformDataset`, `DatasetMetaclass`, `distribute`, `merged_window`.
- External dependencies: TensorFlow internals, custom ops `gen_monolith_ops`, Kafka consumer, data service APIs, feature utils.
- Side effects: defines many flags; registers graph collections; disables iterator save/restore; spawns Kafka polling thread.

**Required Behavior (Detailed)**
- Flags defined: `data_service_dispatcher`, `dataset_use_dataservice`, `dataset_input_patterns`, `dataset_input_use_snappy`, `dataset_input_compression_type`, `dataset_input_use_parquet`, `dataset_input_use_tfrecord`, `dataset_worker_idx`, `dataset_num_workers`, `kafka_other_metadata`.
- `DatasetMetaclass.__call__`:
  - Normalizes shorthand kwargs (`topics_or_files`, `buffer_size_or_group_id`, `input_pb_type_or_servers`).
  - Expands `dataset_input_patterns` into file patterns (DATE/INT range syntax); overwrites `patterns`; removes `file_name`.
  - Forces parquet/tfrecord flags from global FLAGS; prevents both true.
  - If Kafka args present, returns `KafkaDataset`.
  - Otherwise returns `DistributedFilePBDataset`, `ParquetDataset`, `TFRecordDatasetWrapper`, or `FilePBDataset` based on args.
- `PBDataset`: empty init (factory via metaclass).
- `PBDataset.gen_patterns(...)`:
  - Expands date/hour ranges into path patterns.
- `DynamicMatchingFilesDataset`: uses custom op `dynamic_matching_files_dataset`.
- `ParquetDataset`: validates columns/types, sets `OUTPUT_PB_TYPE_GRAPH_KEY`, uses custom parquet op.
- `FilePBDataset`:
  - Determines input/output pb types, uses FeatureList to prune if possible.
  - Configures flags: has_sort_id, kafka_dump, lagrangex_header, compression, snappy.
  - Adds output pb type to collection; calls `pb_dataset` op.
- `DistributedFilePBDataset`:
  - Creates file list, optional dynamic sharding, data service or matching_files.
  - Supports parquet/tfrecord mapping; handles sharding by worker or explicit shard_num.
- `InstanceReweightDataset`: wraps custom op `instance_reweight_dataset` based on action priorities.
- `NegativeGenDataset`: wraps custom op `instance_negative_gen_dataset`, creates item pool.
- `SplitFlowDataset` / `MergeFlowDataset`: custom ops for flow splitting/merging.
- `KafkaDataset`:
  - Initializes Kafka resource via custom ops; uses `kafka_read_next_v2`, unbatch.
  - Sets output pb type collection and flags (sort_id, dump, lagrangex_header).
- `PyKafkaDataset`:
  - Python KafkaConsumer with background polling thread; converts strings to variant via `string_to_variant`.
- `register_dataset` / `from_dataset_id`:
  - Data service register/uncompress; external_state_policy preserved.
- `merged_window`: window and reshape dataset elements.
- `distribute`: data service integration with sync training / ps-worker queue; handles Horovod/BytePS.
- `TransformDataset`: wraps `transform_dataset` op with serialized Transform config.
- Monkey patches: `Dataset.instance_reweight`, `.negative_gen`, `.split_flow`, `.merge_flow`, `.distribute`, `.merged_window`, `.transform`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/src`.
- Rust public API surface: dataset factory + dataset operators.
- Data model mapping: TF Dataset/Variant → Rust streaming dataset abstractions.
- Feature gating: Kafka, Parquet, TFRecord, DataService, custom ops.
- Integration points: data ingestion and training input pipeline.

**Implementation Steps (Detailed)**
1. Decide which dataset sources are in-scope for Rust (file/kafka/parquet).
2. Implement dataset factory/dispatch logic and pattern expansion.
3. Provide custom ops or pure-Rust replacements for instance reweight, negative gen, transform.
4. Add data service integration or document unsupported.

**Tests (Detailed)**
- Python tests: `data_ops_test.py`, `data_service_test.py`, `negative_gen_test.py`, etc.
- Rust tests: dataset factory, pattern expansion, ops parity tests.
- Cross-language parity test: compare dataset counts and parsed outputs for fixtures.

**Gaps / Notes**
- Heavy reliance on custom TF ops; Rust needs replacements or TF runtime backend.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/eager_mode_test.py`
<a id="monolith-native-training-data-eager-mode-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 186
- Purpose/role: Eager-mode dataset parsing tests for Instance/Example/ExampleBatch.
- Key symbols/classes/functions: `DataOpsTest.target`, test methods.
- External dependencies: `tensorflow`, `PBDataset`, `parse_instances/parse_examples/parse_example_batch`, `switch_slot`, `feature_combine`.
- Side effects: reads training_instance fixtures.

**Required Behavior (Detailed)**
- `target(input_pb_type, output_pb_type)`:
  - Builds `PBDataset` for Instance/Example/ExampleBatch and applies instance reweight for ExampleBatch.
  - Parses via `parse_inst_exam` or `parse_eb` depending on pb types.
  - Batches then iterates `dataset.take(5)` and asserts feature dict length ∈ {26,27}.
- `testExampleBatch2Instance`, `testExample2Instance`, `testInstance2Instance` call `target`.
- `testExampleBatch`:
  - Parses ExampleBatch to ExampleBatch via `parse_example_batch`, asserts len ∈ {26,27}.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests`.
- Rust public API surface: dataset parsing tests (eager mode equivalent).
- Data model mapping: dataset pipelines in Rust.
- Feature gating: none.
- Integration points: `monolith-data` parsing pipeline.

**Implementation Steps (Detailed)**
1. Add parsing tests for all pb types with fixed fixtures.
2. Ensure eager-mode behavior maps to Rust pipeline semantics.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: TODO (manual)
- Cross-language parity test: TODO (manual)

**Gaps / Notes**
- TODO (manual)

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/extract_fid_test.py`
<a id="monolith-native-training-data-extract-fid-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 30
- Purpose/role: Tests custom op `extract_fid`.
- Key symbols/classes/functions: `ExtraFidTest.test_parse_search`.
- External dependencies: `tensorflow`, `gen_monolith_ops.extract_fid`.
- Side effects: none.

**Required Behavior (Detailed)**
- `extract_fid(185, 4)` must return `1153447759131936`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests` (or runtime ops crate).
- Rust public API surface: `extract_fid` equivalent.
- Data model mapping: custom op to Rust function.
- Feature gating: requires runtime ops.
- Integration points: feature parsing pipeline.

**Implementation Steps (Detailed)**
1. Implement `extract_fid` in Rust runtime ops.
2. Add a unit test for the exact constant output.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: add exact-match test.
- Cross-language parity test: compare output for fixed inputs.

**Gaps / Notes**
- Relies on custom TF op; no Rust implementation yet.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/feature_list.py`
<a id="monolith-native-training-data-feature-list-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 409
- Purpose/role: Parses feature_list configuration and provides feature lookup, slot mapping, and feature filtering utilities.
- Key symbols/classes/functions: `Feed`, `Cache`, `Feature`, `FeatureList`, `get_feature_name_and_slot`, `add_feature`, `add_feature_by_fids`, `get_valid_features`, `is_example_batch`.
- External dependencies: `absl.flags`, `tensorflow`, `numpy`, `inspect`, `dataclasses`, `monolith.native_training.data.utils`.
- Side effects: populates global `_cache` and `_VALID_FNAMES`; adds to TF collections via `add_to_collections`.

**Required Behavior (Detailed)**
- `new_instance(cls, args)`:
  - Inspects `__init__` signature and passes only matching args.
- `Feed` dataclass:
  - `shared` parsed from truthy strings; `feature_id` cast to int.
  - `name` property returns `feed_name`.
- `Cache` dataclass:
  - `capacity`, `timeout` cast to int if string.
  - `name` property uses `cache_name` > `cache_key_class` > `cache_column` else raises.
- `Feature` dataclass:
  - Parses comma-separated string fields into lists.
  - Casts slot/shared/need_raw/feature_id/version to proper types.
  - `__str__` renders key=value terms based on type hints; bools only if True.
  - `name` strips `fc_`/`f_` prefixes and lowercases terms.
  - `depend_strip_prefix` strips prefixes for dependencies.
- `FeatureList`:
  - Stores column names, feeds, caches, features; builds slot→feature mapping.
  - Adds itself to TF collections `feature_list`.
  - `__getitem__` resolves by slot id or feature name variants (`f_`, `fc_`, dashed names).
  - `get_with_slot(slot)` returns list or empty.
  - `__contains__` supports names and slots.
  - `parse(fname=None, use_old_name=True)`:
    - Reads config file; caches results in `_cache`.
    - Parses `column_name:`, `cache_column:` and `feed/cache/feature` lines into dataclasses.
    - `use_old_name` chooses between raw feature_name or normalized name as key.
- `get_feature_name_and_slot(item)`:
  - Handles int, str, or FeatureColumn-like objects.
  - Uses `FeatureList.parse()` with fallback to slot name utility.
- `is_example_batch()`:
  - Checks `FLAGS.data_type` for example_batch.
- `add_feature` / `add_feature_by_fids`:
  - Maintains `_VALID_FNAMES`; for example_batch fids, resolves feature list by slot and version.
  - Raises if fid cannot be mapped.
- `get_valid_features()` returns list of collected features.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/src/feature_list.rs` (if implemented).
- Rust public API surface: feature list parser + lookup utilities.
- Data model mapping: config parsing + slot/feature mapping.
- Feature gating: none.
- Integration points: data parsing and column pruning.

**Implementation Steps (Detailed)**
1. Implement feature_list.conf parser with the same key parsing rules.
2. Add slot/name resolution helpers.
3. Implement example_batch feature filtering via fid decoding.

**Tests (Detailed)**
- Python tests: none (feature_list_test.py is empty).
- Rust tests: add unit tests for parsing, slot lookup, fid mapping.
- Cross-language parity test: compare parsed feature list for a fixed config file.

**Gaps / Notes**
- Uses global caches and TF collections; Rust should provide equivalent global registry if required.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/feature_list_test.py`
<a id="monolith-native-training-data-feature-list-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 0
- Purpose/role: Empty file (no tests).
- Key symbols/classes/functions: none.
- External dependencies: none.
- Side effects: none.

**Required Behavior (Detailed)**
- None.

**Rust Mapping (Detailed)**
- Target crate/module: none.
- Rust public API surface: none.
- Data model mapping: none.
- Feature gating: none.
- Integration points: none.

**Implementation Steps (Detailed)**
1. No Rust tests required unless feature list gains tests.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: TODO (manual)
- Cross-language parity test: TODO (manual)

**Gaps / Notes**
- TODO (manual)

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/feature_utils.py`
<a id="monolith-native-training-data-feature-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 1070
- Purpose/role: Feature/label filtering, transformation, and utility ops over variant tensors; mostly thin wrappers around custom `gen_monolith_ops` kernels with strict input validation and feature-registration side effects.
- Key symbols/classes/functions: `filter_by_fids`, `filter_by_feature_value`, `filter_by_value`, `add_action`, `add_label`, `scatter_label`, `filter_by_label`, `special_strategy`, `negative_sample`, `feature_combine`, `switch_slot`, `switch_slot_batch`, `label_upper_bound`, `label_normalization`, `use_field_as_label`, `create_item_pool`, `item_pool_random_fill`, `item_pool_check`, `save_item_pool`, `restore_item_pool`, `fill_multi_rank_output`, `use_f100_multi_head`, `map_id`, `multi_label_gen`, `string_to_variant`, `string_to_variant_with_transform`, `variant_to_zeros`, `kafka_resource_init`, `kafka_read_next`, `kafka_read_next_v2`, `has_variant`, `gen_fid_mask`, `tf_example_to_example`.
- External dependencies: TensorFlow, numpy, `idl.matrix.proto.line_id_pb2.LineId`, `data_op_config_pb2.LabelConf`/`TFRecordFeatureDescription`, `gen_monolith_ops` custom kernels.
- Side effects: calls `add_feature`/`add_feature_by_fids` to ensure downstream parsing includes required fields; validates/loads operand files via `tf.io.gfile.exists`; asserts on invalid inputs; builds TF ops that mutate or filter variant tensors.

**Required Behavior (Detailed)**
- `filter_by_fids(variant, filter_fids, has_fids, select_fids, has_actions, req_time_min, select_slots, variant_type)`:
  - Coerces `filter_fids`/`has_fids`/`select_fids` to `np.uint64` then `int64` list; defaults to empty lists.
  - `select_slots` defaults to empty; asserts all slots > 0.
  - If `variant_type != 'instance'`, calls `add_feature_by_fids` for all fid lists.
  - Calls `ragged_data_ops.set_filter(...)` with `has_actions or []`, `req_time_min`, `select_slots`, `variant_type` and returns variant tensor.
- `filter_by_feature_value(variant, field_name, op, operand, field_type, keep_empty, operand_filepath)`:
  - `op` must be in `{gt,ge,eq,lt,le,neq,between,in,not-in,all,any,diff,startswith,endswith}`.
  - Exactly one of `operand` or `operand_filepath` is provided; if filepath set, it must exist and `op` must be in `{in, not-in}`.
  - `field_type` must be in `{int64,float,double,bytes}`; builds `int_operand`, `float_operand`, `string_operand` based on type/op:
    - `all/any/diff` only for `int64` (operand int or list of int).
    - `between` uses a list of numbers (float/double) or ints for int64.
    - `bytes` accepts str or list of str; otherwise raises `RuntimeError("params error!")`.
  - Calls `ragged_data_ops.feature_value_filter(...)` with operands, file path, `keep_empty`, returns variant.
- `filter_by_value(variant, field_name, op, operand, variant_type, keep_empty, operand_filepath)`:
  - For `variant_type != 'instance'`, calls `add_feature('__LINE_ID__')`.
  - `field_name` must exist in `LineId` descriptor; uses proto field `cpp_type`/`has_options` to determine parsing rules.
  - Same operand vs operand_filepath exclusivity; operand file must exist; only `in/not-in` supported with filepath.
  - For repeated fields (`field.has_options`), only `all/any/diff` allowed and only integer types.
  - For `string` fields: operand must be str or list of str, else `RuntimeError("params error!")`.
  - Calls `ragged_data_ops.value_filter(...)` with `variant_type` and returns variant.
- `add_action(variant, field_name, op, operand, action, variant_type)`:
  - For `variant_type != 'instance'`, calls `add_feature('__LINE_ID__')`.
  - `op` in `{gt,ge,eq,lt,le,neq,between,in}`; field must exist in `LineId`.
  - Builds typed operands (float/int/string) based on field cpp_type; for `in/between` on integer types, operand is list of int.
  - Calls `ragged_data_ops.add_action(..., actions=[action], variant_type)`.
- `add_label(variant, config, negative_value, new_sample_rate, variant_type)`:
  - For `variant_type != 'instance'`, calls `add_feature('__LINE_ID__')`.
  - `config` is required; `new_sample_rate` must be in `(0, 1.0]`.
  - Parses `config` with `;` task separator; each task `pos_actions:neg_actions:sample_rate` (empty lists allowed). Skips empty trailing parts.
  - Builds `LabelConf` proto and calls `ragged_data_ops.add_label(..., negative_value, sample_rate=new_sample_rate)`.
- `scatter_label(variant, config, variant_type)`:
  - For `variant_type != 'instance'`, calls `add_feature('__LABEL__')` and `add_feature('__LINE_ID__')`.
  - `config` required; passes through to `ragged_data_ops.scatter_label`.
- `filter_by_label(variant, label_threshold, filter_equal, variant_type)`:
  - For `variant_type != 'instance'`, calls `add_feature('__LABEL__')`.
  - `label_threshold` must be non-empty list.
  - Calls `ragged_data_ops.filter_by_label(..., filter_equal, variant_type)` and returns boolean tensor.
- `special_strategy(variant, strategy_list, strategy_conf, variant_type, keep_empty_strategy)`:
  - For `variant_type != 'instance'`, calls `add_feature('__LABEL__')` and `add_feature('__LINE_ID__')`.
  - `strategy_conf` is optional; parses comma-separated `strategy:sample_rate` or `strategy:sample_rate:label` entries.
  - Ensures lengths consistent and each `sample_rate` in `[0,1]`.
  - Calls `ragged_data_ops.special_strategy(..., keep_empty_strategy, variant_type)`.
- `negative_sample(variant, drop_rate, label_index, threshold, variant_type, action_priority, per_action_drop_rate)`:
  - For `variant_type != 'instance'`, calls `add_feature('__LABEL__')`.
  - `action_priority` and `per_action_drop_rate` are optional strings; if both set, parse lists of actions and per-action drop rates.
  - Calls `ragged_data_ops.negative_sample(..., priorities, actions, per_action_drop_rate)`.
- `feature_combine(src1, src2, slot)`:
  - Requires `tf.RaggedTensor` inputs; calls `ragged_data_ops.feature_combine(..., fid_version=2)`.
  - If `splits[0]` is `float32`, uses `from_row_splits(values, splits[1])`; else `from_nested_row_splits`.
- `switch_slot(ragged, slot)`:
  - Requires `tf.RaggedTensor`; calls `ragged_data_ops.switch_slot(..., fid_version=2)`.
  - If `splits[0]` is `float32`, returns new ragged from row_splits; else returns `ragged.with_flat_values(values)`.
- `switch_slot_batch(variant, features, variant_type, suffix)`:
  - `features` maps feature name → `(inplace, new_slot)`; `variant_type` must be `example` or `example_batch`.
  - Builds `features`, `inplaces`, `slots` arrays; calls `ragged_data_ops.switch_slot_batch(..., suffix)`.
- `label_upper_bound(variant, label_upper_bounds, variant_type)`:
  - `label_upper_bounds` non-empty; calls `ragged_data_ops.label_upper_bound`.
- `label_normalization(variant, norm_methods, norm_values, variant_type)`:
  - `norm_methods` length must equal `norm_values`; calls `ragged_data_ops.label_normalization`.
- `use_field_as_label(variant, field_name, overwrite_invalid_value, label_threshold, variant_type)`:
  - Calls `ragged_data_ops.use_field_as_label` to overwrite labels from LineId field with optional clamping.
- Item pool ops:
  - `create_item_pool(start_num, max_item_num_per_channel, container, shared_name)` asserts `start_num >= 0`, `max_item_num_per_channel > 0`, calls `ItemPoolCreate`.
  - `item_pool_random_fill`, `item_pool_check(model_path, global_step, nshards, buffer_size)`, `save_item_pool`, `restore_item_pool` delegate to custom ops.
- `fill_multi_rank_output(variant, enable_draw_as_rank, enable_chnid_as_rank, enable_lineid_rank_as_rank, rank_num, variant_type)`:
  - For `variant_type != 'instance'`, calls `add_feature('__LINE_ID__')`.
  - Calls `ragged_data_ops.fill_multi_rank_output`.
- `use_f100_multi_head(variant, variant_type)`:
  - Pass-through to `ragged_data_ops.use_f100_multi_head`.
- `map_id(tensor, map_dict, default)`:
  - `map_dict` non-empty; passes `from_value`, `to_value`, `default` to `ragged_data_ops.MapId`.
- `multi_label_gen(variant, head_to_index, head_field, pos_actions, neg_actions, use_origin_label, pos_label, neg_label, action_priority, task_num, variant_type)`:
  - Builds `head_to_index` string (`head:idx`), computes `task_num` if unset; asserts `max_idx < task_num`.
  - If `use_origin_label`, `pos_actions` and `neg_actions` must be empty; otherwise `pos_actions` non-empty.
  - `head_field` must exist in `LineId` descriptor and be int or string.
  - Calls `ragged_data_ops.multi_label_gen(..., action_priority, pos/neg actions, labels, variant_type)`.
- `string_to_variant(...)`:
  - `variant_type` must be `instance|example|examplebatch|example_batch`; converts string tensor into variant using header flags and optional `chnids/datasources`.
- `string_to_variant_with_transform(...)`:
  - Similar to `string_to_variant` but accepts `input_type` and `output_type` for on-the-fly transforms.
- `variant_to_zeros(tensor)`:
  - Calls `ragged_data_ops.variant_to_zeros` to produce zeroed variant tensor.
- Kafka ops:
  - `kafka_resource_init(topics, metadata, input_pb_type, output_pb_type, has_sort_id, lagrangex_header, kafka_dump_prefix, kafka_dump, container, shared_name)` calls `KafkaGroupReadableInit`.
  - `kafka_read_next`/`kafka_read_next_v2` call `KafkaGroupReadableNext`/`NextV2` with poll/stream timeouts.
- `has_variant(input, variant_type)`:
  - Calls `ragged_data_ops.HasVariant`.
- `gen_fid_mask(ragged, fid)`:
  - Casts `fid` to `np.uint64` → `int64`; calls `monolith_gen_fid_mask` with row_splits and flat_values.
- `tf_example_to_example(serialized, sparse_features, dense_features, label, instance_weight)`:
  - Defaults: empty sparse/dense/label/instance_weight if None.
  - Validates no overlaps between sparse/dense/label/instance_weight; slot ids unique; each slot id in `[1, 32768)`.
  - Builds `TFRecordFeatureDescription` proto and calls `MonolithTFExampleToExample` op.
- Error semantics:
  - Many checks are `assert` (raising `AssertionError`), some raise `RuntimeError("params error!")` for invalid bytes operands.
  - Operand file must exist and be used only with `in/not-in` ops; callers rely on these preconditions.
- I/O formats:
  - Variant tensor format is custom monolith variant; string inputs for `string_to_variant*` are framed by headers (sort header or lagrangex header) and length-prefixed protos.
  - Operand file for `operand_filepath` is expected to contain serialized `example_pb2.FilterValues` (see tests).
  - `tf_example_to_example` expects TF Example serialized bytes and emits Monolith Example variant.
- Threading/concurrency:
  - No explicit threading here; concurrency behavior is inside custom ops (e.g., Kafka resources).
- Determinism/perf:
  - Performance relies on custom ops; callers expect these to be safe in `tf.data` pipelines (including parallel map/filter). Determinism depends on op implementations; keep semantics stable.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/src` for dataset/feature ops; optionally `monolith-rs/crates/monolith-tf` for TF-runtime-backed kernels.
- Rust public API surface: `feature_utils` module exposing the same function set (or a `FeatureOps` trait with backend-specific implementations).
- Data model mapping: TF Variant / RaggedTensor → Rust `Variant`/`Ragged` equivalents (likely in `monolith-data` or `monolith-tensor`).
- Feature gating: Kafka ops, TFExample conversion, item-pool ops, and label/negative sampling depend on custom kernels; gate behind a TF backend or feature flags.
- Integration points: parsing (`parsers.py`), datasets (`datasets.py`), hooks (`item_pool_hook.py`), and training pipelines/tests.

**Implementation Steps (Detailed)**
1. Enumerate all custom ops used here and decide per-op strategy: native Rust implementation vs TF runtime binding.
2. Port validation logic exactly (asserts, `RuntimeError("params error!")`, slot range checks).
3. Provide Rust equivalents for `LineId` field metadata (from `monolith-proto`) and reuse it for `filter_by_value`/`add_action`/`multi_label_gen`.
4. Implement operand file reading for `in/not-in` filters using the same `FilterValues` proto.
5. Implement Ragged feature transforms (`feature_combine`, `switch_slot`, `switch_slot_batch`) with fid-v2 rules.
6. Add feature registry side effects equivalent to `add_feature`/`add_feature_by_fids` so parsing includes required fields.
7. Implement item pool ops or wrap TF kernels; include save/restore/check.
8. Implement label ops (`add_label`, `scatter_label`, `filter_by_label`, `label_upper_bound`, `label_normalization`, `use_field_as_label`, `multi_label_gen`) with identical label-invalid sentinel values.
9. Implement `string_to_variant` framing rules (headers, length prefix, flags, chnids/datasources) and `tf_example_to_example` conversion.
10. Add Kafka resource wrappers with poll/stream timeouts and variant conversion.
11. Add Rust tests mirroring Python expectations (see `feature_utils_test.py`) and cross-language fixtures.

**Tests (Detailed)**
- Python tests: `monolith/native_training/data/feature_utils_test.py`, `data_ops_test.py`, `eager_mode_test.py` (feature_combine/switch_slot usage).
- Rust tests: `monolith-rs/crates/monolith-data/tests/feature_utils_*` for filtering, labels, switching slots, map_id, fid mask, string_to_variant, TFExample conversion.
- Cross-language parity test: run Python test fixtures and compare Rust outputs on identical serialized inputs (including FilterValues operand files).

**Gaps / Notes**
- Heavy reliance on `gen_monolith_ops`; Rust must either re-implement kernels or use TF runtime backend (optional per earlier requirement).
- `filter_by_value` and `filter_by_feature_value` behavior is used in parallel dataset filters; caching/parallel safety must match TF op semantics.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/feature_utils_test.py`
<a id="monolith-native-training-data-feature-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 1414
- Purpose/role: End-to-end tests for feature_utils ops over `PBDataset` pipelines, including label ops, filters, slot switching, fid masks, string-to-variant framing, and negative sampling.
- Key symbols/classes/functions: `DataOpsTest`, `pb_dataset_target`, helper generators (`generate_instance`, `write_instance_into_file`), tests for `add_action`, `add_label`, `scatter_label`, `filter_by_*`, `switch_slot_batch`, `map_id`, `multi_label_gen`, `string_to_variant`, `negative_sample`.
- External dependencies: TensorFlow, `PBDataset`, `parsers` (`parse_instances`, `parse_examples`), `example_pb2.FilterValues`, `proto_parser_pb2.Instance`, temporary file IO.
- Side effects: creates/deletes temp files, writes serialized Instance files, writes FilterValues proto files, logs sample counts.

**Required Behavior (Detailed)**
- `pb_dataset_target(...)` helper:
  - Chooses input file based on `input_pb_type` (instance/example/examplebatch fixtures under `monolith/native_training/data/training_instance`).
  - Builds `PBDataset` with header flags; optional `add_action_fn`, optional filter.
  - For ExampleBatch: applies `instance_reweight` with fixed `action_priority` and `reweight` config, `variant_type` based on output.
  - Batches, parses via `parse_instance_or_example` or `parse_example_batch`, and returns a list of `return_result_key` slices.
- Action/format conversions:
  - `test_input_instance_output_instance`: actions are `[[1,0], ...]` for two batches.
  - `test_input_instance_output_instance_add_action`: adding action 2 yields `[[1,2], ...]`.
  - `test_input_instance_output_example`: output actions `[[1,0,0], ...]`.
  - `test_input_instance_output_example_add_action`: `req_time between [1622667900,1622667911]` adds action 2 in some rows.
  - `test_input_example_output_instance` and `*_add_action` mirror the above for Example input.
  - `test_input_example_output_example` and `*_add_action` expect action arrays with 3 columns.
  - `test_input_example_batch_output_instance` and `*_add_action` expect action arrays starting with `2` and optionally `3`.
  - `test_input_example_batch_output_example` and `*_add_action` expect action arrays with 3 columns.
- `test_input_instance_output_instance_add_label`:
  - Builds a temp Instance file with deterministic action patterns.
  - Applies `add_label` with config `1,2:3:1.0;4::0.5` and then `filter_by_label`.
  - Expects total valid instances in range `[340, 360]` for `mock_batch_num=100`.
- `test_input_instance_output_instance_label_upper_bound`:
  - `label_upper_bounds=[0.5,0.5]` clamps labels to `[[0,0.5], ...]`.
- `test_input_instance_output_instance_label_normalization`:
  - `norm_methods=['scale','repow']`, `norm_values=[0.5,3]` results in labels `[[0,8], ...]`.
- `test_input_examplebatch_output_instance_use_field_as_label`:
  - Uses `sample_rate` field as label; with `overwrite_invalid_value` and `label_threshold` combinations expects:
    - threshold 0 → labels `[[1,1], ...]`.
    - threshold 1.1 with prior `label_upper_bound` → labels `[[1,1], ...]`.
    - threshold 0.9 with prior `label_upper_bound` → labels `[[0,0.5], ...]`.
- `test_input_instance_output_instance_filter_by_label_equals`:
  - With `filter_equal=False`, expects 100 batches and labels `[[0,1], ...]`.
  - With `filter_equal=True`, expects 49 batches and labels `[[0,2], ...]`.
- `test_input_instance_output_instance_scatter_label`:
  - `scatter_label_config = '100:3,200:1,300:4'` and `filter_by_label` yields 2 valid instances.
  - Labels contain invalid sentinel `-3.4028235e+38` with the selected index set to original label value.
- `test_filter_by_bytes_value`:
  - Filters `req_id` using `endswith` with `filter_by_value` (LineId) and `filter_by_feature_value` (feature list).
  - Expects 4 outputs `[[b'abckjhfjh'], [b'kjhfjh'], ...]`.
  - Parallel filter path (`dataset.map(...).filter(...)`) must preserve correctness and use cached feature index.
- `test_filter_by_float_value`:
  - Filters `video_play_time > 2.5` using `filter_by_feature_value` (`field_type='float'`).
  - Expects req_id outputs `[[b'huggfyfixyz'], [b'mbzc'], ...]`.
- `test_filter_by_value_not_in`:
  - Writes `FilterValues` proto files (bytes + int64) and uses `operand_filepath` with `in/not-in`.
  - For bytes: `not-in` filters out `hello/world`, expects `excluded/300/400` (or `300/400` when using file).
  - For int64: `in` keeps chnid `[20,30,666]`, expects did values `world/excluded/400`.
  - Both `filter_by_value` and `filter_by_feature_value` must match.
- `test_filter_by_value_all`:
  - Uses `filter_by_feature_value` with `op='all'` on `chnids` list; only `did='excluded'` passes.
- `test_map_id`:
  - `map_id({123:0,456:1,789:2}, default=-1)` transforms `[123,456,789,912]` → `[0,1,2,-1]`.
- `test_filter_by_fids`:
  - Filters instances that contain both slots 2 and 3; verifies resulting ragged values match `get_fid_v1` for indices 1..4.
- `test_multi_label_gen`:
  - Builds labels based on `head_to_index` mapping and action rules; expects label vectors with INVALID_LABEL sentinel except for the matched task.
- `test_string_to_variant`:
  - Builds framed Instance bytes (with headers); one empty record allowed; `string_to_variant` preserves shape; `variant_to_zeros` callable.
- `test_has_variant`:
  - `has_variant` returns `True` for a valid variant tensor.
- `test_switch_slot_batch`:
  - `switch_slot_batch` with mix of in-place and copy-to-suffix behavior; verifies slot IDs in resulting ragged tensors (`>> 48` equals shared slot when expected).
- `test_gen_fid_mask_int64` / `test_gen_fid_mask_int32`:
  - `gen_fid_mask(ragged, fid=3)` yields `[1.,1.,0.,0.]` for both row_splits dtypes.
- `test_negative_sample_with_positive_actions`:
  - Iterates 1000 synthetic samples, applies `negative_sample` with action priority and per-action drop rates.
  - Asserts deterministic outcomes for positive labels and specific action cases; logs drop-rate ratios for matched/mismatched actions.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests` plus any ops crates that implement feature_utils kernels.
- Rust public API surface: tests should call Rust equivalents of `feature_utils` functions and dataset helpers.
- Data model mapping: use Rust dataset pipelines to parse the same fixtures and assert outputs.
- Feature gating: many tests require TF-runtime-backed custom ops (string/variant parsing, label ops, switch-slot, negative sampling).
- Integration points: `monolith-data` parsing, `monolith-proto` for Instance/Example, and dataset fixtures under `monolith/native_training/data/training_instance`.

**Implementation Steps (Detailed)**
1. Port helpers to Rust test utilities: serialize `Instance` protos, write framed records (headers + lengths), and parse with Rust pipelines.
2. Copy the Python expected outputs into Rust assertions (actions arrays, label arrays, invalid label sentinel values).
3. Implement FilterValues file generation in Rust to test `operand_filepath` parity.
4. Recreate dataset pipelines for each test case (including ExampleBatch `instance_reweight`).
5. Add parallel filter test to validate thread-safety/caching behavior.
6. Keep negative-sample test deterministic for mandatory branches; log or assert ratios only if stable.

**Tests (Detailed)**
- Python tests: this file (primary reference).
- Rust tests: new `feature_utils_tests.rs` with one test per Python case + helpers.
- Cross-language parity test: run Python and Rust on same temp fixtures; compare arrays and variant validity.

**Gaps / Notes**
- Many tests depend on `monolith/native_training/data/training_instance/*.pb` fixtures; ensure these are accessible to Rust tests.
- INVALID_LABEL sentinel appears as `-3.4028235e+38` in Python output; Rust must match exact float.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/item_pool_hook.py`

<a id="monolith-native-training-data-item-pool-hook-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 109
- Purpose/role: SessionRunHook to save/restore item pool state during training.
- Key symbols/classes/functions: `ItemPoolSaveRestoreHook`.
- External dependencies: `tensorflow`, `save_item_pool`, `restore_item_pool`, `POOL_KEY`.
- Side effects: writes/reads item pool checkpoints; logs progress.

**Required Behavior (Detailed)**
- `begin()`:
  - Retrieves pools from TF collection `POOL_KEY`.
  - Creates placeholders for save/restore steps.
  - Reads checkpoint state from `model_dir`; if present, builds `restore_item_pool` op.
  - Builds `save_item_pool` op.
- `after_create_session()`:
  - If not PREDICT:
    - Reads global step, restores item pool from checkpoint (if available) using step parsed from checkpoint path.
- `after_run()`:
  - If TRAIN and `save_steps>0`: saves pool when global step advances by `save_steps`.
- `end()`:
  - If TRAIN: saves pool once more if global step advanced.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/src` (item pool ops).
- Rust public API surface: item pool save/restore hook (if training hooks exist).
- Data model mapping: TF collections → Rust registry.
- Feature gating: item pool feature.
- Integration points: training hook lifecycle.

**Implementation Steps (Detailed)**
1. Implement save/restore of item pool state in Rust.
2. Add training hook for periodic save and restore on startup.

**Tests (Detailed)**
- Python tests: `monolith/native_training/data/item_pool_test.py`.
- Rust tests: add item pool save/restore tests.
- Cross-language parity test: compare pool state serialization.

**Gaps / Notes**
- Requires TF collection `POOL_KEY` and custom item pool ops.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/item_pool_test.py`
<a id="monolith-native-training-data-item-pool-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 58
- Purpose/role: Tests create/save/restore/check of item pool ops.
- Key symbols/classes/functions: `ItemPoolTest.test_create_item_pool`.
- External dependencies: `tensorflow`, `feature_utils` item pool ops.
- Side effects: writes item pool files under `$HOME/<user>/tmp/monolith/data/test`.

**Required Behavior (Detailed)**
- `setUpClass`:
  - Creates item pool, randomly fills, and saves to model path with `nshards=2`.
- `test_create_item_pool`:
  - Restores pool and checks it using `item_pool_check`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests`.
- Rust public API surface: item pool ops tests.
- Data model mapping: same save/restore workflow.
- Feature gating: item pool support.
- Integration points: data utils.

**Implementation Steps (Detailed)**
1. Add Rust unit test that saves/restores item pool and validates contents.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: TODO (manual)
- Cross-language parity test: TODO (manual)

**Gaps / Notes**
- TODO (manual)

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/kafka_dataset_test.py`
<a id="monolith-native-training-data-kafka-dataset-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 239
- Purpose/role: Integration test for KafkaDataset ingestion and label parsing.
- Key symbols/classes/functions: `start_producer`, `KafkaDatasetTest.test_kafka_dataset`.
- External dependencies: `kafka.KafkaProducer`, `tensorflow`, `KafkaDataset`, `parse_instances/parse_examples`, `add_label`.
- Side effects: produces Kafka messages to a real cluster; sleeps and joins producer thread.

**Required Behavior (Detailed)**
- Flags control Kafka connection, topic, and data generation.
- `start_producer(input_type)`:
  - Generates Example/Instance/ExampleBatch protos and writes to Kafka with length-prefixed encoding.
  - Uses hard-coded SASL credentials and sleeps 10s before production.
- `test_kafka_dataset(input_type, output_type)`:
  - Starts producer thread.
  - Creates `KafkaDataset` with given variant/output types.
  - Applies `add_label` with config string (click head optional).
  - Batches, parses into features, splits label vector into task labels.
  - Iterates for `num_batch` and prints results.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests`.
- Rust public API surface: Kafka dataset ingestion tests.
- Data model mapping: Kafka stream → dataset parser.
- Feature gating: Kafka support.
- Integration points: data pipeline.

**Implementation Steps (Detailed)**
1. Provide integration tests only in environments with Kafka.
2. Mock Kafka for unit tests to avoid hard-coded credentials.

**Tests (Detailed)**
- Python tests: this file (integration).
- Rust tests: TODO (manual)
- Cross-language parity test: TODO (manual)

**Gaps / Notes**
- TODO (manual)

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/multi_flow_test.py`
<a id="monolith-native-training-data-multi-flow-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 125
- Purpose/role: Tests split/merge flow on instance dataset using lagrangex headers.
- Key symbols/classes/functions: `MultiFlowTest.test_data_flow`.
- External dependencies: `tensorflow`, `PBDataset`, `parse_instances`, `Instance` proto.
- Side effects: writes/reads `data.pb` under `TEST_TMPDIR`.

**Required Behavior (Detailed)**
- `setUpClass`:
  - Generates `NUM_INSTANCE` Instance protos with random fids, line_id fields.
  - Writes lagrangex header and length-prefixed data to `data.pb`.
- `mk_kgx_header(dataflow)`:
  - Computes Java hash code for `dataflow`, writes 4-byte header.
- `test_data_flow`:
  - Reads dataset with `lagrangex_header=True`.
  - Splits into flows by device_types and merges back.
  - Parses instances and batches; expects 8 batches of size 512.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests`.
- Rust public API surface: split_flow / merge_flow dataset operations.
- Data model mapping: lagrangex header parsing.
- Feature gating: none.
- Integration points: data pipeline.

**Implementation Steps (Detailed)**
1. Add lagrangex header parsing and flow split/merge in Rust datasets.
2. Add test for split/merge on synthetic instance data.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: TODO (manual)
- Cross-language parity test: TODO (manual)

**Gaps / Notes**
- TODO (manual)

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/negative_gen_test.py`
<a id="monolith-native-training-data-negative-gen-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 253
- Purpose/role: Tests negative sampling generation for Instance/Example datasets.
- Key symbols/classes/functions: `NegativeGenTest.test_dataset_target`.
- External dependencies: `tensorflow`, `PBDataset`, `negative_gen`, `parse_instances/parse_examples`.
- Side effects: writes a temporary `{variant_type}.pb` file.

**Required Behavior (Detailed)**
- `setUpClass`:
  - Generates sample data with random FIDs and labels; writes length-prefixed protos.
  - Tracks per-channel pos/neg counts and per-gid counts.
- `test_dataset_target`:
  - Reads PBDataset and applies `negative_gen` with configured params:
    - `neg_num`, `start_num`, `max_item_num`, `cache_only_pos`, `per_channel`, `throw_origin`, `throw_origin_neg`.
  - Parses dataset and counts pos/neg labels; verifies counts and expected ranges.
  - Ensures total count equals pos+neg.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests`.
- Rust public API surface: negative sampling dataset transform.
- Data model mapping: `negative_gen` functionality in Rust.
- Feature gating: none.
- Integration points: dataset pipeline.

**Implementation Steps (Detailed)**
1. Implement negative sampling logic in Rust datasets.
2. Add tests for per-channel and non-channel sampling boundaries.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: TODO (manual)
- Cross-language parity test: TODO (manual)

**Gaps / Notes**
- TODO (manual)

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/parse_sparse_feature_test.py`
<a id="monolith-native-training-data-parse-sparse-feature-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 1833
- Purpose/role: Validates sparse feature sharding logic and fused layout parsing across ExampleBatch/Example/Instance.
- Key symbols/classes/functions: `DataOpsV2Test`, `DataOpsV2TestFitPreV2`, `DataOpsV2Testv4`, `DataOpsV2TestFitPre`.
- External dependencies: `tensorflow`, `parse_instances/parse_examples/parse_example_batch`, `sharding_sparse_fids`, proto `FeatureConfigs`.
- Side effects: reads training_instance fixtures; prints debug output.

**Required Behavior (Detailed)**
- Implements reference sharding calculations for multiple versions (v2/v3/v4).
- Validates that `sharding_sparse_fids` outputs (`fid_map`, offsets, row splits) match manually computed results.
- Tests for:
  - ExampleBatch sharding with shared features.
  - Example sharding with generated v2 features.
  - Instance sharding with v1+v2 features.
  - Pre-v2 compatibility path (`DataOpsV2TestFitPre`).
- Uses `ParserCtx.enable_fused_layout` toggle to compare base vs fused outputs.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests`.
- Rust public API surface: sparse sharding utilities and fused layout parser.
- Data model mapping: feature configs → shard maps and offsets.
- Feature gating: none.
- Integration points: parsing pipeline for distributed embedding.

**Implementation Steps (Detailed)**
1. Implement sharding_sparse_fids equivalent in Rust.
2. Port the reference sharding calculations to Rust tests.
3. Compare fused vs non-fused parsing outputs.

**Tests (Detailed)**
- Python tests: this file (extensive).
- Rust tests: TODO (manual)
- Cross-language parity test: TODO (manual)

**Gaps / Notes**
- TODO (manual)

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/parsers.py`
<a id="monolith-native-training-data-parsers-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 782
- Purpose/role: Parsing utilities that turn Monolith Instance/Example/ExampleBatch variant tensors into feature dicts, plus sharding sparse fid helpers and parser context management.
- Key symbols/classes/functions: `ParserCtx`, `ShardingSparseFidsOpParams`, `ProtoType`, `parse_instances`, `parse_examples`, `parse_example_batch`, `parse_example_batch_list`, `sharding_sparse_fids`, `sharding_sparse_fids_with_context`, `_add_dense_features`, `_add_extra_features`, `_assemble`.
- External dependencies: TensorFlow, `LineId` proto, `FeatureConfigs` proto, `LabelConf` proto, `FeatureList`, `gen_monolith_ops` custom kernels, `logging_ops`, `native_task_context`, `FLAGS.dataset_use_dataservice`.
- Side effects: populates TF collections via `add_to_collections`; writes to global parser context; logs timing metrics (`logging_ops.emit_timer`); registers required feature names via `add_feature` when example-batch parsing.

**Required Behavior (Detailed)**
- `ParserCtx` (context manager):
  - Global `_default_parser_ctx` is used if none exists; `get_default_parser_ctx()` creates `ParserCtx(False)` once.
  - `enable_resource_constrained_roughsort` (class-level flag) injects `item_id` into `extra_features` when parsing instances.
  - `enable_fused_layout` toggles v2 parsing ops and sharded sparse fid handling.
  - `parser_type` is set to `'instance'`, `'example'`, or `'examplebatch'` by parse functions.
  - `sharding_sparse_fids_op_params` holds op configuration (see below) and drives `sharding_sparse_fids_with_context` behavior.
  - `set/get` store arbitrary per-parse context values (e.g., `batch_size`).
  - `sharding_sparse_fids_features_insert_to_features` injects nested dict values into `features` with `__sharding_sparse_fids__` prefix; supports two-level dicts only.
  - `sharding_sparse_fids_features_parse_from_features` reverses the prefixing and removes those keys from `features`.
- `ShardingSparseFidsOpParams` dataclass:
  - Fields: `num_ps`, `use_native_multi_hash_table`, `unique` (callable), `transfer_float16`, `sub_table_name_to_config`, `feature_configs`, `enable_gpu_emb`, `use_gpu`.
- `ProtoType.get_tf_type(proto_type)`:
  - Maps proto field types to tf dtypes: INT → `tf.int64`, FLOAT → `tf.float32`, STRING → `tf.string`.
  - Raises `Exception('proto_type {} is not support'.format(proto_type))` for unknown types.
- `_add_dense_features(names, shapes, types, dense_features, dense_feature_shapes, dense_feature_types)`:
  - Requires `dense_features` and `dense_feature_shapes` non-null, same length, shapes > 0.
  - Defaults `dense_feature_types` to `[tf.float32] * len(dense_features)` if None; otherwise lengths must match.
  - Appends to `names`, `shapes`, `types`.
- `_add_extra_features(names, shapes, types, extra_features, extra_feature_shapes)`:
  - Requires `extra_features` and shapes non-null, same length, shapes > 0.
  - Resolves dtype from `LineId` descriptor per field; raises `Exception(f"{name} is not in line id, pls check!")` if missing.
- `_assemble(sparse_features, names, shapes, types, out_list, batch_size)`:
  - For sparse features: takes `split = out_list[i]` (reshaped to `(batch_size+1,)` if batch_size provided) and `value = out_list[i + len(names)]`; returns `tf.RaggedTensor.from_row_splits`.
  - For dense features: uses `out_list[i]` directly.
  - Returns dict of feature name → tensor/ragged tensor.
- `parse_instances(tensor, fidv1_features, fidv2_features, dense_features, dense_feature_shapes, dense_feature_types, extra_features, extra_feature_shapes)`:
  - If `ParserCtx.enable_resource_constrained_roughsort` is True, ensures `item_id` is in `extra_features` with shape 1.
  - Validates dense feature inputs and defaults types to `tf.float32`.
  - Sets parser context type `'instance'` and writes multiple lists to TF collections + context (fidv1/fidv2/dense/extra, shapes/types).
  - Non-fused layout:
    - For `fidv1_features`: adds feature names from slots via `get_feature_name_and_slot`; if all entries are strings, resolves slots via `FeatureList.parse()` and raises `RuntimeError("fidv1_features error")` on failure.
    - Adds `fidv2_features` names; sets shapes to `-1` and types to `tf.int64`.
    - Asserts no duplicate names.
    - Calls `parse_instance_ops.parse_instances(...)` and `_assemble` with sparse features.
  - Fused layout:
    - If no names, injects `__FAKE_FEATURE__` with shape 1/float32.
    - Calls `parse_instances_v2` and `_assemble` (no sparse features list).
    - If `sharding_sparse_fids_op_params` present and (`use_gpu` or `FLAGS.dataset_use_dataservice`), calls `sharding_sparse_fids_with_context(instances, features, ctx)`.
    - Else stores `instances` under `__sharding_sparse_fids__sparse_features` key.
    - Removes `__FAKE_FEATURE__` before returning.
- `parse_examples(...)` and `parse_example_batch(...)`:
  - Same dense/extra validation pattern as `parse_instances`.
  - Sets parser context type `'example'` or `'examplebatch'` and stores config in TF collections.
  - If `is_example_batch()` is True, registers required features via `add_feature`: sparse features, dense features (adds `__LABEL__` for label), and `__LINE_ID__` for extra features.
  - Non-fused: names from sparse features, shapes `-1`, types `tf.int64`, then calls `parse_examples`/`parse_example_batch` and `_assemble` (batch_size from context for example_batch).
  - Fused: same `__FAKE_FEATURE__` fallback, uses `parse_examples_v2`/`parse_example_batch_v2`, then `sharding_sparse_fids_with_context` or stores under `__sharding_sparse_fids__sparse_features`.
- `sharding_sparse_fids(tensor, ps_num, feature_cfgs, unique, input_type, parallel_flag, fid_list_ret_list, version)`:
  - Normalizes `input_type` (`example_batch` → `examplebatch`).
  - Builds sorted `table_name_list` from `feature_cfgs.feature_configs[*].table`; `ps_num=1` if 0; `table_count = len(table_name_list) * ps_num`.
  - Uses `logging_ops.tensors_timestamp` around op call and emits timer `sharding_sparse_fids` with tag `model_name` from `native_task_context`.
  - Calls versioned custom op (`sharding_sparse_fids_v5/v4/v3/v2` or legacy) returning fid lists, row splits, offsets, and sizes.
  - Asserts list lengths for versions 5/4; returns either raw lists (if `fid_list_ret_list` or `version==4`) or dicts keyed by `table:ps_index` with row splits and row_split_size.
- `sharding_sparse_fids_with_context(sparse_features, features, parser_ctx)`:
  - Calls `sharding_sparse_fids` with params from `parser_ctx.sharding_sparse_fids_op_params`.
  - If `enable_gpu_emb`: inserts `shards_value`, `shards_row_lengths`, `shards_table_row_lengths`, offsets, `batch_size`, `fid_list_emb_row_lenth` into `features` using prefixed keys.
  - Else inserts `shards`, offsets, `batch_size`, size stats; if `use_native_multi_hash_table`, also inserts `shards_row_split` and `shards_row_split_size`.
- `parse_example_batch_list(tensor_list, label_config, positive_label, negative_label, names, shapes, dtypes, extra_features)`:
  - Optionally parses `label_config` (semicolon-separated tasks, each `pos_actions:neg_actions`) into `LabelConf`, and adds `label` feature with shape `len(tasks)`.
  - Marks `shapes[i] == -1` as sparse, appends `tf.int64` to `dtypes` for sparse values (to match op output list shape).
  - Calls `parse_example_batch_list` op with serialized label conf, then `_assemble`.
- Error semantics:
  - Extensive `assert` checks for list lengths/shape values, duplicates, and supported types; specific exceptions for invalid LineId fields and fidv1_features name mapping.
- Metrics/logging:
  - `sharding_sparse_fids` emits a timer metric named `sharding_sparse_fids` with model_name tag.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/src` (parsing), `monolith-rs/crates/monolith-proto` (LineId/FeatureConfigs), optional TF backend for custom ops.
- Rust public API surface: `parsers` module with `parse_instances`, `parse_examples`, `parse_example_batch`, and sharding helpers; `ParserCtx` analog for context state.
- Data model mapping: TF Variant/RaggedTensor → Rust datasets/feature maps; need ragged representation and feature registry.
- Feature gating: fused layout parsing, sharding_sparse_fids, and GPU embedding paths behind feature flags.
- Integration points: datasets (`datasets.py`), feature registry (`feature_list.py`), training pipelines expecting collections metadata.

**Implementation Steps (Detailed)**
1. Implement a Rust `ParserCtx` with context manager semantics (scoped override) and a global default.
2. Port `_add_dense_features`, `_add_extra_features`, and `_assemble` with equivalent validation and ragged construction.
3. Implement `parse_instances`/`parse_examples`/`parse_example_batch` in Rust, honoring `enable_fused_layout` and `enable_resource_constrained_roughsort` behavior.
4. Provide `FeatureList` lookups for fidv1 slot-name mapping and raise equivalent errors on failure.
5. Persist metadata to a Rust collection registry mirroring `add_to_collections` semantics.
6. Implement sharding_sparse_fids and sharding_sparse_fids_with_context around native kernels or TF runtime bindings; preserve timing metric emission.
7. Implement parse_example_batch_list with label_config parsing and label feature insertion.
8. Add tests for parsing shape/type inference, ragged assembly, and sharding outputs using small fixture tensors.

**Tests (Detailed)**
- Python tests: `data_ops_test.py`, `parse_sparse_feature_test.py`, `feature_utils_test.py`, `tf_example_to_example_test.py` (parsing paths).
- Rust tests: parser unit tests for each parse_* function; sharding_sparse_fids smoke tests (if backend available).
- Cross-language parity test: parse the same fixture files and compare feature dict keys, shapes, and ragged values.

**Gaps / Notes**
- Fused layout paths depend on custom ops (`parse_*_v2` and `sharding_sparse_fids_*`); must be backed by TF runtime or re-implemented.
- `parse_example_batch_list` mutates dtypes length to match op outputs; replicate this behavior exactly.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/tf_example_to_example_test.py`
<a id="monolith-native-training-data-tf-example-to-example-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 183
- Purpose/role: End-to-end test that converts TF Example records to Monolith Example variants via `tf_example_to_example`, then parses with `parse_examples` and asserts fid/dense defaults.
- Key symbols/classes/functions: `serialize_example`, `get_fid_v2`, `calc_hash_value`, `TFExampleToExampleTest.test_tf_example_to_example`.
- External dependencies: TensorFlow TFRecord, numpy RNG, `tf_example_to_example`, `parse_examples`.
- Side effects: writes `/tmp/test.tfrecord` with 10k TF Examples; uses TF1 session.

**Required Behavior (Detailed)**
- Helper functions:
  - `_bytes_feature`, `_float_feature`, `_int64_feature` wrap values into `tf.train.Feature` list types.
  - `serialize_example(feature0, feature1, feature2, feature3, feature4)` builds a `tf.train.Example` with:
    - `feature0`: int64 (bool values allowed)
    - `feature1`: int64
    - `feature2`: bytes
    - `feature3`: float
    - `feature4`: float
  - `get_fid_v2(slot, signature)` uses `fid_v2_mask=(1<<48)-1` and returns `(slot<<48) | (signature & mask)`.
  - `calc_hash_value(val)` returns `int(log2(abs(val)+1))`.
- `test_tf_example_to_example`:
  - Disables TF2 behavior (`tf.compat.v1.disable_v2_behavior()`), uses TF1 session graph.
  - Generates 10k samples:
    - `feature0`: random bools
    - `feature1`: random ints in [0,4]
    - `feature2`: bytes from `strings[feature1]`
    - `feature3`: random normal float
    - `feature4`: random normal float
  - Writes TFRecord file `/tmp/test.tfrecord` with serialized Examples.
  - Dataset pipeline:
    - `TFRecordDataset` → `map(tf_example_to_example)` with:
      - `sparse_features={'feature0':1,'feature1':2,'feature4':3}` (fid_v2 slots)
      - `dense_features=['feature2']`
      - `label='feature3'`
      - `instance_weight=None`
    - Batch size 2 → `map(parse_examples)` with:
      - `sparse_features=['not_existed1','feature0','feature1','feature4']`
      - `dense_features=['label','feature2','feature3','not_existed2','instance_weight']`
      - `dense_feature_types=[float32,string,float32,float32,float32]`
      - `dense_feature_shapes=[1,1,1,1,1]`
  - In session loop (5k batches):
    - `not_existed1` ragged has zero values.
    - `feature0/feature1` fids equal `get_fid_v2(slot, original int/bool)` per batch.
    - `feature4` fid uses slot 3 and `calc_hash_value` of float value (log2(abs(val)+1)).
    - `label` equals original `feature3` (float) per batch.
    - `feature3` dense output is `[0,0]` (missing in conversion), `not_existed2` is `[0,0]`.
    - `instance_weight` defaults to `[1.0,1.0]`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests` (TFExample conversion) + `monolith-proto` for Example parsing.
- Rust public API surface: test helper for TF Example serialization; conversion op `tf_example_to_example` or equivalent.
- Data model mapping: TF Example bytes → Monolith Example variant → parsed feature dict.
- Feature gating: TFRecord read/write and TFExample conversion backend.
- Integration points: `feature_utils.tf_example_to_example` and `parsers.parse_examples` parity.

**Implementation Steps (Detailed)**
1. Implement TF Example serialization helper in Rust tests (or load TFRecord fixtures generated in Python).
2. Provide `tf_example_to_example` conversion in Rust with the same slot/fid-v2 behavior and hashing for float feature4.
3. Ensure missing sparse features emit empty ragged values; missing dense features emit zeros; `instance_weight` defaults to 1.0.
4. Add a Rust test that mirrors batch size 2 with deterministic input, validating fid values and dense defaults.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: `tf_example_to_example.rs` (new) that asserts identical fid/dense outputs.
- Cross-language parity test: generate a fixed TFRecord in Python and run Rust conversion+parse on it; compare outputs.

**Gaps / Notes**
- Uses `/tmp/test.tfrecord`; Rust tests should use tempdir paths.
- The hash for float sparse feature4 is `int(log2(abs(val)+1))`; must match exactly.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/training_instance/python/instance_dataset_op.py`
<a id="monolith-native-training-data-training-instance-python-instance-dataset-op-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 166
- Purpose/role: TF DatasetSource wrapper around custom `instance_dataset` op for reading serialized Instance records from PB files or stdin, with optional sharding/interleave utilities.
- Key symbols/classes/functions: `_PBInstanceDataset`, `PBInstanceDatasetV2`, `create_instance_dataset`, alias `PBInstanceDataset`.
- External dependencies: TensorFlow Dataset internals, `gen_monolith_ops.instance_dataset`, `distributed_dataset.create_dynamic_sharding_dataset`, `ckpt_hooks.disable_iterator_save_restore`, TF matching_files.
- Side effects: disables iterator save/restore when reading from stdin; logs initialization; uses TF fatal logging on missing file.

**Required Behavior (Detailed)**
- `_PBInstanceDataset(file_name, use_snappy, has_sort_id, kafka_dump, kafka_dump_prefix)`:
  - Calls custom op `instance_dataset` with tensors for file name, snappy, and header flags.
  - `element_spec` is scalar string `TensorSpec([], tf.string)`.
- `PBInstanceDatasetV2`:
  - If `file_name` is empty string, treats input as stdin and calls `ckpt_hooks.disable_iterator_save_restore()`.
  - Creates `_PBInstanceDataset` internally and forwards variant tensor into `DatasetV2`.
  - `_clone` merges kwargs with stored defaults.
  - `_inputs()` returns `[]`.
- `create_instance_dataset(...)`:
  - `files_list=None` defaults to `['']` (stdin).
  - If a single file and no glob expansion/sharding/dynamic sharding, returns `PBInstanceDatasetV2` directly; validates existence when file is non-empty and logs fatal on missing file.
  - `enable_dynamic_sharding=True`:
    - Converts to dataset via `distributed_dataset.create_dynamic_sharding_dataset` and `flat_map` with `PBInstanceDatasetV2`.
  - `enable_sharding=True`:
    - Requires a single file pattern; uses `MatchingFilesDataset`, shards by `shard_num/shard_index`, logs shard info; forces `use_snappy=True`.
  - Else:
    - Uses `MatchingFilesDataset` if `expand_glob_path=True`, otherwise `Dataset.from_tensor_slices`.
  - Final dataset uses `interleave` with `cycle_length`, `block_length`, `num_parallel_calls`, `deterministic=False`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/src` for dataset creation and file reader.
- Rust public API surface: `pb_instance_dataset` (or similar) and `create_instance_dataset` with matching options.
- Data model mapping: custom op variant tensor → Rust stream of serialized Instance bytes.
- Feature gating: stdin mode, dynamic sharding, and TF MatchingFiles behavior.
- Integration points: datasets (`datasets.py`), training pipelines that expect PBInstanceDataset semantics.

**Implementation Steps (Detailed)**
1. Implement a Rust dataset source that wraps Instance file reading with flags for sort_id/kafka headers and snappy.
2. Mirror stdin special case and disable iterator save/restore in Rust equivalents.
3. Implement glob expansion, sharding, and dynamic sharding (or document unsupported) with identical defaults.
4. Preserve interleave behavior and `deterministic=False` semantics.

**Tests (Detailed)**
- Python tests: `instance_dataset_op_test_stdin.py`, other dataset tests using PBInstanceDataset.
- Rust tests: dataset source tests for stdin vs file, sharding path, missing file handling.
- Cross-language parity test: read a fixture PB file in Python and Rust and compare record sequence.

**Gaps / Notes**
- Uses TF Dataset internals; Rust must define a similar streaming abstraction.
- Missing file uses `logging.fatal` in TF; decide equivalent behavior in Rust (panic or error).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/training_instance/python/instance_dataset_op_test_stdin.py`
<a id="monolith-native-training-data-training-instance-python-instance-dataset-op-test-stdin-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 58
- Purpose/role: Smoke test for `PBInstanceDataset` reading from stdin (empty file_name), batching, and parsing with `parse_instances`.
- Key symbols/classes/functions: `PBInstanceDataset`, `parse_instances`, `testInstanceDataset`.
- External dependencies: TensorFlow v1 session, `instance_dataset_ops`, `parse_instance_ops`.
- Side effects: expects stdin data stream; logs warnings; runs one batch read.

**Required Behavior (Detailed)**
- Defines feature lists:
  - `FIDV1_FEATURES = [1..9]`
  - `FIDV2_FEATURES = ['fc_360d_ml_convert_cid', 'fc_360d_ml_convert_advertiser_id']`
  - `FLOAT_FEATURES = ['fc_muse_finish_rough_10168_uid_d128']` with dim `[128]`
  - `INT64_FEATURES = ['fc_dense_external_action']` with dim `[1]`
- `parse(serialized)` calls `parse_instances(serialized, fidv1, fidv2, float_feats, float_dims, int64_feats, int64_dims)`.
- `testInstanceDataset()`:
  - Creates `PBInstanceDataset(file_name='', has_sort_id=True, kafka_dump_prefix=True)` (stdin path).
  - `batch(32)` and `map(parse)`.
  - Builds one-shot iterator, fetches one batch, logs `elements['sample_rate']`.
- Script mode: disables eager and runs `testInstanceDataset()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests`.
- Rust public API surface: stdin dataset reader + parse_instances in Rust.
- Data model mapping: stream of framed Instance records from stdin.
- Feature gating: stdin support in dataset source; parse_instances.
- Integration points: dataset source in `instance_dataset_op.py` parity.

**Implementation Steps (Detailed)**
1. Add a Rust test that simulates stdin input (e.g., pipe fixture data into the dataset reader).
2. Ensure parsing handles fidv1/fidv2 and dense features as in Python.
3. Validate batch size 32 returns expected keys including `sample_rate`.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: add stdin dataset smoke test with a small fixture.
- Cross-language parity test: compare parsed batch fields from the same stdin fixture.

**Gaps / Notes**
- This test assumes stdin provides valid Instance records; Rust tests should supply a controlled fixture stream.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/training_instance/python/instance_negative_gen_dataset_op_test.py`
<a id="monolith-native-training-data-training-instance-python-instance-negative-gen-dataset-op-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 283
- Purpose/role: Tests negative sample generation dataset (`negative_gen` and `InstanceNegativeGenDataset`) over Instance PB data, including per-channel ring buffer behavior.
- Key symbols/classes/functions: `InsNegativeDatasetTest`, `parse1`, `testNegativeGen`, `testRingBufferCache`, `testIgnoreReaNegInstance`, `testUseNegInstance`.
- External dependencies: TensorFlow, `PBDataset`, `InstanceNegativeGenDataset`, `PbType`, custom parse ops `parse_variant_instances`.
- Side effects: reads fixture `monolith/native_training/data/training_instance/instance.pb`.

**Required Behavior (Detailed)**
- Constants:
  - `FILE_NAME` fixture file for Instance PB.
  - `CHANNEL_SLOT=357`, `GROUP_SLOTS=[200..242]`, `LABEL_FIELD='actions'`, `LABEL_INDEX=0`.
  - Negative labels `NEGATIVE_LABEL=-2`, `NEGATIVE_LABEL2=-1`.
  - `GID='gid'` used as misc int64 feature.
- `parse1(pb_variant)`:
  - Uses a fixed `FIDV1_FEATURES` list and `parse_variant_instances` with `misc_int64_features=[GID]`.
- `testNegativeGen`:
  - Builds `PBDataset` (Instance → Instance) with headers; applies `dataset.negative_gen` with:
    - `neg_num=7`, `channel_slot`, `group_slots`, `per_channel_sample=True`, `start_num=0`, `max_group_num_per_channel=10000`, `label_field='actions'`, `label_index=0`, `negative_label=-2`, `use_neg_ins=True`.
  - Batches 8 and parses.
  - Asserts in first batch that `channel_res[0][0] == channel_res[0][i]` for i in 1..7 (negatives share channel), and label at index 1 equals `NEGATIVE_LABEL`.
- `testRingBufferCache`:
  - Same negative_gen config except `max_group_num_per_channel=2`.
  - Collects ~1024 samples; groups by channel and verifies ring buffer behavior:
    - For channels with >2 samples, checks that group fids from later samples are not present in the first sample when gids differ.
  - Logs `valid_count` of checked non-overlapping group features.
- `testIgnoreReaNegInstance`:
  - First applies `dataset.negative_gen(..., negative_label=-2, use_neg_ins=True)`.
  - Then wraps with `InstanceNegativeGenDataset(..., negative_label=-1, use_neg_ins=False)`.
  - Asserts label at index 1 equals `NEGATIVE_LABEL2` (real negatives ignored).
- `testUseNegInstance`:
  - Same as previous but `use_neg_ins=True` in wrapper.
  - Asserts labels: index1/index2 are `NEGATIVE_LABEL2`, index3/index4 are `NEGATIVE_LABEL` (mix of generated vs real negatives).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests` with negative_gen dataset implementation in `monolith-data`.
- Rust public API surface: `negative_gen` dataset operator and `InstanceNegativeGenDataset` wrapper.
- Data model mapping: Instance variant streams with channel/group fid slots and label field `actions`.
- Feature gating: requires negative generation ops + item pool or group cache implementation.
- Integration points: dataset pipeline in `datasets.py` and negative-gen custom ops in Rust/TF backend.

**Implementation Steps (Detailed)**
1. Implement `negative_gen` dataset operator and wrapper in Rust with identical parameters.
2. Ensure per-channel sampling and ring buffer cache behavior match Python semantics.
3. Expose `use_neg_ins` toggle to include/exclude existing negatives.
4. Add Rust tests that load the same fixture PB file and verify label/channel/group constraints.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: new `instance_negative_gen_dataset_op_test.rs` mirroring each test case.
- Cross-language parity test: compare label distributions and channel/group assignments on identical fixtures.

**Gaps / Notes**
- Depends on `instance.pb` fixture and custom ops; ensure Rust has compatible dataset and parse op support.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/training_instance/python/parse_instance_ops.py`
<a id="monolith-native-training-data-training-instance-python-parse-instance-ops-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 245
- Purpose/role: Instance parsing helpers that wrap custom ops to extract fid and dense features into Ragged/Dense tensors, including LineId fields and repeated fields handling.
- Key symbols/classes/functions: `_parse_instance_impl`, `parse_instances2`, `parse_instances`, `monolith_raw_parse_instance`.
- External dependencies: TensorFlow ragged internals (`RowPartition`), `gen_monolith_ops` custom kernels, `get_slot_feature_name`, parser_utils hooks (imported but not used here).
- Side effects: Builds RaggedTensor with precomputed row ids/nrows; uses default misc feature lists.

**Required Behavior (Detailed)**
- `_parse_instance_impl(serialized, fidv1_features, fidv2_features, float_features, float_feature_dims, int64_features, int64_feature_dims, string_features, string_feature_dims, misc_float_features, misc_float_dims, misc_int64_features, misc_int64_dims, misc_string_features, misc_string_dims, cc_op)`:
  - Normalizes all list args to empty lists if None.
  - Calls `cc_op` (custom op) with counts `N/M/O/P/Q/R/S` and all feature lists/dims.
  - Builds `ragged_keys` from `fidv1_features` (via `get_slot_feature_name`) plus `fidv2_features`.
  - For each ragged split/value pair, constructs `RowPartition` with precomputed `value_rowids` and `nrows`, then `tf.RaggedTensor(values, row_partition, internal=True)`.
  - Returns dict mapping ragged + float + int64 + string + misc_* features to their tensors in order.
- `parse_instances2(...)`:
  - Thin wrapper that calls `_parse_instance_impl` with `parse_instance_ops.monolith_parse_instances`.
- `parse_instances(...)`:
  - Adds defaults: `misc_float_features=['sample_rate']`, `misc_int64_features=['req_time','uid']`, `misc_repeated_float_features=['label']`.
  - Normalizes list args to empty lists and sets default dims (1) for misc features.
  - Calls `parse_instances2` with concatenated misc+repeated feature lists/dims.
  - Reshapes non-repeated misc float/int64 features to 1-D (`tf.reshape(features[key], [-1])`).
  - Returns feature dict.
- `monolith_raw_parse_instance`:
  - Exposes `parse_instance_ops.MonolithRawParseInstance` for testing only.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/src` for instance parsing; ragged support in `monolith-tensor`.
- Rust public API surface: `parse_instances`/`parse_instances2` equivalents returning `HashMap<String, Tensor>` with ragged types.
- Data model mapping: custom op outputs (splits/values) → Rust ragged tensors with cached row ids/nrows.
- Feature gating: depends on custom parsing kernels or TF runtime.
- Integration points: dataset parsing pipelines, tests in `parse_instance_ops_test.py` and other training_instance tests.

**Implementation Steps (Detailed)**
1. Implement Rust wrappers for `monolith_parse_instances` (or bind to TF op) that return splits/values arrays.
2. Build ragged tensors with cached row metadata to match TF `RowPartition` behavior.
3. Match default misc feature lists and reshape semantics in `parse_instances`.
4. Preserve feature key ordering in output map to match downstream expectations.

**Tests (Detailed)**
- Python tests: `parse_instance_ops_test.py`, `instance_dataset_op_test_stdin.py`, `instance_negative_gen_dataset_op_test.py`.
- Rust tests: parser unit tests for ragged vs dense outputs; ensure misc defaults applied.
- Cross-language parity test: parse a fixture instance and compare fid/dense outputs.

**Gaps / Notes**
- Uses TF internal ragged APIs; Rust must provide equivalent row-partition caching to avoid perf regressions.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/training_instance/python/parse_instance_ops_test.py`
<a id="monolith-native-training-data-training-instance-python-parse-instance-ops-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 185
- Purpose/role: Validates instance parsing ops and ragged encoding helpers, including missing field defaults and raw parse concat behavior.
- Key symbols/classes/functions: `RaggedEncodingHelperTest`, `ParseInstancesTest`, `RawParseInstanceTest`, helper `generate_instance`, `make_fid_v1`, `make_fid_v2`.
- External dependencies: TensorFlow, `proto_parser_pb2.Instance`, `parse_instance_ops`, `parser_utils.RaggedEncodingHelper`.
- Side effects: none.

**Required Behavior (Detailed)**
- `generate_instance()` builds an Instance with:
  - fidv1 list `[make_fid_v1(i,i) for i in range(10)]`.
  - fidv2 feature `name='fidv2'` with `make_fid_v2(100,i)`.
  - float feature `ue` length 16 (i * 1e-5), int64 feature `int64_feature=100`, string feature `string_feature='test_string'`.
  - label `[1.1,2.2,3.3]`, line_id fields: uid=110, sample_rate=0.5, req_time=64, actions=[0,100], user_id='123'.
- `RaggedEncodingHelperTest.testExpandContract`:
  - Builds a ragged tensor, expands with `RaggedEncodingHelper.expand(..., with_precomputed_value_rowids=True)` and verifies `value_rowids` equals TF-computed.
  - `contract` returns original ragged values and preserves cached value_rowids.
- `ParseInstancesTest.testParseInstance`:
  - Calls `parse_instances2` with explicit fidv1/fidv2/float/int64/string/misc fields and dims.
  - Asserts:
    - 10 fidv1 slots returned (`slot_*`).
    - `slot_1` uses fid_v2 encoding for v1 slot values.
    - `fidv2` ragged equals `get_test_fidv2()`.
    - dense features: `int64_feature=[[100]]`, `string_feature=[[b'test_string']]`, `ue` length 16, `sample_rate=[[0.5]]`, `label=[[1.1,2.2,3.3]]`, `uid=[[110]]`, `actions=[[0,100]]`, `user_id=[['123']]`.
- `ParseInstancesTest.testParseInstanceV1Only`:
  - `parse_instances2` with `fidv1_features=[1]` yields `slot_1` with fid_v1 encoding.
- `ParseInstancesTest.testParseInstanceWithMissingFields`:
  - Requests extra missing fields; expects:
    - Missing ragged fid slot → empty ragged (`[[]]`).
    - Missing fidv2 → empty ragged.
    - Missing float → zeros of specified dim.
    - Missing int64/string → zeros/empty strings of specified dim.
- `RawParseInstanceTest.test_concat`:
  - Calls `monolith_raw_parse_instance` with `fid_output_type='CONCAT'`.
  - Expects first tensor offsets `[0,1,2,len(fidv2)+2]` and second tensor concatenated fids `[fidv1 slot0, fidv1 slot1] + fidv2 list`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests` and ragged utils in `monolith-tensor`.
- Rust public API surface: ragged encoding helper, `parse_instances2`, raw parse op if exposed.
- Data model mapping: same fid encoding rules (v1/v2), missing-field defaults.
- Feature gating: raw parse op requires custom kernel support.
- Integration points: parse_instance_ops implementation and parser_utils utilities.

**Implementation Steps (Detailed)**
1. Implement ragged encoding helper in Rust and verify cached rowids/nrows behavior.
2. Port `parse_instances2` tests with the same synthetic Instance fixture.
3. Ensure missing fields return empty ragged or zero-filled dense tensors as specified.
4. If raw parse op is supported, add concat mode test for offsets + fid list order.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: new `parse_instance_ops_test.rs` matching each test case.
- Cross-language parity test: compare parsed feature dicts for identical serialized Instance.

**Gaps / Notes**
- `slot_*` fidv1 encoding differs between v1-only path and v2 conversion; Rust must replicate both behaviors.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/training_instance/python/parser_utils.py`
<a id="monolith-native-training-data-training-instance-python-parser-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 85
- Purpose/role: Utilities for parser pipelines, including queued extra-parse steps and ragged encoding expansion/contract helpers.
- Key symbols/classes/functions: `_extra_parse_steps`, `add_extra_parse_step`, `RaggedEncodingHelper.expand`, `RaggedEncodingHelper.contract`, `advanced_parse`.
- External dependencies: TensorFlow, `ragged_utils.fused_value_rowids`.
- Side effects: mutates global deque of extra parse steps; mutates RaggedTensor internal row partition caches during `contract`.

**Required Behavior (Detailed)**
- `_extra_parse_steps`:
  - Global `deque` used to store parse step callables.
- `add_extra_parse_step(parse_fn)`:
  - Appends parse_fn to `_extra_parse_steps`.
- `RaggedEncodingHelper.expand(name_to_ragged_ids, with_precomputed_nrows=True, with_precomputed_value_rowids=False)`:
  - For each RaggedTensor value, returns a dict with:
    - `values`, `row_splits`, optional `nrows` (if flag), optional `value_rowids` computed via `ragged_utils.fused_value_rowids` (if flag).
  - Non-ragged entries pass through unchanged.
- `RaggedEncodingHelper.contract(name_to_ragged_ids)`:
  - For dict entries with `values` and `row_splits`, rebuilds `tf.RaggedTensor.from_row_splits(..., validate=False)`.
  - If `nrows` present, asserts `_row_partition._nrows` is None before assigning.
  - If `value_rowids` present, asserts `_row_partition._value_rowids` is None before assigning.
  - Non-dict entries pass through unchanged.
- `advanced_parse(features)`:
  - Pops parse steps from `_extra_parse_steps` in FIFO order and applies each to `features`.
  - Returns final features dict.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/src` (parser utilities) + `monolith-tensor` for ragged.
- Rust public API surface: `add_extra_parse_step` and `advanced_parse` equivalents; ragged expand/contract helpers.
- Data model mapping: RaggedTensor internal encodings → Rust ragged structure with cached rowids/nrows.
- Feature gating: none.
- Integration points: `parse_instance_ops_test.py` uses `RaggedEncodingHelper`.

**Implementation Steps (Detailed)**
1. Implement a global queue of parse steps (with proper synchronization if used across threads).
2. Implement ragged expand/contract; ensure cached rowids/nrows are set only once.
3. Mirror `fused_value_rowids` behavior using Rust ragged utilities.

**Tests (Detailed)**
- Python tests: `parse_instance_ops_test.py` (`RaggedEncodingHelperTest`).
- Rust tests: add unit tests that expand, contract, and verify rowids/nrows caching.
- Cross-language parity test: compare ragged values and cached rowids against Python output.

**Gaps / Notes**
- Directly mutates internal ragged partition caches; Rust must provide an equivalent escape hatch.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/training_instance/python/pb_datasource_ops.py`
<a id="monolith-native-training-data-training-instance-python-pb-datasource-ops-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 48
- Purpose/role: Thin wrappers around `gen_monolith_ops` for filtering and negative sampling on variant tensors in training_instance pipelines.
- Key symbols/classes/functions: `filter_by_fids`, `filter_by_value`, `negative_sample`, `variant_dummy`.
- External dependencies: TensorFlow, `gen_monolith_ops` custom kernels.
- Side effects: none beyond custom op invocation.

**Required Behavior (Detailed)**
- `filter_by_fids(variant, filter_fids, has_fids, select_fids, has_actions)`:
  - Passes list args (defaults to empty) to `pb_datasource_ops.set_filter`.
- `filter_by_value(variant, field_name, op, operand)`:
  - Calls `pb_datasource_ops.value_filter` with given field/op/operand.
- `negative_sample(variant, drop_rate, label_index, threshold)`:
  - Calls `pb_datasource_ops.negative_sample` with drop/threshold params.
- `variant_dummy(variant)`:
  - Calls `pb_datasource_ops.variant_dummy`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/src` (ops wrappers).
- Rust public API surface: minimal wrappers for filtering/negative sampling on variant streams.
- Data model mapping: variant tensor → Rust variant representation.
- Feature gating: custom op availability (TF backend).
- Integration points: training_instance datasets and tests.

**Implementation Steps (Detailed)**
1. Add Rust wrapper functions that call the underlying kernel backend.
2. Ensure default empty list behavior matches Python.
3. Expose in public API for dataset pipelines.

**Tests (Detailed)**
- Python tests: indirectly via `instance_negative_gen_dataset_op_test.py`.
- Rust tests: minimal unit tests for wrappers if backend available.
- Cross-language parity test: verify behavior using fixed fixtures.

**Gaps / Notes**
- This is a thin wrapper; underlying op semantics are defined in C++/TF kernels.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/training_instance/python/test_data_utils.py`
<a id="monolith-native-training-data-training-instance-python-test-data-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 15
- Purpose/role: Placeholder test utility module; currently only imports TensorFlow.
- Key symbols/classes/functions: none.
- External dependencies: TensorFlow.
- Side effects: none.

**Required Behavior (Detailed)**
- No runtime behavior beyond importing TensorFlow.

**Rust Mapping (Detailed)**
- Target crate/module: none.
- Rust public API surface: none.
- Data model mapping: none.
- Feature gating: none.
- Integration points: none.

**Implementation Steps (Detailed)**
1. No Rust port needed unless file is expanded in Python.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: none.
- Cross-language parity test: none.

**Gaps / Notes**
- File is effectively empty; keep an eye on future changes.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/transform/transforms.py`
<a id="monolith-native-training-data-transform-transforms-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 250
- Purpose/role: Declarative transform objects that serialize to `TransformConfig` proto for dataset transform pipelines (filters, label generation, logical composition).
- Key symbols/classes/functions: `Transform` (ABC), `Compose`, `FilterByFid`, `FilterByAction`, `FilterByLabel`, `FilterByValue`, `AddLabel`, `LogicalOr`.
- External dependencies: `transform_config_pb2`, `LineId` proto descriptor for field validation.
- Side effects: none; purely builds proto configs with validation asserts.

**Required Behavior (Detailed)**
- `Transform` abstract base:
  - `as_proto()` returns `transform_config_pb2.TransformConfig`.
  - `_is_leaf_node()` distinguishes leaf vs composite.
- `Compose(transforms)`:
  - Requires all items are `Transform` instances.
  - `as_proto` merges each transform’s proto into a single `TransformConfig` via `MergeFrom` in order.
  - `_is_leaf_node` returns False.
- `FilterByFid(has_fids, filter_fids, select_fids)`:
  - `as_proto` appends a `basic_config.filter_by_fid` entry with respective lists.
  - `_is_leaf_node` True.
- `FilterByAction(has_actions)`:
  - Adds `basic_config.filter_by_action.has_actions`.
  - `_is_leaf_node` True.
- `FilterByLabel(thresholds)`:
  - Adds `basic_config.filter_by_label.thresholds`.
  - `_is_leaf_node` True.
- `FilterByValue(field_name, op, operand, keep_empty=False)`:
  - Validates `op` in `{gt,ge,eq,lt,le,neq,between,in,not-in,all,any,diff,startswith,endswith}`.
  - Validates `field_name` exists in `LineId` descriptor; `operand` is not None.
  - Infers operand type based on field cpp_type and op:
    - Repeated fields (`field.has_options`): only `all/any/diff` allowed; only integer types; operand int or list of int.
    - Float/double: `between` uses list; otherwise single float.
    - Int types: `in/not-in/between` use list; otherwise single int.
    - String: operand is str or list of str; else `RuntimeError("params error!")`.
  - Stores `float_operand`, `int_operand`, `string_operand`, `keep_empty`.
  - `as_proto` fills `basic_config.filter_by_value` with operands and flags.
  - `_is_leaf_node` True.
- `AddLabel(config, negative_value, new_sample_rate)`:
  - Parses config `pos_actions:neg_actions:sample_rate` separated by `;` (skips empty parts).
  - Adds `basic_config.add_label` with negative value + new sample rate and a `task_label_config` entry per task.
  - `_is_leaf_node` True.
- `LogicalOr(x, y)`:
  - Requires both `x` and `y` are leaf nodes.
  - `as_proto` creates `logical_or_config` and copies `basic_config` from each side.
  - `_is_leaf_node` False.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/src` (transform config builders).
- Rust public API surface: `Transform` trait + concrete structs mirroring Python class names; `as_proto()` to `TransformConfig`.
- Data model mapping: transform structs → `transform_config_pb2::TransformConfig`.
- Feature gating: none; pure config serialization.
- Integration points: `TransformDataset` op uses serialized config (see `datasets.py`).

**Implementation Steps (Detailed)**
1. Create Rust transform trait with `as_proto` and `is_leaf` methods.
2. Implement concrete transforms with identical validation (asserts or Result errors).
3. Preserve `Compose` merge ordering and `LogicalOr` leaf-only requirement.
4. Implement `FilterByValue` operand parsing based on LineId descriptor in Rust.

**Tests (Detailed)**
- Python tests: `transforms_test.py`.
- Rust tests: unit tests for each transform’s proto encoding and validation.
- Cross-language parity test: serialize configs in Python and Rust and compare bytes.

**Gaps / Notes**
- `FilterByValue` uses `LineId` field metadata to infer types; Rust must mirror the same descriptor mapping.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/transform/transforms_test.py`
<a id="monolith-native-training-data-transform-transforms-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 70
- Purpose/role: Smoke tests that build transform configs and log the resulting protobufs; no assertions beyond successful construction.
- Key symbols/classes/functions: `TransformsTest`, test methods for each transform type.
- External dependencies: `transforms` module, `absl.logging`, `unittest`.
- Side effects: logs serialized proto configs.

**Required Behavior (Detailed)**
- `test_filter_by_fid`: builds `FilterByFid(has_fids=[1], filter_fids=[2,3], select_fids=None)` and logs proto.
- `test_filter_by_action`: builds `FilterByAction(has_actions=[4])` and logs proto.
- `test_filter_by_label`: builds `FilterByLabel(thresholds=[-100, -100])` and logs proto.
- `test_add_label`: builds `AddLabel(config='1,2:3:1.0;4::0.5', negative_value=0.0, new_sample_rate=0.3)` and logs proto.
- `test_logical_or`: builds `LogicalOr(FilterByAction([1,2]), FilterByFid([10000000]))` and logs proto.
- `test_compose`: builds `Compose([...])` with multiple transforms including `LogicalOr`, logs proto.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests`.
- Rust public API surface: transform builders and `as_proto` serialization.
- Data model mapping: Rust transforms → TransformConfig protobufs.
- Feature gating: none.
- Integration points: verifies transform config serialization used by `TransformDataset`.

**Implementation Steps (Detailed)**
1. Add Rust tests that construct equivalent transforms and ensure `as_proto` succeeds.
2. Optionally compare serialized proto bytes to Python output for deterministic configs.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: `transforms_test.rs` with equivalent constructions.
- Cross-language parity test: serialize each config in both languages and compare bytes.

**Gaps / Notes**
- Tests are smoke-only; Rust should at least mirror construction and serialization.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/data/utils.py`
<a id="monolith-native-training-data-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 55
- Purpose/role: Simple slot/feature-name helpers for training_instance parsing; global mapping of feature names to slots with TOB env toggle.
- Key symbols/classes/functions: `enable_tob_env`, `get_slot_feature_name`, `get_slot_from_feature_name`, `register_slots`, globals `TOBENV`, `USED_FREATUE_NAMES`, `NAME_TO_SLOT`.
- External dependencies: none.
- Side effects: mutates global dictionaries for name/slot mapping.

**Required Behavior (Detailed)**
- Globals:
  - `TOBENV` default `False` toggles slot name prefix (`slot_` vs `fc_slot_`).
  - `USED_FREATUE_NAMES` maps arbitrary feature names to assigned slot ids (incrementing).
  - `NAME_TO_SLOT` maps feature name → slot id (explicit).
- `enable_tob_env()`:
  - Sets `TOBENV = True` globally.
- `get_slot_feature_name(slot)`:
  - Returns `"fc_slot_{slot}"` if `TOBENV` else `"slot_{slot}"`.
- `get_slot_from_feature_name(feature_name)`:
  - If in `NAME_TO_SLOT`, return mapped slot.
  - Else if name starts with `slot_` or `fc_slot_`, parse suffix int; return int or `None` if non-numeric.
  - Else use `USED_FREATUE_NAMES`: assign a new slot id (`len+1`) if missing and return it.
- `register_slots(sparse_features)`:
  - Accepts list/tuple of ints or dict name→slot.
  - For list: asserts ints and converts to dict via `get_slot_feature_name`.
  - Updates `NAME_TO_SLOT` with provided mapping.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/src` (feature utils).
- Rust public API surface: slot/feature-name mapping utilities with global registry or context-bound mapping.
- Data model mapping: feature names → slots used by parsing/feature extraction.
- Feature gating: TOB env toggle.
- Integration points: `parse_instance_ops` (fidv1 slot naming), feature list parsing.

**Implementation Steps (Detailed)**
1. Implement a global or context-local registry for `NAME_TO_SLOT` and `USED_FEATURE_NAMES` with deterministic assignment.
2. Provide `enable_tob_env` toggle and `get_slot_feature_name` logic.
3. Mirror `get_slot_from_feature_name` fallback behavior for unknown names.
4. Implement `register_slots` with list/dict handling and type checks.

**Tests (Detailed)**
- Python tests: none explicit.
- Rust tests: add unit tests for TOB/non-TOB naming and deterministic slot assignment.
- Cross-language parity test: compare mapping outputs for a fixed sequence of names.

**Gaps / Notes**
- Uses global mutable state; Rust must be careful about concurrency or test isolation.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/debugging/debugging_client.py`
<a id="monolith-native-training-debugging-debugging-client-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 98
- Purpose/role: CLI client to query debugging server endpoints for variable values or feature embeddings.
- Key symbols/classes/functions: `main`, CLI flags `type`, `variable_names`, `feature_ids`, `feature_name`, `feature_names`.
- External dependencies: `requests`, `json`, protobuf `text_format`, `embedding_hash_table_pb2.EntryDump`.
- Side effects: HTTP POSTs to local debugging server; logs results; raises exceptions on invalid flag combos.

**Required Behavior (Detailed)**
- Flags:
  - `--type` must be `debugging_variables` or `debugging_features`.
  - `--variable_names` list for variable lookup.
  - `--feature_ids` list for feature lookup.
  - `--feature_name` single name to pair with all ids.
  - `--feature_names` list of names; must be same length as `feature_ids` if provided.
- `debugging_variables` flow:
  - If `variable_names` empty → log and return.
  - POST JSON `{"variable_names": [...]}` to `http://127.0.0.1:<port>/debugging/variables`.
  - Response JSON contains `STATUS`, `SUCCESS/FAIL`, `MSG` keys; on FAIL log reason and return.
  - `MSG` is JSON-encoded dict name→value; logs each variable value or "Not exist".
- `debugging_features` flow:
  - Disallow providing both `feature_name` and `feature_names`.
  - If `feature_ids` empty → log and return.
  - If `feature_name` set, expand to list same length as ids.
  - Validate `len(feature_names) == len(feature_ids)` else raise.
  - POST JSON `{"feature_names": [...], "feature_ids": [...]}` to `/debugging/features`.
  - On FAIL log reason and return.
  - `MSG` is JSON-encoded dict name→id→textproto of `EntryDump`.
  - If present, parse textproto into `EntryDump` and log; else log "Not exist".
- Script mode: sets logging verbosity INFO, disables eager, and runs app.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/debugging` (or CLI crate).
- Rust public API surface: CLI command for debugging server queries.
- Data model mapping: JSON request/response; `EntryDump` textproto parsing.
- Feature gating: requires debugging server running locally.
- Integration points: `debugging_server.py` endpoints.

**Implementation Steps (Detailed)**
1. Implement a Rust CLI that mirrors flags and validation.
2. POST to `/debugging/variables` and `/debugging/features` with identical JSON payloads.
3. Parse response JSON; for features, parse textproto into `EntryDump` (protobuf text format parser).
4. Match logging output patterns and error handling (exceptions for invalid flags).

**Tests (Detailed)**
- Python tests: none.
- Rust tests: integration tests with a mocked debugging server (or golden responses).
- Cross-language parity test: compare outputs against Python client for same server responses.

**Gaps / Notes**
- Depends on `requests` and protobuf text parsing; Rust needs equivalent libraries.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/debugging/debugging_server.py`
<a id="monolith-native-training-debugging-debugging-server-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 217
- Purpose/role: Flask server that exposes debugging endpoints to fetch variable values and feature embeddings from a running training cluster using saved graph metadata.
- Key symbols/classes/functions: `DebuggingWorker`, `create_app`, `/debugging/variables`, `/debugging/features`, `main`.
- External dependencies: Flask, TensorFlow server/meta_graph import, `debugging_info_pb2`, `embedding_hash_table_pb2`, custom ops `monolith_hash_table_lookup_entry`.
- Side effects: spins up TF server, reads model_dir debugging info, imports meta graph, starts Flask server.

**Required Behavior (Detailed)**
- Flags:
  - `--host`, `--port`, `--model_dir` required to bind server and load debugging info.
- `DebuggingWorker(model_dir)`:
  - Reads `DebuggingInfo` proto from `utils.get_debugging_info_file_name(model_dir)`.
  - Starts a local TF server and builds a fake worker cluster where the last worker is the local server; chief/ps addresses come from debugging info.
  - Builds `feature_name_config_map` from debugging info and initializes a `MergedMultiTypeHashTable` with a dummy factory.
  - Creates session config via `cluster_manager.generate_session_config` and imports the meta graph from `utils.get_meta_graph_file_name(model_dir)`.
- `fetch_variables(variable_names)`:
  - Filters requested variables to those present in imported graph; returns dict name → stringified value.
- `fetch_features(feature_names, feature_ids)`:
  - Maps feature names to merged table names via `slot_mapping` in merged table.
  - Buckets by PS index using `fid % num_ps`.
  - For each table/ps index, fetches EntryDump via `monolith_hash_table_lookup_entry`.
  - Parses EntryDump bytes to text proto and returns dict `feature_name -> {fid: textproto}`.
- `create_app()`:
  - Constructs Flask app and a `DebuggingWorker`.
  - `/debugging/variables`: expects JSON `variable_names`; returns `{status, msg}` with msg JSON string.
  - `/debugging/features`: expects JSON `feature_names` + `feature_ids` (same length); returns `{status, msg}`.
  - On exceptions, returns `status=fail` with traceback in msg.
- `main`:
  - Calls `env_utils.setup_hdfs_env()` and runs Flask app.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/debugging` (server) + TF backend bindings for graph/variables.
- Rust public API surface: debugging server binary exposing `/debugging/variables` and `/debugging/features`.
- Data model mapping: DebuggingInfo proto, EntryDump parsing, table lookup by PS index.
- Feature gating: requires TF runtime + custom ops for hash table lookup.
- Integration points: debugging_client CLI, model_dir metadata generation.

**Implementation Steps (Detailed)**
1. Implement a Rust server (e.g., axum/warp) with matching endpoints and JSON payloads.
2. Load DebuggingInfo and meta graph; create TF session with cluster config matching Python.
3. Implement variable lookup and table lookup logic with PS sharding by `fid % num_ps`.
4. Return responses with identical JSON structure and error handling.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: integration tests with mocked debugging info + TF session (if feasible).
- Cross-language parity test: compare responses for same model_dir and queries.

**Gaps / Notes**
- Requires TF runtime and custom ops; Rust implementation may need to shell out or depend on TF C API.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/demo.py`
<a id="monolith-native-training-demo-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 57
- Purpose/role: Minimal demo entrypoint to run a local CPU training job for `TestFFMModel` with export enabled.
- Key symbols/classes/functions: `main`, CLI flags `num_ps`, `model_dir`.
- External dependencies: `cpu_training.local_train`, `TestFFMModel`, `ExportMode`.
- Side effects: launches a training run, writes checkpoints/exports to `model_dir`.

**Required Behavior (Detailed)**
- CLI flags:
  - `--num_ps`: number of parameter servers; `0` runs locally.
  - `--model_dir`: output directory.
- `main`:
  - Builds params via `TestFFMModel.params()` and sets:
    - `params.name = 'test_ffm_model'`
    - `params.train.per_replica_batch_size = 64`
    - `params.serving.export_when_saving = True`
    - `params.serving.export_mode = ExportMode.DISTRIBUTED`
  - Calls `cpu_training.local_train(..., steps=100, save_checkpoints_steps=50)`.
- Script mode: enables INFO logging and disables eager execution.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/examples` (or CLI).
- Rust public API surface: demo binary that runs a comparable CPU training flow.
- Data model mapping: params/config to Rust training config.
- Feature gating: requires training pipeline parity with Python.
- Integration points: `cpu_training` equivalent and model definition in Rust.

**Implementation Steps (Detailed)**
1. Implement a Rust demo that configures an equivalent model and training loop.
2. Mirror flags (`num_ps`, `model_dir`) and default values.
3. Ensure checkpoint/export cadence matches (`steps=100`, `save_checkpoints_steps=50`).

**Tests (Detailed)**
- Python tests: none.
- Rust tests: optional smoke test that runs a short training stub.
- Cross-language parity test: compare produced artifacts for a short run (if feasible).

**Gaps / Notes**
- Depends on `TestFFMModel` and `cpu_training` parity in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/dense_reload_utils.py`
<a id="monolith-native-training-dense-reload-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 457
- Purpose/role: Custom checkpoint restore logic for dense variables, including aliasing/mapping between old and new variable names and partitioned variable splitting.
- Key symbols/classes/functions: `CustomRestoreListener`, `add_mapping_rules`, `node_name`, `get_new_name`, `get_guess_name`, `split_name`, `calc_reorder_info`, `get_full_prefix`, `update_var_name_mapping_for_dense`, `infer_variable_name`, `calc_feed_dict`.
- External dependencies: TensorFlow checkpoint reader, `CheckpointRestorerListener`, `is_exporting`, numpy, regex patterns.
- Side effects: inspects checkpoint files, builds custom restore ops in graph collections, logs extensive info, may create `clear_nn` flag file logic.

**Required Behavior (Detailed)**
- Globals/regex:
  - `CUSTOM_RESTORE_OP` collection key and `CustomRestoreListenerKey` name.
  - `PAT` matches `.../part_<num>/...` for partitioned vars.
  - `DensePat` matches dense layer names for bias/kernel/trainable_kernel_norm.
  - `_NameMapping` regex rules for special-case name conversions; `add_mapping_rules` merges additional regex patterns.
- `node_name(name)`:
  - Strips whitespace, trailing `/`, leading `^`, and `:0` suffix if numeric.
- `get_new_name(name)`:
  - Deduplicates repeated path terms in a name (preserving order) and rejoins with `/`.
- `get_guess_name(name)`:
  - Applies `_NameMapping` regex patterns; returns formatted guess if matched, else original.
- `split_name(name)`:
  - Splits trailing digits; returns `(base, int_suffix)` or `(name, 0)` if none.
- `calc_reorder_info(names, is_ordered=True)`:
  - Optionally sorts by numeric suffix.
  - Returns `(need_reorder, base)` where base is `dense_` for base name `dense` else base name; `need_reorder` when suffix sequence isn't contiguous starting at 0/1 or when multiple names.
- `get_full_prefix(short_prefix, prefix_set)`:
  - Chooses the longest prefix in `prefix_set` that ends with `short_prefix`.
- `update_var_name_mapping_for_dense(var_name_mapping)`:
  - Groups dense layer vars by prefix/dense_name/bias; uses `DensePat` to normalize names.
  - For dense layers with multiple indices, may reorder and rename to `dense_{i}` or base name.
  - Ensures bias entries are present; fills missing entries into `var_name_mapping`.
- `CustomRestoreListener`:
  - `__init__`: accepts `alias_map`, `clear_nn`, `continue_training`, `model_dir`, `enable_alias_map_auto_gen` (defaults True).
  - `begin()`:
    - Skip if `is_exporting()`.
    - Loads checkpoint state from `model_dir`; sets `ckpt_name`.
    - If `clear_nn`:
      - Uses `clear_nn` flag file to skip if present.
      - Adds `global_variables_initializer` to `CUSTOM_RESTORE_OP`; if `continue_training`, adds placeholder + assign op for global_step.
    - Else if `_need_build_custom_init_graph(variables)`:
      - Creates placeholders and assign ops for each variable; stores placeholders + alias map into `CUSTOM_RESTORE_OP`.
  - `_need_build_custom_init_graph(variables)`:
    - Auto-generates alias_map when not provided and enabled:
      - Reads ckpt var names; checks compatibility by removing `/part_<n>`.
      - Builds `var_name_mapping` from `get_new_name(old_name)` to `old_name` and refines via `update_var_name_mapping_for_dense`.
      - Builds `alias_map` for each variable; handles missing dense names with `miss_dense_names` / `miss_dense_map`.
      - For unresolved names, uses `get_guess_name` or `miss_dense_map`; if still missing, logs warning and returns False.
    - Returns True if any variable name is not covered by alias_map values.
- `infer_variable_name(names)`:
  - Removes `/part_<n>` segments to infer merged variable names.
- `calc_feed_dict(ckpt, alias_map, placeholders)`:
  - Builds reverse map old_name → list of new variable names.
  - If inferred new names all exist in checkpoint, returns None (no alias restore needed).
  - Otherwise, builds feed dict mapping placeholders to ckpt tensors.
  - For partitioned vars (multiple new names):
    - Handles dense name grouping and ordering.
    - Sorts by partition index extracted via `PAT`.
    - Splits old tensor by first-dimension sizes from placeholders (`np.split`) and assigns each split to its placeholder.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/checkpoint` (restore hooks) + `monolith-checkpoint` utilities.
- Rust public API surface: custom restore listener/hook that can build alias maps and feed dicts.
- Data model mapping: checkpoint variable names → current graph variable names, including partitioned tensors.
- Feature gating: requires TensorFlow checkpoint reader or compatible reader in Rust.
- Integration points: `basic_restore_hook` and training session initialization.

**Implementation Steps (Detailed)**
1. Implement name normalization helpers (`node_name`, `get_new_name`, `split_name`, `get_guess_name`) in Rust.
2. Port dense name mapping logic (`update_var_name_mapping_for_dense`) including reorder rules and prefix resolution.
3. Implement alias-map auto generation using checkpoint metadata and dense mappings.
4. Build custom restore ops/feeds with placeholders and assign ops; support `clear_nn` + `continue_training` global step update.
5. Implement partitioned variable splitting logic equivalent to `calc_feed_dict`.

**Tests (Detailed)**
- Python tests: `dense_reload_utils_test.py`.
- Rust tests: unit tests for name mapping, alias generation, and feed dict splitting.
- Cross-language parity test: use a sample ckpt with renamed vars and ensure alias restore works identically.

**Gaps / Notes**
- Heavy TF internals: requires checkpoint reader and graph variable manipulation in Rust.
- Auto alias mapping may be fragile; parity requires matching regex and reorder heuristics exactly.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/dense_reload_utils_test.py`
<a id="monolith-native-training-dense-reload-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 192
- Purpose/role: Tests for dense reload utilities: variable name inference, feed dict splitting for partitioned vars, and custom restore listener modes.
- Key symbols/classes/functions: `DenseReloadUtilsTest`, `setUpClass`, `test_infer_variable_name`, `test_calc_feed_dict`, `test_alias_map_listener`, `test_clear_nn_listener`.
- External dependencies: TensorFlow, `GlorotNormal`, `Ones`, `infer_variable_name`, `calc_feed_dict`, `CustomRestoreListener`.
- Side effects: creates and deletes checkpoint files under `./ckpt`.

**Required Behavior (Detailed)**
- `setUpClass`:
  - Builds a graph with `global_step`, a partitioned variable `partition` (shape 1280x512), and `small_var`.
  - Saves checkpoint `ckpt/test-<global_step>` in cwd.
- `tearDownClass`:
  - Removes `./ckpt` directory if exists.
- `test_infer_variable_name`:
  - Creates a partitioned variable and checks `infer_variable_name` removes `/part_xx` to yield `{partition_var.name:0}`.
- `test_calc_feed_dict`:
  - Creates partitioned `partition2` and `small_var2`.
  - Builds `alias_map` mapping new names to old checkpoint names (`small_var2` → `small_var`, `partition2 parts` → `partition`).
  - Creates placeholders with `origin_name` for each var/partition.
  - `calc_feed_dict` returns mapping for each alias; asserts shapes match partition shapes.
- `test_alias_map_listener`:
  - Builds same alias_map/placeholders and calls `CustomRestoreListener(alias_map=..., model_dir=./ckpt).begin()` (no asserts, just should not error).
- `test_clear_nn_listener`:
  - Creates `CustomRestoreListener(clear_nn=True, model_dir=./ckpt)` and calls `begin()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests`.
- Rust public API surface: dense reload utilities and custom restore listener.
- Data model mapping: checkpoint vars/partitioned vars to Rust checkpoint reader and feed dict logic.
- Feature gating: requires checkpoint reader and graph variable introspection.
- Integration points: `dense_reload_utils.py` implementation.

**Implementation Steps (Detailed)**
1. Build Rust tests that create a checkpoint with partitioned variables (or mock the reader).
2. Verify `infer_variable_name` removes partition suffixes.
3. Validate `calc_feed_dict` splitting behavior for partitioned variables.
4. Ensure custom restore listener handles alias_map and clear_nn without error.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: `dense_reload_utils_test.rs` analog with temp directories.
- Cross-language parity test: compare feed dict splits on a shared checkpoint.

**Gaps / Notes**
- The Python tests rely on TF checkpoint creation; Rust tests may need to use Python-generated checkpoints.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/device_utils.py`
<a id="monolith-native-training-device-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 231
- Purpose/role: Device placement utilities for training/serving, including GPU gating, device functions, and MPI/PS placement logic.
- Key symbols/classes/functions: `enable_gpu_training`, `disable_gpu_training`, `is_gpu_training`, `get_visible_gpus`, `default_device_fn`, `maybe_device_if_allowed`, `within_placement_context_of`, `get_device_fn`, `input_device_fn`, `model_device_fn`, `serving_input_device_fn`, `skip_device`.
- External dependencies: TensorFlow DeviceSpec, `device_setter`, MPI rank helper `get_mpi_rank`, flags (`num_ps`, `enable_gpu_training`, `enable_sync_training`, `is_local`).
- Side effects: global `_GPU_PLACEMENT_ALLOWED` flag; influences device placement for ops.

**Required Behavior (Detailed)**
- GPU training flag:
  - `_GPU_PLACEMENT_ALLOWED` default False; `enable_gpu_training()` sets True; `disable_gpu_training()` sets False; `is_gpu_training()` returns it.
- `get_visible_gpus(local_rank, processes_per_gpu=1)`:
  - Ensures `processes_per_gpu` is int >= 1; returns string of `local_rank / processes_per_gpu` as GPU index.
- `_device_rule(device_name)`:
  - Returns `/device:CPU:0` when `device_name` is empty.
  - If assigned GPU but `_GPU_PLACEMENT_ALLOWED` is False or device type empty, merges with default CPU while keeping job/task/replica.
- `skip_device(op)`:
  - Returns True for summary ops (`Write*`, `*Summary`) or string `Const` ops.
- `default_device_fn(op)`:
  - Returns CPU for skipped ops; otherwise applies `_device_rule` to op device.
- `maybe_device_if_allowed(device_name)`:
  - Context manager that forces device via `_device_rule` to prevent unintended GPU placement.
- Placement context helpers:
  - `_FakeOp` and `within_placement_context_of(device_name)` check current placement via graph `_apply_device_functions`.
- `get_device_fn(cluster=None, task=None)`:
  - Determines MPI mode via `OMPI_COMM_WORLD_LOCAL_RANK`.
  - Chooses GPU vs CPU based on `FLAGS.enable_gpu_training` or `_GPU_PLACEMENT_ALLOWED`.
  - If sync training + MPI + PS: builds device spec for chief/worker based on rank and returns custom `_device_fn` that merges with op device.
  - If sync training but no PS: returns `default_device_fn`.
  - If async (no sync training):
    - Returns None for local mode or missing cluster/task.
    - Else uses `tf.compat.v1.train.replica_device_setter` with `ps_tasks=FLAGS.num_ps` and standard PS ops.
- `input_device_fn(op)`:
  - In MPI+PS+sync training returns `/job:chief|worker/replica:0/task:<idx>/device:CPU:0`, else CPU.
- `model_device_fn(op)`:
  - Similar to `_device_fn` but for model scope; uses GPU if enabled, else CPU; respects op.device and `_class` attr.
- `serving_input_device_fn(op)`:
  - Uses op.device if set, else CPU.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/device`.
- Rust public API surface: device placement utilities and device function factories.
- Data model mapping: TF DeviceSpec strings → Rust device strings used by TF runtime bindings.
- Feature gating: GPU placement gate, MPI/PS sync training, replica device setter behavior.
- Integration points: training config, session creation, input pipelines.

**Implementation Steps (Detailed)**
1. Implement GPU gating and visible GPU computation.
2. Implement device rule merging logic with default CPU and job/task retention.
3. Provide Rust equivalents of `get_device_fn`/`input_device_fn`/`model_device_fn` for sync/async modes.
4. Mirror skip-device rules for summary ops and string const.
5. Add placement-context helper or document unsupported if TF internals unavailable.

**Tests (Detailed)**
- Python tests: `device_utils_test.py`.
- Rust tests: unit tests for device rules, GPU gating, and MPI/PS device fn outputs.
- Cross-language parity test: compare device string outputs under fixed flag/env combinations.

**Gaps / Notes**
- Depends on TF internal device functions; Rust may need to mimic device strings rather than enforcing in graph.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/device_utils_test.py`
<a id="monolith-native-training-device-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 104
- Purpose/role: Tests device placement rules and GPU gating in `device_utils`.
- Key symbols/classes/functions: `DeviceUtilsTest` and methods `test_basic`, `test_cpu_only`, `test_str_context`, `test_str_nested_contexts`, `test_cpu_device_merge`, `test_gpu_device_merge`, `test_process_gpu_map`.
- External dependencies: TensorFlow, `device_utils`.
- Side effects: none.

**Required Behavior (Detailed)**
- `test_basic`: default device function places constants on `/device:CPU:0`.
- `test_cpu_only`: when GPU training disabled, explicit GPU device request is overridden to CPU.
- `test_str_context`: with GPU enabled, bare constants default to CPU, `tf.device("GPU:0")` forces GPU:0.
- `test_str_nested_contexts`: nested device contexts maintain correct placement for CPU/GPU overrides.
- `test_cpu_device_merge`: with GPU disabled, device job/task merged with CPU; `within_placement_context_of` reports CPU.
- `test_gpu_device_merge`: with GPU enabled, device job/task merged with GPU; `maybe_device_if_allowed` forces GPU:1 placement and context checks.
- `test_process_gpu_map`: `get_visible_gpus` returns expected indices for local_rank/processes_per_gpu combinations.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests`.
- Rust public API surface: device_utils functions.
- Data model mapping: device strings matching TF conventions.
- Feature gating: GPU training toggle.
- Integration points: training device placement.

**Implementation Steps (Detailed)**
1. Add Rust tests to assert device string outputs for each scenario.
2. Verify GPU gating overrides explicit GPU placement when disabled.
3. Validate visible GPU mapping logic.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: `device_utils_test.rs`.
- Cross-language parity test: compare device string outputs and placement context behavior.

**Gaps / Notes**
- Python tests rely on TF device placement; Rust tests may need to compare string outputs rather than actual TF graph placement.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distribute/distributed_dataset.py`
<a id="monolith-native-training-distribute-distributed-dataset-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 81
- Purpose/role: Builds a dynamic sharding dataset that expands glob patterns on demand using shared queues and a TF session-backed generator.
- Key symbols/classes/functions: `create_dynamic_sharding_dataset`.
- External dependencies: `str_queue.StrQueue`, `session_hooks.get_current_session`, `utils.ps_device`, `native_task_context`.
- Side effects: creates shared queues on PS0 or host; uses TF session to dequeue filenames.

**Required Behavior (Detailed)**
- `create_dynamic_sharding_dataset(glob_patterns, name)`:
  - Creates two shared string queues:
    - `glob_patterns_queue`: seeded with glob patterns.
    - `filenames_queue`: auto-enqueue filenames by expanding patterns.
  - Chooses device on PS0 if `num_ps > 0`, else default device.
  - `glob_pattern()` (tf.function): dequeues a pattern; if not out_of_range, calls `tf.io.matching_files`; else returns `""` and out_of_range.
  - `filenames_queue.dequeue()` returns `(filename_bytes, out_of_range)`.
  - `filename_generator()` runs dequeue via current session; raises `StopIteration` on out_of_range; else decodes bytes to string.
  - Builds `dataset_ops.MapDataset` over a dummy infinite dataset; maps to `tf.py_function(filename_generator)` with `preserve_cardinality=False`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/src`.
- Rust public API surface: dynamic sharding dataset builder for file patterns.
- Data model mapping: file pattern → stream of file paths.
- Feature gating: requires session hooks/queues or Rust equivalents.
- Integration points: `datasets.py` uses this for dynamic sharding.

**Implementation Steps (Detailed)**
1. Implement a Rust dynamic sharding iterator that expands patterns lazily.
2. Support shared queue semantics for multi-worker coordination (or document limitation).
3. Ensure out_of_range yields end of stream and map preserves non-cardinality.

**Tests (Detailed)**
- Python tests: `distributed_dataset_test.py`.
- Rust tests: unit tests for pattern expansion order and termination.
- Cross-language parity test: compare file lists produced for a given glob set.

**Gaps / Notes**
- Relies on TF session and custom `StrQueue`; Rust needs a coordinated queue for distributed use.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distribute/distributed_dataset_test.py`
<a id="monolith-native-training-distribute-distributed-dataset-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 124
- Purpose/role: Tests dynamic sharding dataset expansion, EOF handling, composition with TextLineDataset, and iterator save/restore.
- Key symbols/classes/functions: `DynamicShardingDatasetTest`, `gen_test_files`, `testBasic`, `testEof`, `testWithOtherDataset`, `testSaveRestore`.
- External dependencies: TensorFlow, `distributed_dataset.create_dynamic_sharding_dataset`, `session_hooks.SetCurrentSessionHook`.
- Side effects: writes temp files under `TEST_TMPDIR` and saves/loads iterator checkpoints.

**Required Behavior (Detailed)**
- `gen_test_files(files_dir)`:
  - Creates files `a_0.txt`..`e_1.txt` with two lines each: `a.0.0`, `a.0.1`, etc.
- `setUp`:
  - Uses `TEST_TMPDIR` and creates data dir + files if missing.
  - Builds glob patterns `a_*.txt`..`e_*.txt`.
- `get_test_session()`:
  - Returns `SingularMonitoredSession` with `SetCurrentSessionHook`.
- `testBasic`:
  - Reads 10 filenames from dynamic sharding dataset; expects ordered list of `a_0..e_1` full paths.
- `testEof`:
  - With empty patterns, iterator should raise `OutOfRangeError`; verifies dependent op does not mutate variable `v`.
- `testWithOtherDataset`:
  - `filename_dataset.flat_map(TextLineDataset)` yields lines; first three lines are `a.0.0`, `a.0.1`, `a.1.0`.
- `testSaveRestore`:
  - Creates saveable iterator; reads `a.0.0`, saves; reads `a.0.1`, restores; next read is still `a.0.1`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests`.
- Rust public API surface: dynamic sharding dataset + iterator save/restore (if supported).
- Data model mapping: file list → dataset → line dataset.
- Feature gating: iterator save/restore may depend on TF runtime.
- Integration points: `distributed_dataset` implementation.

**Implementation Steps (Detailed)**
1. Implement Rust tests that generate temp files and validate ordered filename emission.
2. Verify EOF behavior for empty patterns.
3. Test composition with line reader and save/restore semantics.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: `distributed_dataset_test.rs` with tempdir fixtures.
- Cross-language parity test: compare file order and resume position after restore.

**Gaps / Notes**
- `saveable` iterator behavior is TF-specific; Rust may need explicit checkpointing support.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distribute/str_queue.py`
<a id="monolith-native-training-distribute-str-queue-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 114
- Purpose/role: A TF-based string queue with save/restore support, critical section synchronization, and optional auto-enqueue when empty.
- Key symbols/classes/functions: `StrQueue`, `enqueue_many`, `dequeue`, `_raw_enqueue_many`, `_raw_dequeue`.
- External dependencies: TensorFlow `CriticalSection`, variables, tf.function.
- Side effects: maintains internal TF variables (`_arr`, `_offset`, `_arr_size`), uses critical section for synchronization.

**Required Behavior (Detailed)**
- `StrQueue.__init__(initial_elements, critical_section, auto_enqueue_fn, capacity, name)`:
  - Creates a shared `CriticalSection` (or reuses provided).
  - Initializes `_arr` (string array of size `capacity`), `_offset`, `_arr_size` as variables.
  - Enqueues `initial_elements` during initialization via control deps.
  - Uses `_var_for_init` dummy variable to ensure initial enqueue runs.
- `enqueue_many(elements)`:
  - Converts to string tensor and calls `_raw_enqueue_many` inside critical section.
- `dequeue()`:
  - Executes `_raw_dequeue` inside critical section; returns `(element, out_of_range)`.
- `_raw_enqueue_many(elements)` (tf.function):
  - Computes `old_arr_size = _arr_size - _offset`, `new_arr_size = old_arr_size + size(elements)`.
  - Asserts `new_arr_size <= capacity`.
  - Compacts array by shifting remaining elements to front, appends new elements, resets `_offset` to 0, updates `_arr_size`.
- `_raw_dequeue()` (tf.function):
  - Asserts `_offset <= _arr_size`.
  - If `auto_enqueue_fn` provided, loops while empty: calls auto fn to get `(elements, out_of_range)`; enqueues elements unless out_of_range.
  - If still empty, returns `("", True)`.
  - Else returns element at `_offset` and increments `_offset`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/src` (distributed queues).
- Rust public API surface: string queue with enqueue/dequeue and optional auto-fill.
- Data model mapping: TF variables → Rust in-memory or shared queue state.
- Feature gating: requires distributed synchronization if used across workers.
- Integration points: `distributed_dataset.create_dynamic_sharding_dataset`.

**Implementation Steps (Detailed)**
1. Implement a thread-safe queue with capacity and offset/size semantics.
2. Provide auto-enqueue hook that is called when empty.
3. Match out_of_range behavior and empty return value (`""`).
4. If using TF runtime, preserve CriticalSection semantics for shared state.

**Tests (Detailed)**
- Python tests: `str_queue_test.py`.
- Rust tests: queue enqueue/dequeue, auto-enqueue loop, capacity assert.
- Cross-language parity test: compare sequence of dequeued elements for the same auto-enqueue function.

**Gaps / Notes**
- TF CriticalSection semantics may need a custom mutex + barrier if implemented in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distribute/str_queue_test.py`
<a id="monolith-native-training-distribute-str-queue-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 67
- Purpose/role: Tests basic enqueue/dequeue behavior, initialization, out-of-range handling, and auto-enqueue logic for `StrQueue`.
- Key symbols/classes/functions: `QueueTest` with `testBasic`, `testInit`, `testOutOfRange`, `testAutoEnqueue`.
- External dependencies: TensorFlow, `str_queue.StrQueue`.
- Side effects: none.

**Required Behavior (Detailed)**
- `testBasic`:
  - Enqueues `test1`, `test2` and dequeues in order.
- `testInit`:
  - Initializes queue with `initial_elements=['test1']` and dequeues `test1`.
- `testOutOfRange`:
  - Dequeue from empty queue returns `out_of_range=True`.
- `testAutoEnqueue`:
  - `auto_enqueue` increments variable `v` and enqueues stringified values until `v > 2`, then returns out_of_range.
  - Dequeues yield `"1"`, `"2"`, then `out_of_range=True` for subsequent dequeues.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/tests`.
- Rust public API surface: `StrQueue` equivalent.
- Data model mapping: string queue semantics.
- Feature gating: none.
- Integration points: `distributed_dataset` uses StrQueue.

**Implementation Steps (Detailed)**
1. Add Rust tests that validate enqueue/dequeue ordering and init behavior.
2. Implement auto-enqueue hook test with controlled counter.
3. Verify out_of_range behavior persists after exhaustion.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: `str_queue_test.rs`.
- Cross-language parity test: compare dequeued sequences for fixed auto-enqueue behavior.

**Gaps / Notes**
- TensorFlow session semantics are not required; Rust can implement a pure queue test.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distributed_ps.py`
<a id="monolith-native-training-distributed-ps-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 2108
- Purpose/role: Distributed parameter server embedding hash table implementation (single-type and multi-type), sharded lookup/apply gradients, fused layout embedding pipelines, and GPU/Horovod/BytePS all-to-all paths.
- Key symbols/classes/functions: `ps_device`, `DistributedHashTable`, `DistributedMultiTypeHashTable`, `PartitionedHashTable`, `get_sub_table_name`, `PartitionedHashTable.gen_feature_configs`, `merge_feature_config`, `lookup`, `apply_gradients`, `_lookup_gpu`, `_apply_gradients_gpu`.
- External dependencies: TensorFlow, custom ops (`distribution_ops`, `multi_hash_table_ops`), `export_context`, `prefetch_queue`, `hvd`/`bps` (if enabled), FeatureConfigs protos.
- Side effects: creates PS-side graphs/signatures during export; emits lookup timers; enqueues prefetch queues; uses global parser context for sharding configs.

**Required Behavior (Detailed)**
- `ps_device(i)`:
  - Context manager that clears device stack (`colocate_with(None, True)`) and sets device to `utils.ps_device(i)` for PS ops.
- `DistributedHashTable` (single-type table):
  - Constructor builds per-PS tables; sends learning-rate tensors to each PS (unless exporting standalone).
  - `lookup(ids)`:
    - `tf.unique` ids, shard by `id % ps_num`, lookup on each PS, and `map_id_to_embedding` back to original order.
    - Tracks input/output tensors for backprop.
  - `assign/assign_add`: split ids/values by PS and call underlying table method.
  - `apply_gradients`: unique ids, split gradients with `map_id_to_embedding_gradient_back_prop`, apply on each PS (dedup disabled).
  - `as_op`: aggregates PS table ops.
- `DistributedMultiTypeHashTable` (multi-slot table):
  - Builds per-PS multi-type tables; supports raw API if tables are `RawMultiTypeHashTable`.
  - Export mode builds PS subgraphs with `lookup` and optional `raw_lookup` signatures.
  - `lookup(slot_to_id)`:
    - Raw API path: uses ragged IDs and `unique_key_with_value_and_offset` to reduce duplicate lookups, splits by PS, uses raw lookup and `fill_with_offset_map`, then reconstructs embeddings; returns only requested slots.
    - Non-raw path: per-slot unique/split, PS lookup (remote_predict if exporting distributed); maps back by `map_id_to_embedding`.
  - `assign/assign_add`: per-slot split by PS and call underlying table methods.
  - `reinitialize(slot, ids)`: raw-only; splits ids and concatenates status.
  - `apply_gradients`: raw path uses `raw_apply_gradients` with fused flat grads; non-raw path packs keyed tensors, optional float16 transfer.
  - `as_op` combines PS tables; `get_table_dim_sizes` delegates to cc dims.
- `get_sub_table_name(strs)`:
  - Returns `(concat, md5(concat))` for merged table naming.
- `PartitionedHashTable`:
  - `gen_feature_configs`: builds `FeatureConfigs` and `ShardingSparseFidsOpParams` based on feature configs and combiners; supports native multi-hash-table and GPU embedding modes.
  - `merge_feature_config` / `no_merge_feature_config`: compute merged sub-table names (with md5) or keep per-feature tables; handles `fc_slot_` → `slot_` extra restore names.
  - Constructor:
    - Reads `parser_ctx.sharding_sparse_fids_op_params` for PS count, native multi-table mode, feature configs, and GPU options.
    - Creates per-PS tables or GPU table; builds export signatures for lookup/raw_lookup when exporting.
    - Sets up learning-rate tensors for each sub-table.
  - `lookup(features, auxiliary_bundle, ...)`:
    - If GPU embedding enabled, delegates to `_lookup_gpu` and optionally returns callable.
    - Otherwise obtains sharded fids via `sharding_sparse_fids` or `ParserCtx`-encoded features, stores offsets and sizes in `auxiliary_bundle`.
    - Optionally returns `lookup_callable_fn` or `fused_layout_callable_fn` for two-phase lookup.
    - `call_lookup`:
      - Uses raw/native lookup or packed lookup; remote_predict in export mode.
      - Stores per-PS embeddings and optional fids/row_splits in `auxiliary_bundle`.
      - Optionally moves auxiliary tensors to GPU and enqueues prefetch queues.
    - `fused_layout_callable_fn`:
      - Calls `distribution_ops.fused_embedding_to_layout` to reconstruct layout embeddings (CPU/GPU depending on export or `_use_gpu`).
      - Uses `nest_layout` to produce output dict.
  - `apply_gradients(layout_grads_and_vars, global_step, req_time, auxiliary_bundle, async_function_mgr, async_push, grad_scale)`:
    - For non-GPU path: uses `fused_embedding_to_layout_grad` to compute per-PS grads, then applies via raw or packed update; supports async push queues.
    - Includes tensor move CPU helper for GPU-derived tensors.
    - For GPU path, delegates to `_apply_gradients_gpu`.
  - `_lookup_gpu`:
    - Uses all-to-all (HVD/BPS/custom) to exchange ids and embeddings; calls `fused_lookup` on GPU table; then all-to-all embeddings back; finally `fused_embedding_to_layout` (version 4) and `nest_layout`.
    - Populates `auxiliary_bundle` with many intermediate tensors (id_flat_t, splits, offsets, recv embeddings, etc.) and optional pipeline queues.
  - `_apply_gradients_gpu`:
    - Computes `fused_embedding_to_layout_grad` on GPU, performs all-to-all backprop (HVD/BPS/custom), and calls `fused_apply_gradient` on GPU table; supports async optimize queues.
  - `assign/assign_add`:
    - Non-GPU only; routes to `_update` or `_native_hash_table_update` depending on native multi-table mode.
  - `flatten_layout` / `nest_layout`:
    - Deterministic ordering by `feature_configs.out_configs` (sorted names); `OutType.NONE` yields list per slices.
  - Queue hooks:
    - `add_queue_hook` stores local hooks; `get_queue_hooks` collects hooks from tables.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/ps` + `monolith-hash-table` + `monolith-data`.
- Rust public API surface: distributed hash table abstractions, partitioned multi-type tables, lookup/apply_gradients APIs.
- Data model mapping: FeatureConfigs and sharded fids, embedding layouts, and fused layout conversions.
- Feature gating: export mode, raw API support, GPU embedding, Horovod/BytePS all-to-all.
- Integration points: `parsers.sharding_sparse_fids`, `embedding_combiners`, `prefetch_queue`, export signatures.

**Implementation Steps (Detailed)**
1. Implement Rust equivalents for `DistributedHashTable` and `DistributedMultiTypeHashTable` with sharding by `id % ps_num`.
2. Recreate packed tensor transfer and optional float16 transport.
3. Implement `PartitionedHashTable` with sharding feature configs and `fused_embedding_to_layout`/`_grad` equivalents.
4. Add GPU embedding path + all-to-all (if supported); otherwise gate behind features.
5. Mirror export signatures for PS-side lookups.
6. Port queue hook logic for pipelined execution.

**Tests (Detailed)**
- Python tests: `distributed_ps_test.py`, `distributed_ps_sync_test.py`, `distribution_ops_test.py`.
- Rust tests: integration tests for lookup/apply_gradients with small sharded tables and layout configs.
- Cross-language parity test: compare embedding outputs for fixed ids across PS shards.

**Gaps / Notes**
- This module is large and deeply tied to TF custom ops; full parity likely requires a TF backend or substantial Rust kernel work.
- GPU/Horovod/BytePS paths are specialized; may need staged parity plan.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distributed_ps_benchmark.py`
<a id="monolith-native-training-distributed-ps-benchmark-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 168
- Purpose/role: Benchmark tests for distributed hash table lookup and apply_gradients performance, optionally with profiling.
- Key symbols/classes/functions: `_generate_config`, `_get_vocab_hash_table_factory`, `DistributedHashTableTest.lookup`, `DistributedHashTableTest.apply_gradients`.
- External dependencies: TensorFlow local servers, `distributed_ps.DistributedHashTable`, `hash_filter_ops`, `hash_table_ops`, `embedding_hash_table_pb2`.
- Side effects: creates local PS servers, may write profiler logs under `/tmp/distributed_ps_benchmark`.

**Required Behavior (Detailed)**
- `_generate_config(servers, job_name=utils.PS_JOB_NAME)`:
  - Builds `ClusterDef` with job tasks derived from server targets; returns `ConfigProto`.
- `_get_vocab_hash_table_factory(dim)`:
  - Returns factory that builds a hash table with `EmbeddingHashTableConfig` using cuckoo + SGD(1.0) + zeros init and segment dim `dim`.
- `DistributedHashTableTest.lookup(enable_dedup, real_run=True)`:
  - Creates `ps_num=10` local servers; uses server0 with cluster config.
  - Builds hash filters and a `DistributedHashTable`, assigns add for ids 0..num_elements-1.
  - If `real_run`: lookup ids `x//2`, check values equal `x//2` repeated per dim; prints wall time; optional profiler.
  - If `real_run=False`: just runs `hash_table.as_op()` to measure overhead.
- `apply_gradients(real_run=True)`:
  - Similar setup; assigns ones to embeddings, looks up, computes `loss=0.3*embeddings`, grads; applies gradients.
  - If `real_run`: after apply, looks up and expects values `0.4` (1.0 + 0.3*?); prints timing.
  - If `real_run=False`: checks grads equal `0.3` if not profiling.
- Tests invoke both real and overhead modes.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/benches`.
- Rust public API surface: benchmark harness for distributed PS table lookup/apply gradients.
- Data model mapping: hash table config, embedding values.
- Feature gating: requires distributed PS runtime and hash table ops.
- Integration points: `distributed_ps` implementation.

**Implementation Steps (Detailed)**
1. Implement a Rust benchmark that spins up local PS servers (or mock) and measures lookup/apply_gradients.
2. Mirror data sizes (1e6 ids, dim=16) and expected outputs.
3. Optionally add profiling hooks matching Python behavior.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: bench-only; optional correctness assertions for small sizes.
- Cross-language parity test: compare outputs for small benchmark sizes.

**Gaps / Notes**
- Uses TF local servers and profiling; Rust may need a simplified harness.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distributed_ps_factory.py`
<a id="monolith-native-training-distributed-ps-factory-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 262
- Purpose/role: Factory helpers to build distributed or local multi-type hash tables and partitioned hash tables with different network/packet-reduction strategies.
- Key symbols/classes/functions: `MultiHashTableFactory`, `create_in_worker_multi_type_hash_table`, `create_multi_type_hash_table`, `create_native_multi_hash_table`, `create_in_worker_native_multi_hash_table`, `create_partitioned_hash_table`.
- External dependencies: `distributed_ps`, `distributed_ps_sync`, `hash_table_ops`, `hash_filter_ops`, `multi_type_hash_table`, `multi_hash_table_ops`, `entry.HashTableConfigInstance`.
- Side effects: none beyond table creation.

**Required Behavior (Detailed)**
- `MultiHashTableFactory`:
  - Caches converted configs via `multi_hash_table_ops.convert_to_cached_config` keyed by `id(slot_to_config)`.
  - `__call__(idx, slot_to_config)` returns `MultiHashTable.from_cached_config` using hash_filter and sync_client for shard `idx`.
- `create_in_worker_multi_type_hash_table(shard_num, slot_to_config, hash_filter, sync_client, queue_configs)`:
  - Builds a `MergedMultiTypeHashTable` whose underlying factory is `DistributedMultiTypeHashTableMpi` (alltoall) created from a per-worker `MultiTypeHashTable` factory.
- `create_multi_type_hash_table(num_ps, slot_to_config, hash_filters, sync_clients, reduce_network_packets, max_rpc_deadline_millis)`:
  - Validates sync_clients length; fills with None if missing.
  - `num_ps==0`: returns local `MergedMultiTypeHashTable` backed by `MultiTypeHashTable` and local hash tables.
  - `reduce_network_packets=False`: uses `DistributedHashTable` per slot within `MultiTypeHashTable` (dedup on worker, distribute to PS).
  - `reduce_network_packets=True`: uses `DistributedMultiTypeHashTable` (multi-type on PS) to reduce RPC count.
- `create_native_multi_hash_table(num_ps, slot_to_config, hash_filters, sync_clients, max_rpc_deadline_millis)`:
  - `num_ps==0`: returns local `MultiHashTable.from_configs`.
  - Else returns `DistributedMultiTypeHashTable` with `MultiHashTableFactory`.
- `create_in_worker_native_multi_hash_table(shard_num, slot_to_config, hash_filter, sync_client, queue_configs)`:
  - Returns `DistributedMultiTypeHashTableMpi` with a local native `MultiHashTable` per shard.
- `create_partitioned_hash_table(num_ps, use_native_multi_hash_table, max_rpc_deadline_millis, hash_filters, sync_clients, enable_gpu_emb, queue_configs)`:
  - Normalizes hash_filters/sync_clients lists.
  - Chooses `multi_type_factory` based on native vs non-native multi-hash table:
    - Native: `MultiHashTableFactory`.
    - Non-native: `MultiTypeHashTable` with hash tables created per PS.
  - Returns `distributed_ps.PartitionedHashTable` with queue configs.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/ps`.
- Rust public API surface: factory functions for hash table backends (local vs distributed).
- Data model mapping: slot configs → hash tables; shard selection logic.
- Feature gating: `reduce_network_packets`, native multi-hash table, GPU embedding.
- Integration points: used by training setup to instantiate embedding tables.

**Implementation Steps (Detailed)**
1. Implement Rust factories mirroring the three strategies (local, distributed per slot, distributed multi-type).
2. Preserve caching for expensive config conversion (if needed).
3. Ensure `num_ps==0` uses local tables and no RPC.
4. Add `PartitionedHashTable` factory with queue config propagation.

**Tests (Detailed)**
- Python tests: `distributed_ps_factory_test.py`.
- Rust tests: unit tests for factory selection logic and returned table types.
- Cross-language parity test: compare selected strategy for given flags.

**Gaps / Notes**
- Depends on TF custom ops for hash tables; Rust must provide equivalent backends.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distributed_ps_factory_test.py`
<a id="monolith-native-training-distributed-ps-factory-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 87
- Purpose/role: Smoke tests for distributed hash table factory functions; primarily checks that constructors run without errors.
- Key symbols/classes/functions: `_get_test_slot_to_config`, `_get_test_hash_filters`, `FactoryTest`.
- External dependencies: Horovod (enabled via env), TensorFlow PS cluster utilities, `test_utils.generate_test_hash_table_config`.
- Side effects: sets `MONOLITH_WITH_HOROVOD=True`; may initialize Horovod.

**Required Behavior (Detailed)**
- `_get_test_slot_to_config()`:
  - Uses `test_utils.generate_test_hash_table_config(4, learning_rate=0.1)`; returns slot map with keys `"1"`, `"2"`.
- `_get_test_hash_filters(num)`:
  - Returns `hash_filter_ops.create_hash_filters(num, False)`.
- Tests:
  - `test_create_in_worker_multi_type_hash_table*`: calls `create_in_worker_multi_type_hash_table` with hvd initialized.
  - `test_create_multi_type_hash_table_0_ps`: local (no PS) creation.
  - `test_create_multi_type_hash_table_2_ps`: creates PS cluster and calls factory under a session.
  - `test_create_multi_type_hash_table_2_ps_with_reduced_packets`: same with `reduce_network_packets=True`.
  - `test_create_native_multi_hash_table_0_ps` and `_2_ps`: native multi-hash table creation.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests`.
- Rust public API surface: factory functions in `distributed_ps_factory`.
- Data model mapping: slot configs to table instances.
- Feature gating: Horovod/PS cluster support.
- Integration points: distributed PS creation.

**Implementation Steps (Detailed)**
1. Add Rust smoke tests to ensure factory functions are callable in local/PS modes.
2. If Horovod not supported, gate tests accordingly.
3. Verify hash filter creation and config plumbing.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: `distributed_ps_factory_test.rs` (smoke only).
- Cross-language parity test: not required beyond constructor success.

**Gaps / Notes**
- These are grammar/smoke tests, not functional correctness tests.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distributed_ps_sync.py`
<a id="monolith-native-training-distributed-ps-sync-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 531
- Purpose/role: Horovod/BytePS synchronous all-to-all embedding lookup and update for distributed multi-type hash tables.
- Key symbols/classes/functions: `DistributedMultiTypeHashTableMpi.lookup`, `.apply_gradients`, `.as_op`.
- External dependencies: Horovod/BytePS env flags, `distribution_ops`, `feature_utils` (control/dense_opt ops), `prefetch_queue`.
- Side effects: uses enqueue queues for pipelined execution; emits alltoall metrics summaries when enabled.

**Required Behavior (Detailed)**
- Environment flags:
  - `MONOLITH_WITH_HOROVOD`, `MONOLITH_WITH_OPTIMIZED_HOROVOD`, `MONOLITH_WITH_BYTEPS` and related G2G/GDR flags determine alltoall backend and GPU paths.
  - `FLAGS.enable_alltoall_metrics` + `enable_alltoall_metrics_for_slot` control summary emission.
- `DistributedMultiTypeHashTableMpi.__init__(shard_num, table_factory, queue_configs)`:
  - Determines rank from BytePS or Horovod; builds local shard table via `table_factory`.
  - Stores output dims, queue configs, and dependency ops.
- `lookup(slot_to_id, auxiliary_bundle, early_reorder_indicies_res_pack)`:
  - Requires `early_reorder_indicies_res_pack` (support for `reorder_fids_in_data_pipeline=False` dropped).
  - Unpacks `(all_fids, shard_sizes, sharded_slot_sizes, emb_offset_sz, fused_embedding_offsets, req_time)`.
  - Performs alltoall on fids and per-slot sizes via BPS/HVD/custom optimized HVD.
  - Stores key tensors in `auxiliary_bundle` (id_flat_t, id_size_flat_t, emb offsets, recv splits, etc.).
  - Calls `self._table.fused_lookup(...)` on GPU, yielding `fused_embeddings`, splits, offsets, indices.
  - Performs embedding alltoall (fwd) and queues prefetch if configured.
  - Uses `distribution_ops.fused_gather_embeddings_by_input` to assemble per-slot embeddings on GPU.
  - Returns `(slot_to_embedding, auxiliary_bundle)`.
- `apply_gradients(slot_to_grad, auxiliary_bundle, global_step, req_time, scale)`:
  - Uses `feature_utils.control_ops` dependency.
  - Computes `grad_flat` via `fused_gather_embeddings_by_input_gradient`.
  - Optionally casts for BPS bwd.
  - Enqueues async optimize queue if configured.
  - Performs backward alltoall using BPS/HVD/custom optimized HVD.
  - Emits alltoall metrics summaries when enabled.
  - Calls `self._table.fused_apply_gradient` with id/grad buffers and offsets.
  - Supports async optimize queue via `AsyncPushHook`.
- `assign/assign_add/reinitialize`:
  - Not implemented (raises `NotImplementedError`).
- `as_op`:
  - Returns `self._table.as_op` with dependency ops.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/ps`.
- Rust public API surface: synchronous alltoall embedding lookup/update for multi-type tables.
- Data model mapping: packed fid buffers and fused embedding offsets.
- Feature gating: Horovod/BytePS support; GPU alltoall paths.
- Integration points: `distributed_ps_factory.create_in_worker_multi_type_hash_table`.

**Implementation Steps (Detailed)**
1. Implement Rust backend selection for alltoall (HVD/BPS equivalents) or gate feature.
2. Port `fused_lookup` + `fused_gather_embeddings_by_input` and gradient counterparts.
3. Preserve auxiliary_bundle keys and queue-based pipelining.
4. Mirror alltoall metric summaries (if logging/metrics available in Rust).

**Tests (Detailed)**
- Python tests: `distributed_ps_sync_test.py`.
- Rust tests: integration tests with small shard_num and deterministic ids.
- Cross-language parity test: compare embeddings and gradients for small fixtures.

**Gaps / Notes**
- Requires GPU kernels and alltoall comms; may need staged parity.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distributed_ps_sync_test.py`
<a id="monolith-native-training-distributed-ps-sync-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 109
- Purpose/role: Validates synchronous alltoall distributed multi-type hash table lookup and gradient updates under Horovod.
- Key symbols/classes/functions: `DistributedMultiTypeHashTableMpiTest.testBasic`, `gen_test_configs`.
- External dependencies: Horovod, `distribution_ops.fused_reorder_by_indices`, `distributed_ps_sync.DistributedMultiTypeHashTableMpi`.
- Side effects: sets `MONOLITH_WITH_HOROVOD=True` and initializes Horovod.

**Required Behavior (Detailed)**
- `gen_test_configs()`:
  - Builds two test hash table configs: slot "1" dim=1 lr=1.0; slot "2" dim=2 with PolynomialDecay LR.
- `testBasic(use_native_multi_hash_table=False)`:
  - Initializes Horovod, global_step=0.
  - Creates table with `DistributedMultiTypeHashTableMpi(hvd.size(), table_factory)`.
  - `slot_to_ids = {"1": [1,1], "2": [2]}`.
  - Uses `distribution_ops.fused_reorder_by_indices` to produce `reordred` pack (plus None timestamp).
  - First lookup returns zeros.
  - Applies gradients `{1: [[0.5],[0.5]], 2: [[0.5,1.0]]}` with `global_step=0`.
  - Second lookup returns negative values scaled by `hvd.size()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests`.
- Rust public API surface: `DistributedMultiTypeHashTableMpi` and reorder helpers.
- Data model mapping: slot configs, id arrays, gradient arrays.
- Feature gating: Horovod alltoall support.
- Integration points: `distributed_ps_sync` implementation.

**Implementation Steps (Detailed)**
1. Add Rust test that initializes the sync table and performs lookup + apply_gradients.
2. Implement fused reorder (or provide equivalent packed inputs).
3. Validate outputs match expected scaled negatives.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: `distributed_ps_sync_test.rs` with small fixed ids.
- Cross-language parity test: compare outputs for same ids and gradients.

**Gaps / Notes**
- Requires Horovod or equivalent alltoall backend; gate test if unavailable.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distributed_ps_test.py`
<a id="monolith-native-training-distributed-ps-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 979
- Purpose/role: Comprehensive tests for distributed hash tables, multi-type hash tables, export behavior, and partitioned hash table lookup/apply gradients (CPU/GPU).
- Key symbols/classes/functions: `DistributedHashTableTest`, `DistributedMultiTypeHashTableTest`, `DistributedMultiTypeHashTableServingTest`, `PartitionedHashTableTest`.
- External dependencies: TF PS clusters, Horovod env, `distribution_ops`, `export_context`, `sharding_sparse_fids_with_context`.
- Side effects: sets `MONOLITH_WITH_HOROVOD=1` and uses test clusters.

**Required Behavior (Detailed)**
- `DistributedHashTableTest`:
  - `test_basic`: assign_add then lookup equals assigned values.
  - `test_assign`: second assign overwrites ids; lookup after control dependency yields updated values.
  - `test_lookup_dedup`: duplicate ids return repeated embeddings.
  - `test_apply_gradients`: gradients with loss `2*values` updates to `-2` for dim=1.
  - `test_apply_gradients_with_learning_rate_function`: polynomial decay learning rate affects updates; after global_step increment, values change to `-4.2`.
  - `test_apply_gradients_with_duplicates`: duplicate ids produce accumulated gradient; expected `-4` for duplicate id.
  - `test_apply_gradients_with_different_ids`: bp_ids differ from ids; updates only bp ids.
- `DistributedMultiTypeHashTableTest` (param native vs non-native):
  - `testBasic`: assign_add per slot, lookup values, apply_gradients halves values.
  - `test_assign_and_reinitialize`: assign then assign with half values; native mode tests `reinitialize` status and zeros for slot.
  - `test_apply_gradients_with_learning_rate_function`: similar to single-table with polynomial decay; values update with global_step.
  - `test_apply_gradients_float16`: transfer_float16 path; verifies lookup output after apply gradients.
- `DistributedMultiTypeHashTableServingTest`:
  - `test_export_model`: export distributed/standalone/normal training and ensure lookup shapes; verifies `export_ctx.sub_graph_num`.
- `PartitionedHashTableTest`:
  - Helpers: `gen_table_config`, `gen_out_config`, `get_parser_ctx`, `gen_data`, `gen_variant_tensor`.
  - `_test_basic`: assign + assign_add and `_lookup_raw` should return sum of embeddings; runs CPU and GPU variants.
  - `_test_lookup`: assigns const embeddings, sharding sparse fids + lookup yields expected layout tensors (`bias`, `vec`, `deep`).
  - `_test_apply_gradients`: assigns const values, lookup+apply_gradients; verifies updated embeddings against expected FTRL/AdaGrad formulas.
  - `test_apply_gradients_for_gpu_emb`: compares GPU embedding path with CPU path using same gradients; outputs must match.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests`.
- Rust public API surface: distributed hash tables, multi-type tables, partitioned hash table API.
- Data model mapping: fids/embeddings, layout configs, and gradient updates.
- Feature gating: PS clusters, Horovod, GPU embedding path.
- Integration points: `distributed_ps`, `distributed_ps_sync`, `distribution_ops`.

**Implementation Steps (Detailed)**
1. Port test utilities to build PS clusters and configs in Rust (or provide Python-driven fixtures).
2. Recreate expected numeric outputs for assign/lookup/apply_gradients.
3. Implement layout config generation and sharding for partitioned hash table tests.
4. Add GPU embedding parity tests comparing CPU and GPU paths.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: `distributed_ps_test.rs` covering key cases above.
- Cross-language parity test: compare lookup/apply_gradients outputs for small fixed inputs.

**Gaps / Notes**
- File is extensive; ensure Rust tests focus on correctness for representative cases if full coverage is too costly.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distributed_serving_ops.py`
<a id="monolith-native-training-distributed-serving-ops-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 160
- Purpose/role: Remote predict and parameter sync client/server utilities for distributed serving; wraps custom ops for TF Serving RPC and sync.
- Key symbols/classes/functions: `remote_predict`, `create_parameter_sync_clients`, `parameter_sync_client_from_config`, `refresh_sync_config`, `ParameterSyncClient`, `DummySyncServer`.
- External dependencies: `gen_monolith_ops`, `parameter_sync_pb2`, `SyncBackend`, `ServerType`.
- Side effects: creates sync clients/servers on PS devices; uses RPC to remote predict.

**Required Behavior (Detailed)**
- `remote_predict(...)`:
  - Validates `model_name` non-null.
  - Calls `tf_serving_remote_predict` custom op with input/output aliases, model name, task, version, deadline, signature; returns output tensors (index 2 of op result).
- `create_parameter_sync_clients(ps_num)`:
  - For `ps_num==0`, returns single client.
  - Else creates one client per PS on PS device (unless exporting standalone).
- `parameter_sync_client_from_config(config, name_suffix)`:
  - Creates `MonolithParameterSyncClient` op with serialized config and shared_name.
- `refresh_sync_config(sync_backend, ps_index)`:
  - Fetches sync targets; populates `ClientConfig` with targets and extra info; sets model name, signature `hashtable_assign`, timeout 3000ms; returns serialized bytes.
- `create_dummy_sync_client` / `create_dummy_sync_server`:
  - Wrap dummy sync ops.
- `ParameterSyncClient`:
  - `create_sync_op` calls `monolith_parameter_sync` op with client handle and config string.
  - `as_op` wraps client handle with `tf.group`.
- `DummySyncServer`:
  - `shutdown` and `get_port` wrap dummy server ops; `as_op` groups server handle.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/serving` or `monolith-serving`.
- Rust public API surface: remote predict wrapper + parameter sync client/server wrappers.
- Data model mapping: `ClientConfig` protobuf and sync target lists.
- Feature gating: TF runtime + custom ops for remote predict and sync.
- Integration points: distributed PS and export paths.

**Implementation Steps (Detailed)**
1. Implement RPC wrapper for remote predict (TF Serving or custom stub).
2. Implement parameter sync client creation and config refresh logic.
3. Provide dummy client/server for tests.

**Tests (Detailed)**
- Python tests: `distributed_serving_ops_test.py`.
- Rust tests: integration tests for config building and dummy sync ops.
- Cross-language parity test: compare serialized config bytes.

**Gaps / Notes**
- `remote_predict` relies on custom op; Rust likely needs TF C API bindings.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distributed_serving_ops_test.py`
<a id="monolith-native-training-distributed-serving-ops-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 142
- Purpose/role: Tests parameter sync client/server ops and sync config generation for ZK backend and replica watcher.
- Key symbols/classes/functions: `ParameterSyncOpsTest`, `test_parameter_sync_client`, `test_refresh_sync_config_1`, `test_refresh_sync_config_2`.
- External dependencies: `DummySyncServer`, `ParameterSyncClient`, agent_service backend mocks, FakeKazooClient, `parameter_sync_pb2`.
- Side effects: creates dummy sync servers and ZK backend state; uses fake clients.

**Required Behavior (Detailed)**
- `test_parameter_sync_client`:
  - Creates two `DummySyncServer`s, gets ports.
  - Builds `ParameterSyncClient` with targets; creates hash table with `sync_client`.
  - Runs lookup + apply_gradients; expects embeddings `[[0.2,0.2,0.2],[0.1,0.1,0.1]]`.
  - Calls `client.create_sync_op` with config; prints JSON; shuts down servers.
- `test_refresh_sync_config_1`:
  - Mocks `ReplicaWatcher` with FakeKazooClient; sets replica meta with address `localhost:8500`.
  - `refresh_sync_config` should set `model_name='ps_1'` and targets `['localhost:8500']`.
- `test_refresh_sync_config_2`:
  - Sets up ZK backend with container services; syncs available saved models.
  - `refresh_sync_config` with ps_index 1 yields `model_name='test_ffm_model:ps_1'` and targets `['localhost:8888']`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests`.
- Rust public API surface: parameter sync client/server wrappers + config refresh.
- Data model mapping: ZK backend or equivalent; ClientConfig proto.
- Feature gating: requires sync backend mocks or fixtures.
- Integration points: distributed_serving_ops + agent_service backends.

**Implementation Steps (Detailed)**
1. Implement dummy sync server/client wrappers in Rust for test harness.
2. Add tests that apply gradients and verify embedding updates.
3. Add mock backend tests for `refresh_sync_config` with ZK-style data.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: `distributed_serving_ops_test.rs` with mocks.
- Cross-language parity test: compare ClientConfig bytes and target lists.

**Gaps / Notes**
- Depends on agent_service backends; Rust will need lightweight mocks.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distribution_ops.py`
<a id="monolith-native-training-distribution-ops-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 889
- Purpose/role: Wrapper utilities around custom distribution ops for sharding, embedding layout transforms, and gradient backprop helpers.
- Key symbols/classes/functions: `split_by_indices`, `ragged_split_by_indices`, `unique_key_with_value_and_offset`, `fill_with_offset_map`, `finalize_shared_tensor`, `reorder_by_indices`, `fused_reorder_by_indices`, `map_id_to_embedding`, `fused_embedding_to_layout` (+ grads), `map_id_to_embedding_gradient_back_prop`, `fused_gather_embeddings_by_input`, `fused_gather_embeddings_by_input_gradient`, reduce/sorted-segment ops.
- External dependencies: `gen_monolith_ops` custom kernels, `FeatureConfigs` proto.
- Side effects: registers custom gradients for several ops.

**Required Behavior (Detailed)**
- `split_by_indices(indices, tensor, num_splits)`:
  - Calls `monolith_split_by_indices` custom op; gradient registered via `monolith_split_by_indices_gradient`.
- `ragged_split_by_indices(indices, num, num_splits)`:
  - Splits ragged tensor by indices; returns list of ragged tensors + list of corresponding positions.
- `unique_key_with_value_and_offset(key, dims, generate_buffer=True)`:
  - Deduplicates ragged keys and returns `unique_key`, `value_offset` (ragged) and `value_buffer` sized by dims.
- `fill_with_offset_map(pos, value, value_offset_map, value_buffer, dims)`:
  - Fills `value_buffer` positions from offsets; gradient registered via `fill_with_offset_map_gradient`.
- `finalize_shared_tensor(shared_tensor_handles, dtype, shape)`:
  - Finalizes shared tensor handles; gradient returns upstream grad (identity).
- `reorder_by_indices` / `fused_reorder_by_indices`:
  - Reorders input ids by shard indices; `fused_reorder_by_indices` returns packed tensors and offsets for fused pipelines.
- `map_id_to_embedding(ids, embeddings, input)`:
  - Maps sharded embeddings back to original id order; gradient hook registered.
- `fused_embedding_to_layout(embeddings_list, fid_offset, feature_offset, nfl_offset, batch_size, ...)`:
  - Converts flattened embeddings into layout tensors using `FeatureConfigs` and offsets; supports multiple versions and GPU paths.
  - Gradient functions `_fused_embedding_to_layout_grad_v{1..5}` and `fused_embedding_to_layout_grad` wrap custom ops.
- `map_id_to_embedding_gradient_back_prop(ids, input, grads)`:
  - Builds gradients back to sharded embeddings.
- `gather_embeddings_by_input` / `fused_gather_embeddings_by_input`:
  - Gathers embedding vectors using ids and offsets; fused variants use offsets and sizes.
- Reduce ops:
  - `reduce_mean`, `reduce_sum`, `reduce_sqrtn` with custom gradients.
  - `fused_sorted_segment_sum`, `fused_reduce_sum_and_split`, `fused_reduce_and_split_gpu` for GPU fused reductions with gradients.
- `normalize_merged_split(row_split, size)`:
  - Normalizes row split sizes for merged ragged splits.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/ops` (or `monolith-tensor` for ragged).
- Rust public API surface: distribution ops wrappers, gradient-friendly functions if TF backend.
- Data model mapping: ragged tensors, embedding offsets, FeatureConfigs.
- Feature gating: requires custom kernel bindings or reimplementation.
- Integration points: distributed_ps, partitioned hash table, sharding pipelines.

**Implementation Steps (Detailed)**
1. Bind or reimplement each custom op with identical signatures and gradient behavior.
2. Implement ragged split/unique/offset map logic in Rust if not using TF.
3. Support fused embedding to layout and gradient versions used by PartitionedHashTable.
4. Add tests for each op with small deterministic tensors.

**Tests (Detailed)**
- Python tests: `distribution_ops_test.py`, `distribution_ops_fused_test.py`.
- Rust tests: unit tests for each op wrapper; integration tests with PartitionedHashTable.
- Cross-language parity test: compare outputs for fixed inputs and gradient checks.

**Gaps / Notes**
- Many ops are custom kernels; full parity requires substantial backend work.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distribution_ops_benchmark.py`
<a id="monolith-native-training-distribution-ops-benchmark-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 118
- Purpose/role: Benchmarks `map_id_to_embedding` and `gather_embeddings_by_input` with/without multi-threading; optional profiler output.
- Key symbols/classes/functions: `DistributionOpsBenchmarkTest.map_id_to_embedding`, `test_gather_embeddings_by_ids_basic`, `test_gather_embeddings_by_ids_multi_threads`.
- External dependencies: TensorFlow profiler, `distribution_ops`.
- Side effects: writes profiler logs under `/tmp/distribution_ops_benchmark/*`.

**Required Behavior (Detailed)**
- `map_id_to_embedding(use_multi_threads)`:
  - Creates 1e6 ids, dim=16, ps_num=10; splits ids/embeddings and maps back.
  - Asserts mapped embeddings equal original; starts/stops TF profiler in log dir.
- `test_gather_embeddings_by_ids_basic`:
  - Benchmarks gather with 100k features; runs for dim=32 and dim=256, different input lengths.
- `test_gather_embeddings_by_ids_multi_threads`:
  - Same as above but `use_multi_threads=True`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/benches`.
- Rust public API surface: bench harness for distribution ops.
- Data model mapping: ids/embeddings tensors.
- Feature gating: multi-threaded execution support.
- Integration points: `distribution_ops` implementation.

**Implementation Steps (Detailed)**
1. Implement Rust benchmarks for `map_id_to_embedding` and `gather_embeddings_by_input`.
2. Mirror tensor sizes and check correctness for small sizes; run benchmarks for large sizes.
3. Add optional profiling hooks.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: benches or microbench harness.
- Cross-language parity test: not required beyond correctness checks.

**Gaps / Notes**
- Pure benchmark; not a correctness test.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distribution_ops_fused_benchmark.py`
<a id="monolith-native-training-distribution-ops-fused-benchmark-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 61
- Purpose/role: Benchmarks `fused_reorder_by_indices` performance on large random IDs.
- Key symbols/classes/functions: `run_fused_reorder_by_indicies`.
- External dependencies: numpy, TensorFlow, `distribution_ops`.
- Side effects: none; prints average wall time.

**Required Behavior (Detailed)**
- Generates ~1e6 unique int64 IDs, 30 slots, 256 shards.
- For each slot, duplicates IDs to force duplicates and shuffles.
- Runs `distribution_ops.fused_reorder_by_indices(ids_list, num_of_shards=256)` in a session and times execution.
- Main prints average wall time over 5 runs.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/benches`.
- Rust public API surface: fused reorder benchmark.
- Data model mapping: list of id tensors, shard count.
- Feature gating: fused reorder op implementation.
- Integration points: `distribution_ops` fused reorder.

**Implementation Steps (Detailed)**
1. Implement Rust bench that generates similar random IDs and runs fused reorder.
2. Use consistent shard count and slot count for comparability.

**Tests (Detailed)**
- Python tests: this file (benchmark).
- Rust tests: bench only.
- Cross-language parity test: not required beyond output correctness.

**Gaps / Notes**
- Pure benchmark; no correctness assertions.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distribution_ops_fused_test.py`
<a id="monolith-native-training-distribution-ops-fused-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 148
- Purpose/role: Tests for `fused_reorder_by_indices` correctness and embedding offset outputs.
- Key symbols/classes/functions: `_test_fused_reorder_by_indices`, `test_fused_reorder_by_indices`, `test_ragged_tensor_workflow`.
- External dependencies: TensorFlow, `distribution_ops`.
- Side effects: none.

**Required Behavior (Detailed)**
- `test_benchmark`: runs a large random `fused_reorder_by_indices` to smoke test.
- `_test_fused_reorder_by_indices`:
  - Calls `fused_reorder_by_indices(ids_list, num_of_shards, dim_sizes)`.
  - Asserts output order, split sizes, and sharded slot sizes; optionally checks embedding offsets.
- `test_fused_reorder_by_indices`:
  - Multiple cases: single slot, extra empty slot, plus offset ids, empty slots, different shard counts, and dim_sizes for offsets.
- `test_ragged_tensor_workflow`:
  - Builds merged slot values from ragged tensors and validates fused reorder output.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests`.
- Rust public API surface: `fused_reorder_by_indices` op.
- Data model mapping: list of id tensors → reordered ids + split sizes.
- Feature gating: fused distribution ops.
- Integration points: partitioned hash table lookup pipeline.

**Implementation Steps (Detailed)**
1. Implement Rust tests mirroring each expected output case.
2. Validate embedding offsets for dim_sizes cases.
3. Include ragged workflow test for merged slots.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: `distribution_ops_fused_test.rs`.
- Cross-language parity test: compare outputs and offsets for fixed inputs.

**Gaps / Notes**
- Uses Python list inputs; Rust tests should use deterministic tensors.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distribution_ops_test.py`

<a id="monolith-native-training-distribution-ops-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 536
- Purpose/role: TensorFlow test coverage for custom distribution ops (split/reorder, ragged routing, embedding gather, reduction, fused GPU ops).
- Key symbols/classes/functions: `DistributionOpsTest` test cases.
- External dependencies: `numpy`, `tensorflow`, `tensorflow.python.framework.test_util`, `random`, `monolith.native_training.distribution_ops`.
- Side effects: Requires GPU for `@test_util.run_gpu_only` tests; uses TF v1 sessions and graph mode.

**Required Behavior (Detailed)**
- `test_split_by_indices`:
  - `ids=[0,1,2,2,3]`, `indices=ids % 3`, `split_by_indices(..., num_splits=3)` -> `[[0,3],[1],[2,2]]`.
- `test_reorder_by_indices`:
  - `ids=[0,1,2,2,3,5]`, `indices=ids % 3`, `reorder_by_indices(..., num_of_shards=3)` -> `output=[3,0,1,5,2]`, `split_sizes=[2,1,2]`.
- `test_split_by_indices_gradient`:
  - Gradient of split over `tensor=[[0,0],[1,1],[2,2]]` returns all ones.
- `test_split_by_indices_empty_gradient`:
  - Empty inputs return empty gradient `[]`.
- `test_ragged_split_by_indices`:
  - Ragged `num=[[],[],[4,3,2],[1],[],[]]`, `indices=[0,1,0,1]` -> `splits` and `pos` arrays match expected nested ragged values.
- `test_unique_key_with_value_and_offset_and_fill_with_offset_map`:
  - `unique_key_with_value_and_offset` over ragged keys returns:
    - `unique_key=[[],[0,1,2],[0,1],[]]`
    - `value_offset=[[],[[0,8],[2,6],[4]],[[10,16],[13]],[]]`
  - `fill_with_offset_map` + `finalize_shared_tensor` yields `buffer=[0,1,2,3,4,5,2,3,0,1,6,7,8,9,10,11,6,7,8]`.
  - Gradient of `buffer` wrt `value` equals `[8,10,8,10,4,5,26,28,30,13,14,15]`.
- `test_fill_with_offset_map_error_case`:
  - When `value` length too small (10 vs expected 12), evaluating `filled_tensor` raises `InvalidArgumentError`.
- `test_unique_key_with_value_and_offset_empty`:
  - Empty ragged keys -> empty `unique_key`/`value_offset`.
- `test_map_id_to_embedding`:
  - Map ids `[1]`,`[2]` to embeddings `[[1,1]]`,`[[2,2]]` and input `[[1],[2]]` -> output `[[[1,1]],[[2,2]]]`.
- `test_map_id_to_embedding_multi_threads`:
  - 1k ids, 16-dim embeddings, `ps_num=10` -> multi-threaded mapping returns exact original embeddings.
- `test_map_id_to_embedding_gradient`:
  - Loss vs target `[[2,2],[2,2],[2,2]]` yields gradients `embeddings1=[[-2,-2]]`, `embeddings2=[[-1,-1]]`.
- `test_gather_embeddings_by_ids`:
  - `ids=[1,2,3]`, `embeddings=[[1,1],[2,2],[3,3]]`, input `[[2],[1],[2]]` -> output `[[[2,2]],[[1,1]],[[2,2]]]`, `index_mapping=[[1],[0],[1]]`.
- `test_gather_embeddings_by_ids_gradient`:
  - Gradient wrt embeddings equals `[[-2,-2],[-1,-1],[0,0]]`.
- `test_gather_embeddings_by_ids_gradient_back_prop`:
  - `ids=[2,3,1]`, `grads` + `index_mapping=[1,0,1,2]` -> output `[[2,2],[5,5],[8,8]]`.
- `test_fused_gather_embeddings_by_input` (GPU only):
  - Uses fused embeddings + offsets with large SCALE; expects exact outputs per slot (repeated SCALE times).
- `test_fused_gather_embeddings_by_input_gradient` (GPU only):
  - `fused_embeddings_size=22`, `embedding_dims=[3,2]`, SCALE=888 -> output length 22 and expected sums scaled; tolerance `rtol=1e-7 * SCALE`.
- `test_reduce_mean` and `test_reduce_mean_gradient`:
  - Mean reductions produce expected values; gradients are `[-1,-1]` per row.
- `test_reduce_sum` and `test_reduce_sum_gradient`:
  - Sum reductions produce expected values; gradients are `[-1,-1]` per row.
- `test_reduce_sqrtn`, `test_reduce_sqrtn_gradient`, `test_reduce_sqrtn_gradient_zero`:
  - Sqrt-N reductions and gradients match expected numeric values; zero inputs yield zero gradients.
- `test_fused_reduce_sum_and_split`:
  - CPU-only; verifies split sizes `[2,1]` and `[1,2]` for consecutive/non-consecutive indices, with zero-filled rows for gaps.
- `test_fused_reduce_sum_and_split_grad`:
  - Gradient wrt id_values is all ones.
- `test_fused_reduce_scatter` (GPU only):
  - `fused_sorted_segment_sum` matches `scatter_nd` output and gradient across multiple shapes (includes empty tensor case).
- `test_fused_reduce_and_split_gpu` (GPU only):
  - For ragged rows and many embedding lengths, outputs match scatter+split and gradients match for all outputs.
- `test_aligned_concat_split` (GPU only):
  - Random tensors round-trip through `monolith_aligned_flat_concat`/`monolith_aligned_flat_split`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf` (TF runtime adapter) + new tests.
- Rust public API surface: wrappers in `monolith-rs/crates/monolith-tf/src/distribution_ops.rs` (new) and tests in `monolith-rs/crates/monolith-tf/tests/distribution_ops_test.rs` and `monolith-rs/crates/monolith-tf/tests/distribution_ops_gpu_test.rs`.
- Data model mapping: TF tensors (dense and ragged), custom op handles, gradient support for TF runtime.
- Feature gating: `tf-runtime` feature for these tests; GPU-only tests gated on CUDA availability.
- Integration points: custom op library load (libmonolith_ops) before creating the TF graph/session.

**Implementation Steps (Detailed)**
1. Add TF runtime harness in Rust: build graph/session, load `libmonolith_ops`, wrap op invocation.
2. Implement CPU tests for `split_by_indices`, `reorder_by_indices`, `ragged_split_by_indices`, `unique_key_with_value_and_offset`, `fill_with_offset_map`, `finalize_shared_tensor`, `map_id_to_embedding`, `gather_embeddings_by_input`, and `reduce_*` ops.
3. Add gradient checks using TF gradient API; if Rust bindings do not expose gradients, run parity via Python harness and document skip in Rust.
4. Add GPU-only tests for fused gather, fused reduce scatter, fused reduce+split GPU, and aligned concat/split; skip when CUDA/custom ops missing.
5. Seed RNG or replace random tensors with deterministic values to avoid flakiness (especially `test_aligned_concat_split`).
6. Validate error handling for `fill_with_offset_map` with invalid input sizes (InvalidArgumentError).
7. Document Candle backend deviations: these ops require TF custom kernels and are only supported under the TF runtime feature.

**Tests (Detailed)**
- Python tests: `monolith/native_training/distribution_ops_test.py`.
- Rust tests: `monolith-rs/crates/monolith-tf/tests/distribution_ops_test.rs` (CPU) and `monolith-rs/crates/monolith-tf/tests/distribution_ops_gpu_test.rs` (GPU).
- Cross-language parity test: run Python and Rust TF tests on identical inputs; compare tensors within tolerance and verify gradients.

**Gaps / Notes**
- Requires TF custom ops build + dynamic loading; without this, Rust tests must be skipped.
- GPU tests are sensitive to CUDA availability and may need CI skips.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/distribution_utils.py`
<a id="monolith-native-training-distribution-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 443
- Purpose/role: BytePS/Horovod initialization, MPI helpers, sync training config updates, GPU session config tweaks, and BytePS micro-benchmarks.
- Key symbols/classes/functions: `bps_init`, `byteps_benchmark_ar`, `byteps_benchmark_a2a`, `bps_comm_benchmark`, `init_sync_train_and_update_conf`, `get_mpi_rank`, `get_mpi_local_rank`, `get_mpi_size`, `get_mpi_local_size`, `enable_sync_training`, `try_init_cuda`, `get_device_str`, `get_sync_run_hooks`, `update_session_config_for_gpu`.
- External dependencies: `absl.flags`, `absl.logging`, `tensorflow`, `byteps.tensorflow` (optional), `horovod.tensorflow` (optional), `monolith.native_training.metric.metric_hook.ByteCCLTelemetryHook`.
- Side effects: Sets many env vars, creates `/tmp/bps_<uuid>_socket_<id>` dir, runs `ip addr show` via shell, enables eager execution in benchmark funcs, initializes BytePS/Horovod, mutates config object fields.

**Required Behavior (Detailed)**
- Global state:
  - `_SYNC_TRAIN_INITED` gate to avoid repeated init.
  - `enable_bps = int(os.getenv("MONOLITH_WITH_BYTEPS", "0"))` evaluated at import time.
- `bps_init(uuid)`:
  - Ensures `BYTEPS_ALLTOALL_SESSION_SIZE` default `3`.
  - Mirrors `OMPI_COMM_WORLD_*` into `BYTEPS_LOCAL_SIZE`, uses `BYTEPS_LOCAL_SIZE` to compute `local_rank` and `phy_node_id`.
  - Computes `socket_path = /tmp/bps_<uuid>_socket_<phy_node_id>` and creates it.
  - Chooses network interface:
    - If `BYTEPS_GPU_NIC_BINDING_MODE=0`, uses `DMLC_INTERFACE` (default `eth0`).
    - Else, binds NIC by GPU index (`NUM_GPU_PER_NIC=2`), sets `CUDA_VISIBLE_DEVICES` and UCX/GDR envs when `MONOLITH_WITH_BYTEPS_FWD_GDR` or `MONOLITH_WITH_BYTEPS_BWD_GDR` is enabled.
    - If `BYTEPS_WITH_ALL_NICS=1`, sets `UCX_NET_DEVICES` to a list of mlx5 + eth; else only `mlx5_<nic_id>:1`.
  - Runs `ip addr show <interface>` to compute host IP; exports as `UCX_RDMA_CM_SOURCE_ADDRESS` and `DMLC_NODE_HOST`.
  - Sets required BytePS/PSLite env vars (role, worker/server counts, UUID, ranks, telemetry, log levels, perf knobs, partition sizes).
  - Ensures `BYTEPS_P2P_PARTITION_BYTES` and `BYTEPS_PARTITION_BYTES` defaults computed from `size`.
  - Imports `byteps.tensorflow` and calls `bps.init(lazy=False)`.
- `byteps_benchmark_ar(total_len, total_niter=10000, use_cpu=False, op='pushpull')`:
  - Enables eager execution; uses `bps.push_pull` by default.
  - Creates tensor of shape `[total_len, 1]` on CPU/GPU.
  - Runs `total_niter` iterations, logs latency/Goodput every 20 iterations, returns `goodputs[1:]`.
- `byteps_benchmark_a2a(total_len, total_niter=10000, dst_gpu=True, src_gpu=True)`:
  - Enables eager execution; if CPU-only (`dst_gpu=False` and `src_gpu=False`) reduces `total_len` by 8.
  - Builds splits and recv_splits; selects correct BytePS alltoall variant (`alltoall`, `alltoall_cpu2gpu`, `alltoall_gpu2cpu`).
  - Runs loop and returns `goodputs[1:]`.
- `bps_comm_benchmark()`:
  - Reads `MONOLITH_BENCHMARK_BPS`, `MONOLITH_BENCHMARK_ITERS`, and length env vars; sets TF memory growth for all GPUs.
  - Runs selected benchmarks and prints summary tuples `(total_len, avg_goodput)`.
- `init_sync_train_and_update_conf(dct_config)`:
  - Logs entry; imports BytePS or Horovod as needed; initializes once.
  - If not `merge_sync_training_ckpt`, updates `dct_config.model_dir` with `index-<rank>` suffix under `model_dir/uuid/`.
  - Sets `num_ps=0`, `reorder_fids_in_data_pipeline=True`, `index=hvd.rank()`, `num_workers=hvd.size()`, `enable_variable_partition=False`.
  - Catches ImportError/NotFoundError and logs warning.
- MPI helpers:
  - `get_mpi_rank/local_rank/size/local_size` pull from `OMPI_COMM_WORLD_*` envs; warn and use defaults (0/1) when missing.
- `enable_sync_training()`:
  - Returns `FLAGS.enable_sync_training and 'OMPI_COMM_WORLD_LOCAL_RANK' in os.environ`; returns False on exception.
- `try_init_cuda()`:
  - If `CUDA_VISIBLE_DEVICES` not set but MPI local rank present, set `CUDA_DEVICE_ORDER=PCI_BUS_ID` and `CUDA_VISIBLE_DEVICES=<local_rank>`.
  - If sync training enabled and not initialized, tries to import BytePS or Horovod (based on `MONOLITH_WITH_BYTEPS`/`MONOLITH_WITH_HOROVOD`) and `hvd.init()`; logs exceptions.
- `get_device_str(force_on_cpu=False)`:
  - Uses `FLAGS.enable_gpu_training` or `device_utils._GPU_PLACEMENT_ALLOWED` to choose GPU vs CPU.
  - For MPI + sync training:
    - In PS mode (`FLAGS.num_ps > 0`): returns `/job:chief` for rank 0 else `/job:worker` with `task` offsets and `/device:{GPU|CPU}:0`.
    - Without PS mode: returns empty string.
  - Otherwise returns `/device:{GPU|CPU}:0`.
- `get_sync_run_hooks(is_full_sync=False)`:
  - Returns empty list when not in sync mode.
  - Uses BytePS `BroadcastGlobalVariablesHook` when `MONOLITH_WITH_BYTEPS` and `MONOLITH_WITH_BYTEPS_BCAST` are set.
  - If `MONOLITH_WITH_BYTEPS_BCAST == -1`, returns empty list.
  - Adds `ByteCCLTelemetryHook(50)` when `is_full_sync` and using BytePS broadcast.
  - Falls back to Horovod `BroadcastGlobalVariablesHook` when not using BytePS.
- `update_session_config_for_gpu(session_config)`:
  - When sync training is enabled, sets `gpu_options.visible_device_list` to local rank.
  - If `MONOLITH_FORCE_GPU_COMPATIBLE=1`, sets `force_gpu_compatible=True`.
  - If BytePS GDR alltoall enabled (`MONOLITH_WITH_BYTEPS_FWD_GDR` or `MONOLITH_WITH_BYTEPS_BWD_GDR`), disables `allow_growth`, sets `per_process_gpu_memory_fraction=0.4` and visible device list to local rank.
  - Otherwise enables `allow_growth`.
  - When not in sync training, still sets `allow_growth=True`.

**Rust Mapping (Detailed)**
- Target crate/module: new `monolith-rs/crates/monolith-training/src/distribution_utils.rs` (sync training env + device helpers) and `monolith-rs/crates/monolith-training/src/distributed.rs` for MPI helpers.
- Rust public API surface: `init_sync_train_and_update_conf`, `get_mpi_*`, `enable_sync_training`, `try_init_cuda`, `get_device_str`, `get_sync_run_hooks`, `update_session_config_for_gpu` equivalents; optional TF-specific BytePS/Horovod bridge behind feature flags.
- Data model mapping: map Python `dct_config` mutation to Rust config struct (likely in `monolith-training` or `monolith-cli`).
- Feature gating: `tf-runtime` (BytePS/Horovod) and `cuda` (GPU-specific paths); default Candle backend should no-op or provide safe fallbacks.
- Integration points: `monolith/native_training/estimator.py`, `cpu_training.py`, `device_utils.py`, `model_export/saved_model_exporters.py`, and `data/datasets.py` equivalents in Rust.

**Implementation Steps (Detailed)**
1. Define a Rust config struct that mirrors `dct_config` fields used here (`uuid`, `model_dir`, `merge_sync_training_ckpt`, `num_ps`, `reorder_fids_in_data_pipeline`, `index`, `num_workers`, `enable_variable_partition`).
2. Implement env parsing for MPI (`OMPI_COMM_WORLD_*`) and BytePS/Horovod gating (`MONOLITH_WITH_BYTEPS`, `MONOLITH_WITH_HOROVOD`).
3. Add `get_device_str` logic to Rust; plumb in `enable_gpu_training` and `num_ps` flags (from CLI or config).
4. For TF runtime, implement BytePS/Horovod initialization and broadcast hooks; otherwise return empty hooks and log warnings.
5. Implement `try_init_cuda` that sets env vars before GPU runtime init (Rust side); keep `_SYNC_TRAIN_INITED` equivalent.
6. Implement `update_session_config_for_gpu` only when using TF sessions; in Candle backend, document as no-op.
7. Port benchmark helpers only if TF BytePS runtime is supported; otherwise document as unsupported.

**Tests (Detailed)**
- Python tests: none in-tree specific to this file.
- Rust tests: add unit tests for env parsing and `get_device_str` permutations; integration tests for config updates and no-op behavior when BytePS/Horovod missing.
- Cross-language parity test: compare outputs of MPI helpers and device string formatting for a matrix of env/flag combinations.

**Gaps / Notes**
- Uses shell command `ip addr show` to resolve interface IP; Rust port should use OS APIs or run the command for parity.
- Heavy BytePS/Horovod coupling means full parity likely only under TF runtime with custom ops.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/embedding_combiners.py`
<a id="monolith-native-training-embedding-combiners-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 102
- Purpose/role: Defines embedding combiner strategies for ragged inputs (sum/mean pooling and FirstN sequence padding).
- Key symbols/classes/functions: `Combiner`, `ReduceSum`, `ReduceMean`, `FirstN`.
- External dependencies: `tensorflow`, `distribution_ops`, `ragged_utils`, `device_utils`.
- Side effects: None beyond device placement in `FirstN.combine`.

**Required Behavior (Detailed)**
- `Combiner`:
  - Stores `max_seq_length` and exposes `combine(...)` abstract method.
- `ReduceSum.combine(key, embedding, name=None)`:
  - Uses `ragged_utils.fused_value_rowids(key)` to map values to row ids.
  - Calls `distribution_ops.reduce_sum(expand_dims(rowids), embedding, expand_dims(key.nrows(), 0), name=name)`.
- `ReduceMean.combine(key, embedding, name=None)`:
  - Same as `ReduceSum` but calls `distribution_ops.reduce_mean`.
- `FirstN.__init__(seq_length)`:
  - Asserts `seq_length > 0`, sets `max_seq_length` to `seq_length`.
- `FirstN.combine(key, embedding, name=None)`:
  - If `embedding` is not a `tf.Tensor`, converts it.
  - Computes `batch_size_tensor = key.nrows()`.
  - Converts `key` to sparse (`key_sparse = key.to_sparse()`), uses `key_sparse.indices` to scatter.
  - Builds `shape = [batch_size, max(max_seq_length, key_sparse.dense_shape[1]), embedding_dim]` with `embedding.shape.as_list()[1]`.
  - Under `device_utils.maybe_device_if_allowed('/device:GPU:0')`, calls `tf.scatter_nd(indices, embedding, shape)`.
  - Returns `tf.slice(scattered, [0,0,0], [-1, max_seq_length, -1])` to enforce sequence length.
  - Rows with fewer embeddings are zero-padded by scatter.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/src/embedding.rs` and/or new `monolith-rs/crates/monolith-layers/src/combiner.rs`.
- Rust public API surface:
  - `Combiner` trait with `combine(key, embedding) -> Tensor`.
  - `ReduceSum` and `ReduceMean` pooling for ragged sequences.
  - `FirstN` equivalent using `SequenceEmbeddingLookup` or a new combiner wrapper.
- Data model mapping: Ragged input represented as `(values, row_lengths)` or `(values, row_splits)`; must map to pooled or padded tensors.
- Feature gating: TF runtime path can call distribution_ops; Candle backend should implement native pooling and padding.
- Integration points: use in `feature.py`/`feature_utils.py` equivalents and embedding table lookup paths.

**Implementation Steps (Detailed)**
1. Define a Rust `Combiner` trait and enums for `ReduceSum`, `ReduceMean`, `FirstN`.
2. Implement pooling for ragged sequences using row lengths (sum/mean) with deterministic order.
3. Implement `FirstN` by zero-padding to `[batch, max_seq_length, dim]` and truncating when longer.
4. Preserve shape inference behavior: unknown batch size => dynamic dimension, but known `max_seq_length` and embedding dim.
5. If TF runtime is enabled, optionally route to distribution_ops to match TF kernels exactly.
6. Add device placement logic for GPU (if supported) and document when CPU is forced.

**Tests (Detailed)**
- Python tests: `monolith/native_training/embedding_combiners_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/embedding_combiners_test.rs` (new) or extend `monolith-layers/src/embedding.rs` tests.
- Cross-language parity test: compare pooled and padded outputs for the same ragged inputs and ensure shape inference matches.

**Gaps / Notes**
- Python uses `ragged_utils.fused_value_rowids` and custom reduce ops; Rust must replicate row-id logic exactly.
- `FirstN` uses `scatter_nd` behavior; ensure zero-fill for missing entries and correct truncation.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/embedding_combiners_test.py`
<a id="monolith-native-training-embedding-combiners-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 47
- Purpose/role: Validates `ReduceSum` and `FirstN` combiners, including unknown shape handling.
- Key symbols/classes/functions: `CombinerTest` test cases.
- External dependencies: `tensorflow`, `embedding_combiners`.
- Side effects: Uses TF v1 graph mode when run as main.

**Required Behavior (Detailed)**
- `testReduceSum`:
  - `key = RaggedTensor.from_row_lengths([1,2,3], [1,2])` and `emb=[[1.0],[2.0],[3.0]]`.
  - `ReduceSum.combine` returns `[[1.0],[5.0]]`.
- `testFirstN`:
  - `key = RaggedTensor.from_row_lengths([1,2,3,4,5,6], [1,2,3])`, `emb` 6x1.
  - `FirstN(2)` returns `[[[1.0],[0.0]], [[2.0],[3.0]], [[4.0],[5.0]]]` (zero-padded for row 0).
- `testFirstNUnknownShape`:
  - `key` is ragged placeholder, `emb` placeholder `[None,6]`.
  - `FirstN(2)` result shape is `[None, 2, 6]`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/embedding_combiners_test.rs` (new) or `monolith-rs/crates/monolith-layers/src/embedding.rs` unit tests.
- Rust public API surface: `ReduceSum`, `FirstN` combiners or equivalent pooling + sequence embedding logic.
- Data model mapping: ragged input modeled as `(values, row_lengths)`; tests should construct identical ragged cases.
- Feature gating: none for Candle backend; TF runtime tests optional.
- Integration points: `embedding_combiners` module or embedded in `embedding` layers.

**Implementation Steps (Detailed)**
1. Add Rust tests mirroring the three cases above.
2. Ensure `FirstN` produces zero-padded outputs for short rows.
3. Verify output shape inference for unknown batch size, but fixed `max_seq_length` and embedding dim.

**Tests (Detailed)**
- Python tests: `monolith/native_training/embedding_combiners_test.py`.
- Rust tests: add parity tests in `monolith-rs/crates/monolith-layers/tests/embedding_combiners_test.rs`.
- Cross-language parity test: compare outputs for the same ragged inputs.

**Gaps / Notes**
- Python uses ragged tensors; Rust tests must define an equivalent ragged representation.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/entry.py`
<a id="monolith-native-training-entry-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 630
- Purpose/role: Defines optimizer/initializer/compressor wrappers and hash table config helpers that emit `embedding_hash_table_pb2` proto configs for Monolith hash tables.
- Key symbols/classes/functions: `Optimizer`, `SgdOptimizer`, `AdagradOptimizer`, `AdadeltaOptimizer`, `AdamOptimizer`, `AmsgradOptimizer`, `BatchSoftmaxOptimizer`, `MomentumOptimizer`, `MovingAverageOptimizer`, `RmspropOptimizer`, `RmspropV2Optimizer`, `FTRLWithGroupSparsityOptimizer`, `AdaGradWithGroupLassoOptimizer`, `DynamicWdAdagradOptimizer`, `FtrlOptimizer`, `Initializer`, `ZerosInitializer`, `ConstantsInitializer`, `RandomUniformInitializer`, `BatchSoftmaxInitializer`, `Compressor`, `OneBitCompressor`, `FixedR8Compressor`, `Fp16Compressor`, `Fp32Compressor`, `CombineAsSegment`, `HashTableConfig`, `CuckooHashTableConfig`, `HashTableConfigInstance`.
- External dependencies: `tensorflow`, `monolith_export`, `embedding_hash_table_pb2` (package `monolith.hash_table`).
- Side effects: None (pure config assembly), except for exceptions in constructors and learning-rate helpers.

**Required Behavior (Detailed)**
- `_convert_to_proto(obj, proto)`:
  - Calls `proto.SetInParent()`.
  - Iterates `obj.__dict__` and assigns any non-`None` field values to the proto fields with the same name.
- `Optimizer` (abstract): `as_proto()` returns `embedding_hash_table_pb2.OptimizerConfig`.
- `StochasticRoundingFloat16OptimizerWrapper(optimizer)`:
  - Wraps any optimizer; `as_proto()` calls inner optimizer then sets `stochastic_rounding_float16 = True` on the returned config.
- Optimizers (all call `_convert_to_proto` on their respective `OptimizerConfig` sub-message):
  - `SgdOptimizer(learning_rate=None)` -> `opt.sgd`.
  - `AdagradOptimizer(learning_rate=None, initial_accumulator_value=None, hessian_compression_times=1, warmup_steps=0, weight_decay_factor=0.0)` -> `opt.adagrad`.
  - `AdadeltaOptimizer(learning_rate=None, weight_decay_factor=0.0, averaging_ratio=0.9, epsilon=0.01, warmup_steps=0)` -> `opt.adadelta`.
  - `AdamOptimizer(learning_rate=None, beta1=0.9, beta2=0.99, use_beta1_warmup=False, weight_decay_factor=0.0, use_nesterov=False, epsilon=0.01, warmup_steps=0)` -> `opt.adam`.
  - `AmsgradOptimizer(learning_rate=None, beta1=0.9, beta2=0.99, weight_decay_factor=0.0, use_nesterov=False, epsilon=0.01, warmup_steps=0)` -> `opt.amsgrad` (not `monolith_export`).
  - `BatchSoftmaxOptimizer(learning_rate=None)` -> `opt.batch_softmax`.
  - `MomentumOptimizer(learning_rate=None, weight_decay_factor=0.0, use_nesterov=False, momentum=0.9, warmup_steps=0)` -> `opt.momentum`.
  - `MovingAverageOptimizer(momentum=0.9)` -> `opt.moving_average` (not `monolith_export`).
  - `RmspropOptimizer(learning_rate=None, weight_decay_factor=0.0, momentum=0.9)` -> `opt.rmsprop`.
  - `RmspropV2Optimizer(learning_rate=None, weight_decay_factor=0.0, momentum=0.9)` -> `opt.rmspropv2`.
  - `FTRLWithGroupSparsityOptimizer(learning_rate=None, initial_accumulator_value=None, beta=None, warmup_steps=0, l1_regularization=None, l2_regularization=None)` -> `opt.group_ftrl` with `l1_regularization_strength` and `l2_regularization_strength` fields set.
  - `AdaGradWithGroupLassoOptimizer(learning_rate=None, beta=None, initial_accumulator_value=None, l2_regularization=None, weight_decay_factor=0.0, warmup_steps=0)` -> `opt.group_adagrad` with `l2_regularization_strength` set.
  - `DynamicWdAdagradOptimizer(learning_rate=None, initial_accumulator_value=None, hessian_compression_times=1, warmup_steps=0, weight_decay_factor=0.0, decouple_weight_decay=True, enable_dynamic_wd=True, flip_direction=True, dynamic_wd_temperature=1.0)` -> `opt.dynamic_wd_adagrad`.
  - `FtrlOptimizer(learning_rate=None, initial_accumulator_value=None, beta=None, warmup_steps=0, l1_regularization=None, l2_regularization=None)` -> `opt.ftrl` with `l1_regularization_strength` and `l2_regularization_strength` fields set.
- `Initializer` (abstract): `as_proto()` returns `embedding_hash_table_pb2.InitializerConfig`.
  - `ZerosInitializer()` -> `init.zeros`.
  - `ConstantsInitializer(constant)` -> `init.constants` with `constant` set.
  - `RandomUniformInitializer(minval=None, maxval=None)` -> `init.random_uniform`.
  - `BatchSoftmaxInitializer(init_step_interval)`:
    - Raises `ValueError` if `init_step_interval < 1`.
    - Stores `constant = init_step_interval` and returns `init.constants`.
- `Compressor` (abstract): `as_proto()` returns `embedding_hash_table_pb2.FloatCompressorConfig`.
  - `OneBitCompressor(step_size=200, amplitude=0.05)` -> `comp.one_bit` with `step_size` and `amplitude`.
  - `FixedR8Compressor(fixed_range=1.0)` -> `comp.fixed_r8` with `r` field set.
  - `Fp16Compressor()` -> `comp.fp16`.
  - `Fp32Compressor()` -> `comp.fp32`.
- `CombineAsSegment(dim_size, initializer, optimizer, compressor)`:
  - Accepts either wrapper objects or raw proto configs.
  - Creates `EntryConfig.Segment`, sets `dim_size`, and `CopyFrom` for init/opt/comp configs.
- `HashTableConfig` (abstract): `mutate_table(table_config)`.
- `CuckooHashTableConfig(initial_capacity=1, feature_evict_every_n_hours=0)`:
  - `mutate_table` sets `table_config.initial_capacity` and `table_config.cuckoo.SetInParent()`.
  - If `feature_evict_every_n_hours > 0`, sets `enable_feature_eviction=True` and `feature_evict_every_n_hours`.
- `HashTableConfigInstance(table_config, learning_rate_fns, extra_restore_names=None)`:
  - Stores a copy of `extra_restore_names` (default `[]`).
  - `__str__` returns `TableConfigPB:<serialized>, LearningRateFns:[<fn_strs>]` where proto is `SerializeToString()` and each fn uses `str(fn)`.
  - `call_learning_rate_fns()`:
    - Under name scope `learning_rate`, calls each fn if callable, else casts to `tf.float32`.
    - Returns `tf.stack(learning_rates)`; raises `Exception` if list is empty.
  - `call_learning_rate_fns_fewer_ops()`:
    - Same call rules but returns raw list (no `tf.cast` for non-callables) and raises if empty.
  - `set_learning_rate_tensor()` stores computed tensor; `learning_rate_tensor` property exposes it.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-hash-table/src` plus proto types in `monolith-rs/crates/monolith-proto` (`monolith::hash_table::*`).
- Rust public API surface:
  - Builder structs mirroring the optimizer/initializer/compressor wrappers (e.g., `SgdOptimizerConfig`, `AdamOptimizerConfig`, `RandomUniformInitializerConfig`, `OneBitCompressorConfig`).
  - `CombineAsSegment` equivalent that produces `monolith::hash_table::EntryConfig::Segment`.
  - `HashTableConfig` trait and `CuckooHashTableConfig` implementation.
  - `HashTableConfigInstance` struct holding a `EmbeddingHashTableConfig`, learning-rate fn list, and extra restore names.
- Data model mapping: Use `monolith::hash_table::OptimizerConfig`, `InitializerConfig`, `FloatCompressorConfig`, `EmbeddingHashTableConfig` from `monolith-proto`.
- Feature gating: none for Candle backend; TF runtime should reuse the same proto configs.
- Integration points: `feature.py`, `hash_table_ops.py`, `multi_hash_table_ops.py`, and `cpu_training.py` equivalents.

**Implementation Steps (Detailed)**
1. Add Rust config builder types that mirror field names and defaults from Python (including `None`-skip semantics).
2. Implement a `_convert_to_proto` equivalent that only sets fields when they are `Some(...)`.
3. Implement `StochasticRoundingFloat16OptimizerWrapper` as a decorator that toggles `stochastic_rounding_float16` on `OptimizerConfig`.
4. Implement `CombineAsSegment` with enum inputs to accept either builder or direct proto.
5. Port `HashTableConfigInstance.__str__` to a deterministic `Display` implementation using serialized proto bytes + fn string signatures.
6. Implement `call_learning_rate_fns` and `call_learning_rate_fns_fewer_ops` using Candle/Tensor APIs; preserve error messages when list is empty.
7. Add unit tests for each builder and for `CombineAsSegment` output.

**Tests (Detailed)**
- Python tests: `monolith/native_training/entry_test.py`.
- Rust tests: `monolith-rs/crates/monolith-hash-table/tests/entry_test.rs` (new) to mirror optimizer/initializer/compressor config creation and `HashTableConfigInstance.__str__` behavior.
- Cross-language parity test: compare serialized proto bytes produced by Python and Rust for each optimizer/initializer/compressor config.

**Gaps / Notes**
- Proto fields must match Python names exactly; default handling must skip `None` to avoid overwriting proto defaults.
- `HashTableConfigInstance.__str__` depends on `SerializeToString()` ordering; ensure Rust uses the same proto serialization (protobuf binary) for parity.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/entry_test.py`
<a id="monolith-native-training-entry-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 84
- Purpose/role: Smoke tests for optimizer/initializer/compressor config builders and `HashTableConfigInstance` string equality.
- Key symbols/classes/functions: `EntryTest` test cases.
- External dependencies: `entry`, `learning_rate_functions`, `embedding_hash_table_pb2`.
- Side effects: None.

**Required Behavior (Detailed)**
- `test_optimizers`:
  - Instantiates each optimizer class and calls `as_proto()` without error.
  - Covers: `SgdOptimizer`, `AdagradOptimizer`, `FtrlOptimizer`, `DynamicWdAdagradOptimizer`, `AdadeltaOptimizer`, `AdamOptimizer`, `AmsgradOptimizer`, `MomentumOptimizer`, `MovingAverageOptimizer`, `RmspropOptimizer`, `RmspropV2Optimizer`, `BatchSoftmaxOptimizer`.
- `test_initializer`:
  - Calls `as_proto()` for `ZerosInitializer`, `RandomUniformInitializer(-0.5,0.5)`, `BatchSoftmaxInitializer(1.0)`.
- `test_compressor`:
  - Calls `as_proto()` for `Fp16Compressor`, `Fp32Compressor`, `FixedR8Compressor`, `OneBitCompressor`.
- `test_combine`:
  - Calls `CombineAsSegment(5, ZerosInitializer(), SgdOptimizer(), Fp16Compressor())` and expects no error.
- `test_hashtable_config`:
  - Instantiates `CuckooHashTableConfig`.
- `test_hashtable_config_entrance`:
  - Creates `EmbeddingHashTableConfig` instances and `HashTableConfigInstance` wrappers.
  - Validates `str(config1) == str(config2)` for same numeric learning rate.
  - Validates `str(config3) == str(config4)` for same callable learning-rate function.
  - Validates `str(config1) != str(config3)`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-hash-table/tests/entry_test.rs` (new).
- Rust public API surface: config builder types mirroring the Python constructors and `as_proto` equivalents.
- Data model mapping: `monolith::hash_table::EmbeddingHashTableConfig` from `monolith-proto`.
- Feature gating: none.
- Integration points: `HashTableConfigInstance` display and equality semantics.

**Implementation Steps (Detailed)**
1. Add Rust tests that call each builder and assert proto creation succeeds.
2. Verify `CombineAsSegment` builds a segment with `dim_size=5` and correct config types.
3. Implement `HashTableConfigInstance` string or equality behavior to match Python `__str__` semantics.
4. Add test cases that compare string outputs for numeric vs callable learning rate functions.

**Tests (Detailed)**
- Python tests: `monolith/native_training/entry_test.py`.
- Rust tests: `monolith-rs/crates/monolith-hash-table/tests/entry_test.rs`.
- Cross-language parity test: compare serialized proto bytes and `__str__` outputs for the same configs.

**Gaps / Notes**
- Python uses `learning_rate_functions.PolynomialDecay` for callable learning-rate fns; Rust needs an equivalent or a stub to produce deterministic string output.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/env_utils.py`
<a id="monolith-native-training-env-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 32
- Purpose/role: Minimal environment utility stubs for HDFS setup, UUID->PSM conversion, and ZooKeeper auth data.
- Key symbols/classes/functions: `setup_hdfs_env`, `generate_psm_from_uuid`, `get_zk_auth_data`.
- External dependencies: `os`, `absl.logging` (unused), `contextlib`, `hashlib`, `subprocess`, `socket` (unused).
- Side effects: `get_zk_auth_data` prints `ZK_AUTH` to stdout when set.

**Required Behavior (Detailed)**
- `setup_hdfs_env()`:
  - Currently a no-op (`pass`).
- `generate_psm_from_uuid(s)`:
  - Returns the input string unchanged.
- `get_zk_auth_data()`:
  - Reads `ZK_AUTH` env var.
  - If set, prints `"ZK_AUTH <value>"` and returns `[('digest', ZK_AUTH)]`.
  - If unset, returns `None`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/env_utils.rs` (new) or `monolith-rs/crates/monolith-core/src/env_utils.rs`.
- Rust public API surface: `setup_hdfs_env`, `generate_psm_from_uuid`, `get_zk_auth_data` equivalents.
- Data model mapping: `get_zk_auth_data` returns `Option<Vec<(String, String)>>` or a domain-specific auth struct.
- Feature gating: none; functions are pure env utilities.
- Integration points: any ZooKeeper or HDFS setup codepaths in Rust equivalents.

**Implementation Steps (Detailed)**
1. Implement `setup_hdfs_env` as a no-op in Rust until actual HDFS setup logic is defined.
2. Implement `generate_psm_from_uuid` as identity function.
3. Implement `get_zk_auth_data` to read `ZK_AUTH` and return digest tuple; log/print similarly.
4. Add tests to verify behavior with `ZK_AUTH` set/unset.

**Tests (Detailed)**
- Python tests: none (see `env_utils_test.py`, empty).
- Rust tests: add unit tests in `monolith-rs/crates/monolith-training/tests/env_utils_test.rs`.
- Cross-language parity test: set `ZK_AUTH` env and compare return value.

**Gaps / Notes**
- Many imports are unused; indicates incomplete implementation in Python.
- If upstream has a richer implementation, this file should be revisited for parity.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/env_utils_test.py`
<a id="monolith-native-training-env-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 23
- Purpose/role: Placeholder test file; no tests implemented.
- Key symbols/classes/functions: None.
- External dependencies: `os`, `unittest`, `mock`, `env_utils` (unused).
- Side effects: None.

**Required Behavior (Detailed)**
- No runtime behavior; file only defines imports and `unittest.main()` when run as main.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/env_utils_test.rs` (new).
- Rust public API surface: tests for `env_utils` functions.
- Feature gating: none.
- Integration points: ensure env parsing helpers are covered by unit tests.

**Implementation Steps (Detailed)**
1. Implement Rust tests for `get_zk_auth_data` with and without `ZK_AUTH`.
2. Add a smoke test for `generate_psm_from_uuid` identity behavior.
3. Keep `setup_hdfs_env` as no-op test (ensures no panic).

**Tests (Detailed)**
- Python tests: none.
- Rust tests: `monolith-rs/crates/monolith-training/tests/env_utils_test.rs`.
- Cross-language parity test: not required beyond env value checks.

**Gaps / Notes**
- This file has no actual tests; Rust should still cover behavior to keep parity validated.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/estimator.py`
<a id="monolith-native-training-estimator-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 667
- Purpose/role: High-level Estimator API for local/distributed training, evaluation, prediction, and saved_model export/import.
- Key symbols/classes/functions: `EstimatorSpec`, `RunConfig`, `Estimator`, `import_saved_model`.
- External dependencies: TensorFlow Estimator, Kazoo/ZK, AgentService, CpuTraining, RunnerConfig, service discovery, DumpUtils, device_utils, distribution_utils.
- Side effects: mutates env vars, initializes ZK clients/backends, logs metrics, writes model dumps, may start/stop distributed services.

**Required Behavior (Detailed)**
- `EstimatorSpec` (namedtuple):
  - Fields: `label`, `pred`, `head_name`, `loss`, `optimizer`, `classification`.
  - `__new__` sets defaults `head_name=None`, `loss=None`, `optimizer=None`, `classification=True`.
  - `_replace` forbids changing `mode` if present in kwargs and different from existing (defensive check).
- `RunConfig` (dataclass_json):
  - Fields include: `is_local`, `num_ps`, `num_workers`, timeout settings, layout flags, retry settings, parameter-sync fields, checkpoint/export settings, profiling flags, alias map settings, kafka settings, metrics flags, summary/log cadence.
  - `to_runner_config()`:
    - Builds `RunnerConfig` with key fields.
    - For each RunConfig field, if value differs from RunConfig default and differs from current conf value, updates conf (preserves CLI overrides).
    - Converts `ServiceDiscoveryType.CONSUL` to `ServiceDiscoveryType.ZK`.
    - If `enable_gpu_training` is False, ensures `embedding_prefetch_capacity >= 1` and `enable_embedding_postpush=True`.
    - If `enable_parameter_sync` is True, sets `enable_realtime_training` or `enable_parameter_sync` on `RunnerConfig` (raises if neither exists).
  - `__post_init__`:
    - Serializes to JSON and records config in `DumpUtils`.
    - Records user params that differ from defaults to `DumpUtils`.
- `Estimator.__init__(model, conf, warm_start_from=None)`:
  - Converts `RunConfig` to `RunnerConfig` if needed.
  - Sets deep-insight metrics on the model based on local vs distributed and runner_conf values.
  - If realtime training on PS, initializes sync backend via ZK (either `ZKBackend` or `ReplicaWatcher` with `MonolithKazooClient`).
  - Applies `params_override` JSON to model `.p` or `.params` when present.
  - Attempts `env_utils.setup_hdfs_env()` if `HADOOP_HDFS_HOME` missing (logs errors).
  - Exports env vars `TF_GRPC_WORKER_CACHE_THREADS` and `MONOLITH_GRPC_WORKER_SERVICE_HANDLER_MULTIPLIER` from runner_conf.
- `Estimator._est` (lazy property):
  - Deep-copies model; sets mode `PREDICT` and instantiates `CpuTraining` task.
  - Deletes `TF_CONF` env var if present.
  - Constructs `tf.estimator.Estimator` with `model_fn`, `model_dir`, and `RunConfig(log_step_count_steps=...)`, with `warm_start_from` if provided.
- `Estimator.train(steps=None, max_steps=None, hooks=None)`:
  - Validates hooks are `tf.estimator.SessionRunHook`.
  - Sets metric prefix `monolith.training.<deep_insight_name>`.
  - Deep-copies model, sets mode `TRAIN`, overrides steps/max_steps if provided.
  - If local: choose model_dir (default `/tmp/<user>/<model>`), call `local_train_internal`, and write `DumpUtils` to `model_dump`.
  - If distributed: disable DumpUtils; start sync backend and subscribe model; log env + flags + params.
    - If `enable_full_sync_training`: call `init_sync_train_and_update_conf` then `distributed_sync_train`.
    - Else: use `monolith_discovery` context; if `enable_gpu_training` -> `device_utils.enable_gpu_training()` and disable `use_gpu_emb_table`; if partial sync and worker -> `try_init_cuda()` and set `device_fn`.
    - Call `distributed_train`.
  - Calls `close()` at end.
- `Estimator.evaluate(steps=None, hooks=None)`:
  - Mirrors `train()` but uses mode `EVAL` and `distributed_train` (no user hooks in distributed eval except full sync).
- `Estimator.predict(...)`:
  - Creates estimator via `_est`, builds input_fn, calls `est.predict(...)`, then `close()`.
- `Estimator.export_saved_model(batch_size=64, name=None, dense_only=False, enable_fused_layout=False)`:
  - Copies model/conf; sets `enable_fused_layout`, model name, batch size, mode `PREDICT`.
  - Creates `CpuTraining` task and exporter; uses `ParserCtx(enable_fused_layout=...)`.
  - Calls `exporter.export_saved_model` with `serving_input_receiver_fn`.
- `import_saved_model(saved_model_path, input_name='instances', output_name='output', signature=None)`:
  - Context manager that resolves latest numeric version directory if `saved_model_path` not numeric.
  - Loads SavedModel with `tf.saved_model.load`, chooses signature (default serving).
  - Builds placeholders dict from requested inputs.
  - Builds output dict from requested outputs or all outputs if none provided.
  - Returns `infer(features)` callable that runs session and maps output tensor names to output names.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/estimator.rs` plus new `run_config.rs`/`runner_config.rs` and export/import helpers.
- Rust public API surface:
  - `EstimatorSpec` struct analog for model outputs.
  - `RunConfig` struct with JSON serialization + `to_runner_config` merge semantics.
  - `Estimator` wrapper that orchestrates training/eval/predict, plus saved-model import/export if TF runtime is enabled.
- Data model mapping: use `monolith-training` model traits (`ModelFn`) for Candle backend; optional TF runtime path for SavedModel import/export.
- Feature gating: `tf-runtime` feature for SavedModel and TF Estimator parity; default Candle backend implements local training only.
- Integration points: `cpu_training`/`distributed_train` equivalents, service discovery, device utils, dump utilities, parameter sync backend.

**Implementation Steps (Detailed)**
1. Implement `RunConfig` in Rust with all fields and defaults; add JSON serialization to match Python `dataclass_json`.
2. Implement `to_runner_config` merge logic (only override when RunConfig value differs from default and from current runner config).
3. Implement `Estimator` struct with local training/eval/predict flows mirroring Python.
4. Add optional ZK/AgentService integration or stub with clear errors when unavailable.
5. Implement env var exports (`TF_GRPC_WORKER_CACHE_THREADS`, `MONOLITH_GRPC_WORKER_SERVICE_HANDLER_MULTIPLIER`).
6. Implement SavedModel export/import only when TF runtime is enabled; otherwise document as unsupported.
7. Add parity tests for RunConfig merging and local train/eval/predict call flow.

**Tests (Detailed)**
- Python tests: `estimator_test.py`, `estimator_dist_test.py`, `estimator_mode_test.py`.
- Rust tests: add `estimator_test.rs` for local flow and config merging; integration tests for distributed modes as available.
- Cross-language parity test: compare config merge outputs, model_dir resolution, and SavedModel import/export behavior.

**Gaps / Notes**
- Python relies on TF Estimator and distributed training stack; Rust currently has only stubs for distributed execution.
- SavedModel import/export likely requires TF runtime and custom ops.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/estimator_dist_test.py`
<a id="monolith-native-training-estimator-dist-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 166
- Purpose/role: Integration test for distributed training/eval using TF_CONFIG-style discovery and multi-process PS/worker setup.
- Key symbols/classes/functions: `EstimatorTrainTest`, `get_cluster`, `get_free_port`.
- External dependencies: `tensorflow`, `RunnerConfig`, `TestFFMModel`, `TfConfigServiceDiscovery`, `Estimator`.
- Side effects: Spawns multiple processes, binds local ports, writes checkpoints under tmp.

**Required Behavior (Detailed)**
- `get_free_port()`:
  - Binds a local socket on port 0 to find an available port; closes socket and returns port.
- `get_cluster(ps_num, worker_num)`:
  - Returns dict with `ps`, `worker`, and `chief` addresses on free ports (workers exclude chief).
- `EstimatorTrainTest.setUpClass`:
  - Removes existing `model_dir` if present.
  - Creates `TestFFMModel` params with deep insight disabled and batch size 64.
- `EstimatorTrainTest.train()`:
  - Spawns `ps_num` PS processes and `worker_num` worker/chief processes.
  - Each process builds `TF_CONFIG`-like dict, uses `TfConfigServiceDiscovery`, constructs `RunnerConfig`, and calls `Estimator.train(steps=10)`.
  - Waits for all processes; asserts exitcode 0 for each.
- `EstimatorTrainTest.eval()`:
  - Same as train but calls `Estimator.evaluate(steps=10)`.
- `test_dist`:
  - Runs `train()` then `eval()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/estimator_dist_test.rs` (new).
- Rust public API surface: distributed training harness and config discovery equivalent.
- Data model mapping: distributed cluster config (ps/worker/chief), runner config, model params.
- Feature gating: likely `tf-runtime` or `distributed` feature; skip if distributed stack not available.
- Integration points: service discovery and process orchestration.

**Implementation Steps (Detailed)**
1. Implement a Rust integration test that spawns multiple processes (or threads) for PS/worker roles.
2. Provide a discovery config equivalent to `TfConfigServiceDiscovery` and ensure `Estimator` can use it.
3. Run short train/eval steps and assert clean exit.
4. Add timeouts and cleanup for spawned processes and temp directories.

**Tests (Detailed)**
- Python tests: `monolith/native_training/estimator_dist_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/estimator_dist_test.rs` (integration).
- Cross-language parity test: verify training/eval complete under equivalent cluster topology.

**Gaps / Notes**
- Uses real multi-process TF; Rust currently lacks distributed PS/worker runtime.
- Port binding is fragile; consider deterministic port assignment for CI.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/estimator_mode_test.py`
<a id="monolith-native-training-estimator-mode-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 417
- Purpose/role: End-to-end integration tests for multiple distributed modes (CPU, sparse+ dense GPU, full GPU) by launching the training binary with various env/config permutations.
- Key symbols/classes/functions: `DistributedTrainTest`, `_run_test`, `run_cpu`, `sparse_dense_run`, `full_gpu_run`.
- External dependencies: TensorFlow, `RunnerConfig`, training binary `monolith/native_training/tasks/sparse_dense_gpu/model`, `gen_input_file`, `MultiHeadModel`, `test_util`.
- Side effects: Creates temp dataset files, spawns multiple processes (including mpirun), sets many env vars, writes logs, deletes temp dirs.

**Required Behavior (Detailed)**
- `setUpClass`:
  - Generates dataset file via `gen_input_file` and creates symlinks for suffixes 0..9.
  - Updates `FLAGS.dataset_input_patterns` to include `{INT(0,99)}`.
- `find_free_port(count)`:
  - Finds `count` available local ports (no reuse).
- `_run_test(...)`:
  - Creates `cur_modir` under test tmp dir; removes existing.
  - Builds `args_tmpl` list for the training binary with flags:
    - mode=train, model_dir, num_ps/workers, uuid, dataset flags, discovery settings, timeouts, metrics disable, dataservice toggle, cluster type.
  - Populates MLP_* env vars per role via `fill_host_env`.
  - Allocates ports for PS/worker/dsworker/dispatcher and sets env accordingly.
  - `start_process`:
    - For `use_mpi_run=True`, writes a hostfile and uses `mpirun` with Horovod-related env exports.
    - Else, spawns subprocess per role with `MLP_ROLE`, `MLP_ROLE_INDEX`, `MLP_PORT`, and `MLP_SSH_PORT` envs, writing logs to files.
  - Starts dispatcher, dsworker, ps, worker processes.
  - `wait_for_process` enforces timeouts; may terminate on timeout when `ignore_timeout=True`.
  - Cleans up log files and removes `cur_modir`.
- `run_cpu(...)`:
  - Skips if GPU is available; runs CPU cases with `enable_gpu_training=False`, `enable_sync_training=False` and embedding prefetch/postpush flags.
- `sparse_dense_run(...)`:
  - Requires GPU; uses MPI run and sets sync training flags, partial sync, params_override, and dataset service.
- `full_gpu_run(...)`:
  - Requires GPU; uses MPI run with `enable_sync_training`, `reorder_fids_in_data_pipeline`, `filter_type=probabilistic_filter`, `enable_async_optimize=False`.
- Test variants:
  - CPU tests `test_cpu0..3` vary `enable_fused_layout` and `use_native_multi_hash_table`.
  - Sparse+Dense GPU tests `test_sparse_dense0..3` vary layout and native hash table.
  - Full GPU tests `test_full_gpu_0..3` vary layout and native hash table.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/estimator_mode_test.rs` (new) or CI scripts.
- Rust public API surface: distributed training CLI entrypoint and env-based cluster discovery.
- Feature gating: GPU and MPI required; tests should be behind `gpu`/`mpi` feature flags and skipped in CI by default.
- Integration points: training binary CLI, dataset service, Horovod/BytePS integration.

**Implementation Steps (Detailed)**
1. Implement or stub the Rust training binary to accept similar CLI flags.
2. Add integration tests that spawn subprocesses with env roles for PS/worker/dsworker/dispatcher.
3. Add MPI-based test harness if Horovod/BytePS parity is required.
4. Ensure temp dirs and logs are cleaned up even on failure.

**Tests (Detailed)**
- Python tests: `monolith/native_training/estimator_mode_test.py`.
- Rust tests: integration tests in `monolith-rs/crates/monolith-training/tests/` or CI scripts.
- Cross-language parity test: compare training completion and exit codes across CPU/GPU modes.

**Gaps / Notes**
- Heavy integration tests require external binaries and GPU; likely to be skipped in Rust CI.
- Port allocation is fragile; may need reserved port ranges.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/estimator_test.py`
<a id="monolith-native-training-estimator-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 112
- Purpose/role: Local-mode Estimator smoke tests for train/eval/predict/export/import flow.
- Key symbols/classes/functions: `EstimatorTrainTest`, `get_saved_model_path`.
- External dependencies: TensorFlow, `RunnerConfig`, `TestFFMModel`, `generate_ffm_example`, `import_saved_model`.
- Side effects: Writes checkpoints and exported models under temp dirs; performs inference loops.

**Required Behavior (Detailed)**
- `setUpClass`:
  - Removes existing model_dir if present.
  - Sets model params: deep insight disabled, batch size 64, export dir base, `shared_embedding=True`.
  - Creates `RunnerConfig(is_local=True, num_ps=0, model_dir=..., use_native_multi_hash_table=False)`.
- `train/eval/predict`:
  - Instantiate `Estimator` and call the respective method (steps=10 for train/eval).
- `export_saved_model`:
  - Calls `Estimator.export_saved_model()` with defaults.
- `import_saved_model`:
  - Uses latest saved model dir from `export_base`.
  - Runs inference through `import_saved_model` context for 10 iterations, with generated FFM examples.
- `test_local`:
  - Runs train, eval, predict, export, and import in sequence.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/estimator_test.rs` (new).
- Rust public API surface: local Estimator train/eval/predict/export/import.
- Feature gating: SavedModel export/import requires `tf-runtime`; otherwise stub or skip.
- Integration points: training data generation, model definition, export path handling.

**Implementation Steps (Detailed)**
1. Implement a local-only Estimator test in Rust that runs train/eval/predict with a simple model.
2. Add SavedModel export/import tests behind `tf-runtime` feature.
3. Ensure temp dirs are cleaned up after tests.

**Tests (Detailed)**
- Python tests: `monolith/native_training/estimator_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/estimator_test.rs`.
- Cross-language parity test: compare export/import outputs on fixed inputs.

**Gaps / Notes**
- Python import_saved_model uses TF sessions and custom ops; Rust parity likely requires TF runtime.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/feature.py`
<a id="monolith-native-training-feature-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 663
- Purpose/role: Defines feature slots/columns, embedding table interfaces, and embedding slice fusion logic for Monolith feature pipelines.
- Key symbols/classes/functions: `FeatureEmbTable`, `FeatureSlotConfig`, `FeatureSlot`, `FeatureColumn`, `FeatureFactory`, `DummyFeatureEmbTable`, `DummyFeatureFactory`, `_FeatureFactoryFusionHelper`, `create_embedding_slices`, `EmbeddingFeatureEmbTable`, `FeatureFactoryFromEmbeddings`, `EmbeddingLayoutFactory`.
- External dependencies: TensorFlow, `entry`, `embedding_combiners`, `distribution_ops`, `device_utils`, `ragged_utils`, `embedding_hash_table_pb2`, `prefetch_queue`, `is_exporting`.
- Side effects: Uses env var `MONOLITH_GPU_FEATURE_FACTORY_FUSION_LEVEL`; adds tensors to TF collection `monolith_reduced_embs`.

**Required Behavior (Detailed)**
- Constants:
  - `_FEATURE_STRAT_END_KEY = "{}:{}_{}"` used to key embedding slices by feature name and slice bounds.
  - `DEFAULT_EXPIRE_TIME = 36500` (days).
- `FeatureEmbTable` (abstract):
  - `add_feature_slice(segment, learning_rate_fn)` and `set_feature_metadata(feature_name, combiner)` are no-ops in base.
  - `embedding_lookup(feature_name, start, end)` is abstract.
- `FeatureSlice`: NamedTuple with `feature_slot`, `start`, `end`.
- `FeatureSlotConfig`:
  - Defaults for bias/default vector configs using `entry` builders; default expire time and occurrence threshold.
  - `__post_init__` sets `name` to `slot_id` string if not provided.
- `FeatureSlot`:
  - Holds table/config, current dim size, and registered feature columns.
  - If `has_bias` true, creates a bias slice of dim 1 using bias configs.
  - `add_feature_slice`:
    - Applies defaults for initializer/optimizer/compressor/learning_rate_fn.
    - Creates `EntryConfig.Segment` via `entry.CombineAsSegment` and registers with table.
    - Returns `FeatureSlice(start, end)` and updates `_current_dim_size`.
  - Registers feature columns via `_add_feature_column`, and updates table metadata.
  - `get_feature_columns`, `get_bias_slice`, `slot` (int of name), `name`.
- `FeatureColumn`:
  - Factory helpers: `reduce_sum`, `reduce_mean`, `first_n(seq_length)`.
  - `embedding_lookup(s)` asserts slice belongs to slot and delegates to table.
  - `get_all_embeddings_concat()` returns full embedding tensor for gradients (start/end None).
  - `get_all_embedding_slices()` returns per-slice tensors for this feature name.
  - `get_bias()` returns bias slice embeddings.
  - `set_size_tensor(row_lengths)`:
    - Only for `FirstN` combiner; builds boolean mask `[B, max_seq_length]` from row_lengths and stores as int32 `size_tensor`.
- `FeatureFactory` (abstract):
  - Manages `slot_to_occurrence_threshold` and `slot_to_expire_time`.
  - `apply_gradients` default raises `NotImplementedError`.
- `DummyFeatureEmbTable` (config collection):
  - `add_feature_slice` auto-infers `learning_rate_fn` from optimizer config:
    - If optimizer has `warmup_steps > 0`, uses `PolynomialDecay` from 0.0 to `learning_rate` over warmup_steps.
    - Else uses `opt_config.learning_rate` value directly.
  - `embedding_lookup` builds placeholders and combines via combiner; respects fixed batch size.
  - `get_table_config` merges slices via `_merge_slices` and returns `TableConfig` with:
    - `slice_configs` merged
    - `feature_names`
    - `unmerged_slice_dims` (original slice sizes)
    - `hashtable_config`
    - `feature_to_combiners`.
  - `_merge_slices` merges adjacent slices when proto config (excluding dim_size) and `learning_rate_fn` string match; sums dim_size.
- `DummyFeatureFactory`:
  - Ensures unique table name; registers slot thresholds/expire times by slot_id.
  - `apply_gradients` returns `tf.no_op()`.
  - `get_table_name_to_table_config` errors if a table has no slices.
- `EmbeddingFeatureEmbTable`:
  - Wraps actual embeddings and embedding_slices; returns full embedding when start/end None; otherwise uses `_FEATURE_STRAT_END_KEY`.
- `_FeatureFactoryFusionHelper`:
  - Collects ragged rows, value_rowids, embeddings, batch_size, and slice_dims.
  - `reduce_and_split`: CPU scatter_nd reduce and split; adds reduced tensor to collection.
  - `fused_reduce_and_split`: uses `distribution_ops.fused_reduce_sum_and_split` on CPU.
  - `fused_reduce_then_split`: GPU `distribution_ops.fused_reduce_and_split_gpu` and then manual split mapping.
- `create_embedding_slices(...)`:
  - For each feature:
    - If combiner is `ReduceSum`, uses helper for fused reduce+split.
    - Else uses combiner + `tf.split` on combined embeddings.
  - Chooses fusion path:
    - If not exporting and within GPU placement and `MONOLITH_GPU_FEATURE_FACTORY_FUSION_LEVEL==1` -> `fused_reduce_then_split`.
    - Else if not exporting -> `reduce_and_split` (CPU) or `fused_reduce_and_split` (CPU fused) depending on placement.
    - If exporting -> `reduce_and_split`.
  - Constructs `embedding_slices` dict keyed by name/start/end.
- `FeatureFactoryFromEmbeddings`: builds `EmbeddingFeatureEmbTable` from `name_to_embeddings` and `name_to_embedding_slices`.
- `EmbeddingLayoutFactory`:
  - Uses `PartitionedHashTable` to apply gradients with layout embeddings and optional async push.
  - `get_layout` and `flattened_layout` expose layout tensors.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/feature.rs` plus embedding combiner logic in `monolith-rs/crates/monolith-layers` and hash table config in `monolith-rs/crates/monolith-hash-table`.
- Rust public API surface:
  - `FeatureSlot`, `FeatureSlice`, `FeatureColumn` equivalents in `monolith-core`.
  - Combiner types (`ReduceSum`, `ReduceMean`, `FirstN`) in `monolith-layers`.
  - `create_embedding_slices` and fusion helpers in a new `monolith-training` or `monolith-layers` module.
- Data model mapping: represent ragged inputs as `(values, row_splits)` and carry slice dimensions explicitly.
- Feature gating: GPU fused ops require TF runtime or custom kernels; Candle backend should use CPU reduce+split.
- Integration points: `feature_utils`, embedding lookup, and hash table update paths.

**Implementation Steps (Detailed)**
1. Implement FeatureSlot/FeatureColumn config layering in Rust, mapping to existing `monolith-core` feature structs.
2. Implement `DummyFeatureEmbTable` and `DummyFeatureFactory` for config collection and tests.
3. Implement `create_embedding_slices` with reduce/split logic; add optional fused path when TF runtime is available.
4. Preserve learning rate warmup logic in `DummyFeatureEmbTable.add_feature_slice`.
5. Add `EmbeddingLayoutFactory` wrapper around Rust hash table gradient application (or stub if hash table not available).
6. Add tests matching Python expectations for slice merging, bias, combiner selection, and fused behavior.

**Tests (Detailed)**
- Python tests: `monolith/native_training/feature_test.py`.
- Rust tests: add `monolith-rs/crates/monolith-core/tests/feature_test.rs` and/or `monolith-rs/crates/monolith-layers/tests/feature_factory_test.rs`.
- Cross-language parity test: compare slice configs (serialized proto), embedding slice outputs, and combiners.

**Gaps / Notes**
- Many operations rely on TF ragged tensors and custom fused ops; Rust backend must choose between TF runtime or native reductions.
- The `_FEATURE_STRAT_END_KEY` key format must match exactly to ensure lookup parity.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/feature_test.py`
<a id="monolith-native-training-feature-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 266
- Purpose/role: Tests for feature slot/column config collection, slice merging, and embedding slice creation (including fused paths and FirstN behavior).
- Key symbols/classes/functions: `CollectingConfigTest`, `EmbeddingTest`.
- External dependencies: TensorFlow, `entry`, `embedding_combiners`, `feature`, `learning_rate_functions`, `embedding_hash_table_pb2`, protobuf `text_format`.
- Side effects: None beyond TensorFlow graph execution.

**Required Behavior (Detailed)**
- `CollectingConfigTest.test_basic`:
  - Dummy table + segment dim_size=5 with sgd; reduce_sum combiner; embedding_lookup produces placeholder shape `[4, 5]`.
- `test_basic_with_seq_features`:
  - FirstN(10) combiner -> embedding_lookup placeholder shape `[4, 10, 5]`.
- `test_info`:
  - Adds segments with adagrad warmup and sgd; two adjacent sgd slices with same learning_rate_fn should merge to dim_size=4.
  - Ensures learning_rate_fn for warmup slice is a `LearningRateFunction`.
  - `feature_names` list contains `feature1`.
- `test_factory`:
  - DummyFeatureFactory creates slot and feature columns; table config includes feature names and slice dim_size=5.
- `test_factory_with_seq_features`:
  - FirstN combiners stored in `feature_to_combiners` map; verifies mapping.
- `test_factory_with_slot_occurrence_threshold`:
  - Factory stores occurrence thresholds keyed by slot_id.
- `test_factory_with_applying_gradients`:
  - Dummy factory apply_gradients accepts grads and returns no-op.
- `test_bias`:
  - Slot with `has_bias=True` exposes bias lookup without errors.
- `EmbeddingTest.test_factory`:
  - Uses `create_embedding_slices` with ReduceSum; lookup returns `[[1],[2]]` for ragged ids.
- `test_factory_with_seq_features`:
  - FirstN(2) returns sequence embeddings `[[[1],[3]], [[5],[7]]]`.
- `test_fused_factory`:
  - ReduceSum with ragged splits producing zeros in empty rows; verifies slice outputs for each slice.
- `test_fused_factory_with_seq_features_larger_than_max_seq_length`:
  - FirstN(2) truncates rows longer than max_seq_length; verifies outputs.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/tests/feature_test.rs` and/or `monolith-rs/crates/monolith-layers/tests/feature_factory_test.rs` (new).
- Rust public API surface: `FeatureSlot`, `FeatureColumn`, `DummyFeatureFactory`, `create_embedding_slices`, combiners.
- Data model mapping: ragged inputs as values + row_splits; test outputs should match Python arrays.
- Feature gating: fused GPU paths behind TF runtime or CUDA feature; CPU paths always available.
- Integration points: `entry` config builder and learning rate functions.

**Implementation Steps (Detailed)**
1. Port each test case with deterministic tensors.
2. Validate slice merging behavior using serialized proto bytes for segments.
3. Add tests for FirstN shape and truncation behavior.
4. Ensure `DummyFeatureFactory` tracks occurrence thresholds and bias slice creation.

**Tests (Detailed)**
- Python tests: `monolith/native_training/feature_test.py`.
- Rust tests: add parity tests under `monolith-rs/crates/monolith-core/tests`.
- Cross-language parity test: compare outputs and merged slice configs with Python reference.

**Gaps / Notes**
- Fused GPU behavior depends on custom ops; Rust tests should skip if TF runtime is unavailable.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/feature_utils.py`
<a id="monolith-native-training-feature-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 419
- Purpose/role: Applies gradients to dense variables and embedding tables with optional clipping, Horovod/BytePS allreduce, and async embedding updates.
- Key symbols/classes/functions: `allreduce_cond`, `GradClipType`, `_gen_norm_warmup`, `apply_gradients_with_var_optimizer`, `apply_gradients`.
- External dependencies: TensorFlow, `clip_ops`, `distribution_ops.gen_distribution_ops`, `device_utils`, `feature`, `NativeContext`, Horovod/BytePS (optional).
- Side effects: Reads env vars, performs allreduce, writes TF summaries, updates global step, mutates globals `control_ops` and `dense_opt_ops`.

**Required Behavior (Detailed)**
- Env flags read at import time:
  - `MONOLITH_WITH_HOROVOD`, `MONOLITH_WITH_BYTEPS`, `MONOLITH_WITH_BYTEPS_ALLREDUCE`, `MONOLITH_WITH_ALLREDUCE_FUSION`, `MONOLITH_WITH_ALLREDUCE_FP16`, `MONOLITH_SKIP_ALLREDUCE`.
  - If Horovod enabled, imports `horovod.tensorflow` and compression classes.
- `allreduce_cond(grads, scale=1)`:
  - Selects BytePS or Horovod compression (FP16 vs None) based on envs.
  - Filters `None` grads, allreduces only non-None grads, then maps results back into original positions.
  - Fusion modes:
    - `one`: uses `monolith_aligned_flat_concat` + allreduce + `monolith_aligned_flat_split`.
    - `grouped`: uses `hvd.grouped_allreduce` (not supported with BytePS).
    - `multi`: raises `RuntimeError` (dropped).
    - default: allreduces each grad individually with Average op.
- `GradClipType` enum: `ClipByNorm`, `ClipByGlobalNorm`, `ClipByValue`, `ClipByDenseAndSparse`, `NoClip`.
- `_gen_norm_warmup(clip_norm, global_step_var, warmup_step)`:
  - Returns `clip_norm` scaled linearly from 0 to 1 over `warmup_step` using `tf.cond`.
- `apply_gradients_with_var_optimizer(...)`:
  - Computes grads for dense variables + embedding tensors.
  - For fused layout, replaces missing grads with zeros.
  - Splits dense vs sparse grads and optionally applies UE conditional gradient check.
  - Supports clip by global norm (dense/sparse), value, or per-tensor norm; optional sparse warmup.
  - Defers global norm clipping to a scale factor when using GPU + allreduce (fused with later kernels).
  - Optionally writes gradient/variable histograms and norms to summaries.
  - Dense grads optionally allreduced and L2 weight-decayed.
  - Applies dense grads via custom per-variable optimizer or shared `var_opt` (async via `ctx.add_async_function`).
  - Applies embedding grads via `ctx.apply_embedding_gradients` (on CPU) with optional scale.
  - Increments `global_step` after optimize ops with control dependencies.
- `apply_gradients(...)`:
  - Similar flow for layout-based embeddings (`ctx.layout_factory.flattened_layout()`) with simpler clipping logic.
  - If no dense variables, still increments global_step.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/feature_utils.rs` (new) and `monolith-rs/crates/monolith-optimizer` for optimizer integration.
- Rust public API surface: gradient application helpers for dense + embedding params, clip modes, and allreduce hooks.
- Data model mapping: Candle tensors for dense grads; embedding grads routed through hash table/update API.
- Feature gating: Horovod/BytePS allreduce under `tf-runtime` or `distributed` feature; default backend uses local grads only.
- Integration points: `NativeContext`, `EmbeddingLayoutFactory`, async function manager, and training loop.

**Implementation Steps (Detailed)**
1. Implement `GradClipType` enum and clipping helpers in Rust.
2. Implement global norm computation and optional warmup scaling.
3. Implement dense vs sparse gradient separation (embedding tensors tracked separately).
4. Add optional allreduce hooks (no-op when disabled) and fusion strategy `one` if TF runtime is enabled.
5. Add weight decay for dense grads.
6. Wire into `NativeContext.apply_embedding_gradients` equivalent and async scheduling.
7. Add summary/logging equivalents where possible.

**Tests (Detailed)**
- Python tests: `monolith/native_training/feature_utils_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/feature_utils_test.rs` (new).
- Cross-language parity test: verify gradient updates and global_step increments on identical toy graphs.

**Gaps / Notes**
- Fusion path depends on custom TF ops (`monolith_aligned_flat_concat/split`).
- UE gradient check logic depends on feature tensors and names; requires parity in Rust model representation.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/feature_utils_test.py`
<a id="monolith-native-training-feature-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 144
- Purpose/role: Tests gradient application for dense vars and embeddings, including fused allreduce path and async embedding push.
- Key symbols/classes/functions: `_setup_test_embedding`, `FeatureUtilsTest` cases.
- External dependencies: TensorFlow, `feature_utils`, `feature`, `embedding_combiners`, `NativeContext`, `prefetch_queue`.
- Side effects: Sets env `MONOLITH_WITH_ALLREDUCE_FUSION=one`.

**Required Behavior (Detailed)**
- `_setup_test_embedding(is_async=False)`:
  - Builds embedding var and ragged ids, creates embedding slices via `create_embedding_slices`.
  - Mocks `feature_factory.apply_gradients` to subtract gradients from embedding vars.
  - Returns `(ctx, fc, emb_var, emb)`.
- `test_apply_gradients_with_dense_optimizer`:
  - Loss includes dense var and embedding sum; clip_norm=1.0.
  - After one step: dense var becomes 0.5; embedding var becomes `[0.5,0.5,0.5,1.0]`; global_step=1.
- `test_apply_gradients_with_dense_optimizer_gpu` (GPU-only):
  - Same expectations with `use_allreduce=True` and no summary; tests deferred clip fusion path.
- `test_apply_gradients_with_dense_optimizer_post_push`:
  - Async embedding push enabled; running op three times triggers two async pushes.
  - Dense var becomes -1.0; embedding var becomes `[-2.0,-2.0,-2.0,1.0]`.
- `test_apply_gradients_without_dense_optimizer`:
  - Loss uses embeddings only; after step, embedding var becomes `[0.0,0.0,0.0,1.0]` and global_step=1.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/feature_utils_test.rs` (new).
- Rust public API surface: gradient application helpers and async embedding push hooks.
- Feature gating: GPU tests behind `cuda`/`tf-runtime` feature; skip if unavailable.
- Integration points: `NativeContext` equivalent and embedding update interface.

**Implementation Steps (Detailed)**
1. Port `_setup_test_embedding` logic to create a small embedding table and feature column in Rust.
2. Implement tests for dense+embedding gradients with clipping and global_step increments.
3. Add GPU test for deferred clip + allreduce path (skip if no GPU).
4. Add async embedding push test verifying delayed updates.

**Tests (Detailed)**
- Python tests: `monolith/native_training/feature_utils_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/feature_utils_test.rs`.
- Cross-language parity test: compare updated dense var and embedding values after a single step.

**Gaps / Notes**
- Tests assume deterministic gradients and initial values; Rust must mirror initialization and scaling exactly.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/gen_seq_mask.py`
<a id="monolith-native-training-gen-seq-mask-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 26
- Purpose/role: Wrapper around custom op to generate sequence masks from row splits.
- Key symbols/classes/functions: `gen_seq_mask`.
- External dependencies: TensorFlow, `gen_monolith_ops`.
- Side effects: None.

**Required Behavior (Detailed)**
- Accepts `splits` as Tensor or RaggedTensor; uses `row_splits()` when ragged.
- Calls `ops.gen_seq_mask(splits=..., max_seq_length=...)`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/src/gen_seq_mask.rs` (new).
- Rust public API surface: `gen_seq_mask` wrapper.
- Feature gating: TF runtime + custom ops.

**Implementation Steps (Detailed)**
1. Add binding for `gen_seq_mask` custom op.
2. Accept either row_splits tensor or ragged wrapper.

**Tests (Detailed)**
- Python tests: `gen_seq_mask_test.py`.
- Rust tests: `monolith-rs/crates/monolith-tf/tests/gen_seq_mask_test.rs`.
- Cross-language parity test: compare masks for fixed splits.

**Gaps / Notes**
- Requires custom ops library.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/gen_seq_mask_test.py`
<a id="monolith-native-training-gen-seq-mask-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 42
- Purpose/role: Tests gen_seq_mask for int32 and int64 splits.
- Key symbols/classes/functions: `GenSeqMaskTest`.
- External dependencies: TensorFlow, `gen_seq_mask`.
- Side effects: None.

**Required Behavior (Detailed)**
- For splits `[0,5,7,9,13]` and `max_seq_length=6`, mask equals expected matrix for both int32 and int64.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/tests/gen_seq_mask_test.rs`.
- Rust public API surface: gen_seq_mask wrapper.
- Feature gating: TF runtime + custom ops.

**Implementation Steps (Detailed)**
1. Add tests for int32 and int64 splits with expected outputs.

**Tests (Detailed)**
- Python tests: `gen_seq_mask_test.py`.
- Rust tests: `monolith-rs/crates/monolith-tf/tests/gen_seq_mask_test.rs`.
- Cross-language parity test: compare masks.

**Gaps / Notes**
- None.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/gflags_utils.py`
<a id="monolith-native-training-gflags-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 282
- Purpose/role: Utilities to extract flags from dataclass docstrings and link flags to dataclass defaults.
- Key symbols/classes/functions: `extract_help_info`, `extract_flags`, `extract_flags_decorator`, `update`, `LinkDataclassToFlags`, `update_by_flags`.
- External dependencies: `absl.flags`, `dataclasses`, `Enum`, `inspect`, `re`.
- Side effects: Defines gflags and mutates dataclass instances based on flags.

**Required Behavior (Detailed)**
- `extract_help_info` parses `:param` lines and returns normalized help strings.
- `extract_flags` defines flags for type-hinted fields (int/bool/str/float/enum) with defaults; skips missing help or skip list.
- `extract_flags_decorator` returns decorator that calls `extract_flags`.
- `get_flags_parser` returns parser that logs errors and exits on invalid flags.
- `update` applies flags to config when config field is default and flag is non-default.
- `LinkDataclassToFlags` validates fields/flags and records mappings.
- `update_by_flags` patches `__init__` to apply linked flags when field is default.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-cli/src/gflags_utils.rs` (new) or `monolith-core` config utilities.
- Rust public API surface: config flag extraction and update helpers.
- Feature gating: CLI only.

**Implementation Steps (Detailed)**
1. Implement help metadata parsing (or explicit metadata in Rust).
2. Provide flag registration for primitive and enum types.
3. Implement update logic that respects defaults vs overrides.
4. Provide helper to link flags to dataclass fields.

**Tests (Detailed)**
- Python tests: `gflags_utils_test.py`.
- Rust tests: unit tests for help parsing and update/flag linking behavior.
- Cross-language parity test: compare config updates for equivalent flags.

**Gaps / Notes**
- Python relies on docstring parsing; Rust likely needs explicit metadata.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/gflags_utils_test.py`
<a id="monolith-native-training-gflags-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 217
- Purpose/role: Tests help parsing, flag extraction, config update, and flag linking.
- Key symbols/classes/functions: `GflagUtilsTest`.
- External dependencies: `absl.flags`, `absltest`, `gflags_utils`.
- Side effects: Defines flags in test scope.

**Required Behavior (Detailed)**
- `test_extract_help_info`: parses `:param` lines and joins multi-line help.
- `test_update`: updates config fields only when default and flag is non-default.
- `test_extract_gflags_decorator`: ensures flags are defined for decorated dataclasses and skipped for removed/base fields.
- `test_link_flag` / `test_link_flag_inheritance`: validates linked flags override defaults and inheritance behavior.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-cli/tests/gflags_utils_test.rs` (new).
- Rust public API surface: flag extraction and linking utilities.
- Feature gating: CLI only.

**Implementation Steps (Detailed)**
1. Add tests for help parsing and update logic.
2. Add tests for linked flags and inheritance behavior.

**Tests (Detailed)**
- Python tests: `gflags_utils_test.py`.
- Rust tests: `monolith-rs/crates/monolith-cli/tests/gflags_utils_test.rs`.
- Cross-language parity test: compare flag update outcomes.

**Gaps / Notes**
- Flag registration order in Rust may differ; ensure deterministic behavior.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/graph_meta.py`
<a id="monolith-native-training-graph-meta-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 30
- Purpose/role: Stores per-graph metadata in a TF collection.
- Key symbols/classes/functions: `get_meta`.
- External dependencies: TensorFlow.
- Side effects: Mutates graph collections.

**Required Behavior (Detailed)**
- Uses graph collection `monolith_graph_meta` to store a single dict.
- If key missing, calls `MetaFactory()` and stores result.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/src/graph_meta.rs` (new).
- Rust public API surface: graph metadata helper.
- Feature gating: TF runtime only.

**Implementation Steps (Detailed)**
1. Implement get-or-create metadata storage keyed in graph collection.
2. Mirror single-dict behavior.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: add unit test for get_meta caching.

**Gaps / Notes**
- None.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/graph_utils.py`
<a id="monolith-native-training-graph-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 26
- Purpose/role: Adds batch-norm moving average assign ops into TF UPDATE_OPS collection.
- Key symbols/classes/functions: `add_batch_norm_into_update_ops`.
- External dependencies: TensorFlow.
- Side effects: Mutates default graph collections.

**Required Behavior (Detailed)**
- `add_batch_norm_into_update_ops()`:
  - Scans default graph operations.
  - Selects ops where `"AssignMovingAvg"` is in the op name and `op.type == "AssignSubVariableOp"`.
  - Adds each to `tf.GraphKeys.UPDATE_OPS` collection.

**Rust Mapping (Detailed)**
- Target crate/module: TF runtime utility module (e.g., `monolith-rs/crates/monolith-tf/src/graph_utils.rs`).
- Rust public API surface: `add_batch_norm_into_update_ops` equivalent for TF graphs.
- Feature gating: `tf-runtime` only.
- Integration points: training graph construction when using batch norm layers.

**Implementation Steps (Detailed)**
1. Implement graph op scan in TF runtime bindings.
2. Filter ops by name substring and op type.
3. Add ops to UPDATE_OPS collection.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: add a small TF graph with batch norm and verify UPDATE_OPS contains moving average assigns.
- Cross-language parity test: compare the count of added ops for a known graph.

**Gaps / Notes**
- No-op for Candle backend; document as TF-only behavior.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hash_filter_ops.py`
<a id="monolith-native-training-hash-filter-ops-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 217
- Purpose/role: Builds hash filter resources, intercepts gradients, and handles save/restore for hash filters.
- Key symbols/classes/functions: `FilterType`, `create_hash_filters`, `save_hash_filter`, `restore_hash_filter`, `intercept_gradient`, `HashFilterCheckpointSaverListener`, `HashFilterCheckpointRestorerListener`.
- External dependencies: TensorFlow custom ops `gen_monolith_ops`, `save_utils.SaveHelper`, `basic_restore_hook`, `utils.ps_device`.
- Side effects: Creates TF resources, writes checkpoint assets, registers gradient for custom op.

**Required Behavior (Detailed)**
- Constants:
  - `HASH_FILTER_CAPACITY=300000000`, `HASH_FILTER_SPLIT_NUM=7`, `_TIMEOUT_IN_MS=1800000`.
- `FilterType`: string constants `SLIDING_HASH_FILTER`, `PROBABILISTIC_FILTER`, `NO_FILTER`.
- `create_hash_filter(capacity, split_num, config, name_suffix)`:
  - Calls `MonolithHashFilter` custom op with shared_name `MonolithHashFilter<suffix>`.
- `create_probabilistic_filter(equal_probability, config, name_suffix)`:
  - Calls `MonolithProbabilisticFilter` op with shared_name `MonolithProbabilisticFilter<suffix>`.
- `create_dummy_hash_filter(name_suffix)`:
  - Calls `MonolithDummyHashFilter` op with shared_name `DummyHashFilter<suffix>`.
- `_create_hash_filter(...)`:
  - Selects real or dummy filter based on `enable_hash_filter` and `filter_type`; invalid type raises `ValueError`.
- `create_hash_filters(ps_num, enable_hash_filter, ...)`:
  - If `ps_num==0`, returns a single filter.
  - Else, for each PS index, creates filter on `utils.ps_device(i)` unless exporting standalone.
- `save_hash_filter(hash_filter, basename, enable_hash_filter)`:
  - If enabled, uses `monolith_hash_filter_save` custom op; else returns `tf.no_op()`.
- `restore_hash_filter(hash_filter, basename, enable_hash_filter)`:
  - If enabled, uses `monolith_hash_filter_restore` custom op; else returns `tf.no_op()`.
- `intercept_gradient(filter_tensor, ids, embeddings)`:
  - Calls `MonolithHashFilterInterceptGradient`; filters gradients based on ids.
- `HashFilterCheckpointSaverListener`:
  - Builds save graph with placeholders per hash filter; writes to asset dir using SaveHelper.
  - `before_save` writes to `hash_filter_<ps_idx>` files under asset dir and runs save op with timeout.
- `HashFilterCheckpointRestorerListener`:
  - Looks up latest checkpoint; restores from asset dir or legacy prefix.
  - Uses placeholders for hash_filter basenames and runs restore op with timeout.
- Registered gradient `MonolithHashFilterInterceptGradient`:
  - Uses `MonolithHashFilterInterceptGradientGradient` custom op to produce filtered gradients; returns `(None, None, filtered_grad)`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/src/hash_filter_ops.rs` (new) or TF runtime adapter.
- Rust public API surface: hash filter creation, save/restore, and gradient intercept wrappers.
- Feature gating: `tf-runtime` + custom ops required; no-op in Candle backend.
- Integration points: hash filter use in embedding tables and training loops.

**Implementation Steps (Detailed)**
1. Wrap custom ops for hash filter creation and gradient intercept.
2. Implement save/restore helpers using TF session run with timeouts.
3. Add Rust equivalents of saver/restorer listeners if using TF Estimator.
4. Ensure asset dir layout matches Python (`hash_filter_<ps_idx>` files).

**Tests (Detailed)**
- Python tests: `monolith/native_training/hash_filter_ops_test.py`.
- Rust tests: integration tests under `monolith-rs/crates/monolith-tf/tests/hash_filter_ops_test.rs` (new).
- Cross-language parity test: compare gradient filtering behavior and save/restore contents.

**Gaps / Notes**
- Requires custom ops from `libmonolith_ops` and TF runtime; not supported on pure Candle backend.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hash_filter_ops_test.py`
<a id="monolith-native-training-hash-filter-ops-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 217
- Purpose/role: Tests hash filter gradient interception and save/restore behavior.
- Key symbols/classes/functions: `HashFilterOpsTest` cases.
- External dependencies: TensorFlow, `hash_filter_ops`, `embedding_hash_table_pb2`, TFRecord reader.
- Side effects: Writes TFRecord checkpoint shards under `TEST_TMPDIR`.

**Required Behavior (Detailed)**
- `test_hash_filter_basic`:
  - Creates hash filter with occurrence threshold 3; intercepts gradient.
  - Gradients are filtered progressively: first two runs zero out more ids, later runs allow gradients.
- `test_hash_filter_save_restore`:
  - Saves filter to basename; verifies 7 split files created.
  - Restores and verifies gradient filtering state persists across saves.
- `test_hash_filter_save_restore_across_multiple_filters`:
  - Creates filter with split_num=100; verifies each shard contains expected `HashFilterSplitMetaDump` fields.
  - After second save, first 4 shards contain 2 elements; remaining shards contain 0.
- `test_dummy_hash_filter_basic`:
  - Dummy filter should not filter gradients (all ones).
- `test_dummy_hash_filter_save_restore`:
  - Save/restore with dummy filter produces no files and no effect on gradients.
- `test_restore_not_found`:
  - Restoring from non-existent path raises exception.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/tests/hash_filter_ops_test.rs` (new).
- Rust public API surface: hash filter creation, save/restore, intercept gradient ops.
- Feature gating: `tf-runtime` + custom ops; skip otherwise.
- Integration points: TFRecord parsing of `HashFilterSplitMetaDump`.

**Implementation Steps (Detailed)**
1. Port test cases to Rust TF runtime harness.
2. Implement TFRecord reader for `HashFilterSplitMetaDump` proto in Rust.
3. Validate gradient filtering sequence and save/restore shard counts.

**Tests (Detailed)**
- Python tests: `monolith/native_training/hash_filter_ops_test.py`.
- Rust tests: `monolith-rs/crates/monolith-tf/tests/hash_filter_ops_test.rs`.
- Cross-language parity test: compare shard metadata and gradient outputs for same ids.

**Gaps / Notes**
- Tests depend on custom ops and TFRecord writer/reader compatibility.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hash_table_ops.py`
<a id="monolith-native-training-hash-table-ops-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 1208
- Purpose/role: Core TensorFlow hash table wrapper with custom ops for lookup, update, save/restore, and fused operations.
- Key symbols/classes/functions: `BaseHashTable`, `HashTable`, `hash_table_from_config`, `test_hash_table`, `fused_lookup`, `fused_apply_gradient`, checkpoint saver/restorer listeners.
- External dependencies: TensorFlow, custom ops `gen_monolith_ops`, `embedding_hash_table_pb2`, `hash_filter_ops`, `distributed_serving_ops`, `save_utils`, `graph_meta`.
- Side effects: Registers hash tables in graph collection, writes/reads checkpoint assets, registers proto serialization hooks.

**Required Behavior (Detailed)**
- `BaseHashTable` abstract API: `assign`, `assign_add`, `lookup`, `apply_gradients`, `as_op`, `dim_size`.
- `_HASH_TABLE_GRAPH_KEY` collection stores `HashTable` instances for save/restore.
- `HashTable`:
  - Ensures unique `shared_name` via metadata; raises if duplicate.
  - Wraps `monolith_hash_table_*` custom ops for assign/assign_add/lookup/lookup_entry/optimize/save/restore/size.
  - `apply_gradients` uses `monolith_hash_table_optimize` and returns a new table with control dependency.
  - `save_as_tensor` dumps entries as serialized `EntryDump` strings with sharding and offsets.
  - `to_proto`/`from_proto` serialize state via `hash_table_ops_pb2.HashTableProto` and `_BOOL_MAP`.
- `fused_lookup`:
  - Calls `monolith_hash_table_fused_lookup` and returns embeddings, recv_splits, id_offsets, emb_offsets.
- `fused_apply_gradient`:
  - Calls `monolith_hash_table_fused_optimize` with ids, indices, fused_slot_size, grads, offsets, learning rates, req_time, global_step.
- `hash_table_from_config`:
  - Builds table op using `EmbeddingHashTableConfig` and `HashTableConfigInstance`.
  - Chooses GPU vs CPU based on table type; forces SERVING entry type when exporting.
  - Creates hash filter and sync client if not provided.
- `test_hash_table` and `vocab_hash_table` helpers create simple tables for tests.
- `HashTableCheckpointSaverListener`:
  - Builds save ops using placeholders; writes asset files with randomized sleep to reduce metadata pressure.
- `HashTableCheckpointRestorerListener`:
  - Restores from latest checkpoint assets; supports sparse-only assets; uses thread pool to resolve prefixes.
- `HashTableRestorerSaverListener`:
  - Triggers restore after save (used for evicting stale entries).
- Registers proto serialization with `ops.register_proto_function`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-hash-table` + TF runtime wrappers in `monolith-rs/crates/monolith-tf`.
- Rust public API surface: hash table struct with assign/lookup/optimize, fused lookup/optimize wrappers, save/restore helpers.
- Data model mapping: use `monolith::hash_table` protos for config and serialization.
- Feature gating: TF custom ops required; Candle backend may implement a native hash table for local use.
- Integration points: feature factory, embedding gradients, distributed serving.

**Implementation Steps (Detailed)**
1. Implement a Rust hash table wrapper that mirrors assign/assign_add/lookup semantics and returns updated handles.
2. Add proto serialization helpers and maintain a registry for save/restore discovery.
3. Wrap fused lookup/optimize in TF runtime; add no-op stubs otherwise.
4. Implement checkpoint saver/restorer listeners and asset dir layout matching Python.
5. Add thread-pool based restore prefix matching for extra restore names.

**Tests (Detailed)**
- Python tests: `hash_table_ops_test.py`.
- Rust tests: `monolith-rs/crates/monolith-tf/tests/hash_table_ops_test.rs` (TF runtime) and native hash table unit tests.
- Cross-language parity test: compare lookup/update outputs and serialized dumps.

**Gaps / Notes**
- Heavy reliance on custom TF ops; full parity requires TF runtime + compiled ops.
- Save/restore path handling must match asset dir naming to avoid silent failures.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hash_table_ops_benchmark.py`
<a id="monolith-native-training-hash-table-ops-benchmark-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 148
- Purpose/role: Benchmarks hash table lookup and optimize paths (single-thread vs multi-thread).
- Key symbols/classes/functions: `HashTableOpsBenchmark` tests.
- External dependencies: TensorFlow, `hash_table_ops`.
- Side effects: Prints timing to stdout.

**Required Behavior (Detailed)**
- `test_lookup` / `test_lookup_multi_thread`:
  - Build table with len=10000, dim=32; assign ones for all but last 5 IDs.
  - Run lookup in a loop and validate embeddings (ones vs zeros).
- `test_basic_optimize` / `test_multi_threads_optimize` / `test_multi_threads_optimize_with_dedup`:
  - Build table with len=1,000,000; lookup, compute grads, apply gradients (optionally MT/dedup), print timing.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-examples/src/bin/hash_table_ops_benchmark.rs` (new).
- Rust public API surface: hash table benchmark harness.
- Feature gating: TF runtime + custom ops for parity.

**Implementation Steps (Detailed)**
1. Port benchmark loops to Rust, mirroring data sizes and operations.
2. Add CLI flags for MT/dedup variants.
3. Validate outputs in debug mode.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: none; benchmark only.
- Cross-language parity test: compare output correctness and rough timing.

**Gaps / Notes**
- Uses `tf.test.TestCase` for benchmarking; Rust should use criterion or custom timers.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hash_table_ops_test.py`
<a id="monolith-native-training-hash-table-ops-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 1206
- Purpose/role: Comprehensive tests for hash table lookup, update, save/restore, eviction, fused ops, and hooks.
- Key symbols/classes/functions: `HashTableOpsTest` methods, helper `test_hash_table_with_hash_filters`.
- External dependencies: TensorFlow, `hash_table_ops`, `hash_filter_ops`, `embedding_hash_table_pb2`, `learning_rate_functions`.
- Side effects: Writes checkpoint assets under `TEST_TMPDIR`.

**Required Behavior (Detailed)**
- Basic ops:
  - `test_basic` assigns values and verifies lookup and size; table name auto-unique.
  - `test_assign` verifies assign overwrites values under control dependencies.
  - `test_lookup_entry` parses `EntryDump` strings; missing ids return empty bytes.
  - `test_save_as_tensor` ensures serialized entries can be parsed.
  - `testNameConflict` duplicate shared_name raises `ValueError`.
- Gradient updates:
  - `test_gradients` updates embeddings with SGD (learning_rate=0.1) -> `[[0.2],[0.1]]` for ids [0,1].
  - `test_gradients_with_learning_rate_fn` accepts callable LR.
  - `test_gradients_with_learning_rate_decay` uses PolynomialDecay; expected outputs `[[0.04],[0.02]]`.
  - `test_gradients_with_dedup` enables dedup; expected outputs `[[0.3...],[0.2...]]` for vec_dim=10.
  - `test_gradients_with_different_ids` applies grads with mismatched ids -> `[[0.1],[0.2]]`.
  - `test_gradients_with_hash_filter` verifies occurrence threshold gating across repeated updates.
- Save/restore:
  - `test_save_restore` round-trips assign_add values.
  - `test_restore_from_another_table` uses extra_restore_names to restore.
- Feature eviction / TTL:
  - `test_save_restore_with_feature_eviction_assign_add` and `...apply_gradients` evict entries older than expire_time.
  - `test_entry_ttl_zero` evicts all entries on restore.
  - `test_entry_ttl_not_zero` preserves entries when TTL positive.
  - `test_entry_ttl_by_slots` uses slot_expire_time_config for per-slot TTL.
- Hooks and restore flows:
  - `test_restore_not_found` raises on missing checkpoint.
  - `test_save_restore_hook` saver + restorer hook restores after sub_op.
  - Additional tests validate restore-after-save, feature eviction with hooks, and cleanup of save paths.
- Advanced/fused ops:
  - `test_fused_lookup` and `test_fused_optimize` cover fused operations.
  - `test_batch_softmax_optimizer` verifies BatchSoftmax behavior.
  - `test_extract_fid` checks `extract_slot_from_entry`.
  - `test_meta_graph_export` ensures proto export/import works.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/tests/hash_table_ops_test.rs` (new).
- Rust public API surface: hash table operations, fused lookup/optimize, save/restore, eviction logic.
- Feature gating: TF runtime + custom ops required; skip otherwise.
- Integration points: hash filter ops, learning rate functions, save_utils equivalents.

**Implementation Steps (Detailed)**
1. Port basic lookup/assign/size tests to Rust TF runtime.
2. Implement gradient update tests for SGD and learning rate schedules.
3. Add save/restore tests with asset files; verify eviction by TTL and per-slot settings.
4. Add hook-based save/restore ordering tests.
5. Add fused op tests and meta-graph export/import tests.

**Tests (Detailed)**
- Python tests: `monolith/native_training/hash_table_ops_test.py`.
- Rust tests: `monolith-rs/crates/monolith-tf/tests/hash_table_ops_test.rs`.
- Cross-language parity test: compare serialized `EntryDump` bytes and lookup results.

**Gaps / Notes**
- Many tests depend on TF custom ops and checkpoint assets; may be too heavy for Rust CI.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hash_table_utils.py`
<a id="monolith-native-training-hash-table-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 46
- Purpose/role: Utility helpers to iterate hash tables and infer embedding dim sizes from config.
- Key symbols/classes/functions: `iterate_table_and_apply`, `infer_dim_size`.
- External dependencies: TensorFlow, `embedding_hash_table_pb2`.
- Side effects: Iterates table via `save_as_tensor` and calls `apply_fn`.

**Required Behavior (Detailed)**
- `iterate_table_and_apply(table, apply_fn, limit=1000, nshards=4, name="IterateTable")`:
  - Runs in `tf.function`.
  - Iterates `nshards` shards; for each shard, repeatedly calls `table.save_as_tensor(i, nshards, limit, offset)` until dump size < limit and offset != 0.
  - Uses `tf.autograph.experimental.set_loop_options` with shape invariants to allow dynamic `dump` size.
  - Calls `apply_fn(dump)` for each dump batch (serialized EntryDump strings).
- `infer_dim_size(config)`:
  - Sums `segment.dim_size` across `config.entry_config.segments`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-hash-table/src/utils.rs` (new) or `monolith-rs/crates/monolith-hash-table/src/lib.rs`.
- Rust public API surface: iterator over table dumps and dim-size inference from proto config.
- Data model mapping: `EmbeddingHashTableConfig` from `monolith-proto`.
- Feature gating: table iteration requires TF runtime or native hash table implementation.
- Integration points: model dump utilities and table export.

**Implementation Steps (Detailed)**
1. Implement a Rust iterator that pages through table dumps with `limit` and `offset` semantics.
2. Provide a safe callback API for applying functions to each batch.
3. Implement `infer_dim_size` by summing segment dims from proto.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: add unit tests for `infer_dim_size` and mock iteration semantics.
- Cross-language parity test: compare dim_size for a known config.

**Gaps / Notes**
- `iterate_table_and_apply` depends on `save_as_tensor` behavior; Rust must match sharding and offset semantics.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hash_table_utils_test.py`
<a id="monolith-native-training-hash-table-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 45
- Purpose/role: Tests `iterate_table_and_apply` paging across shards.
- Key symbols/classes/functions: `HashTableUtilsTest.test_iterate_table_and_apply`.
- External dependencies: TensorFlow, `hash_table_utils`, `hash_table_ops`.
- Side effects: Creates a test hash table and updates a counter variable.

**Required Behavior (Detailed)**
- Creates a test hash table with 100 ids.
- Uses `iterate_table_and_apply` with `limit=2` and `nshards=10` to iterate.
- `count_fn` increments a counter by `tf.size(dump)` for each batch.
- Final count must equal 100.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-hash-table/tests/hash_table_utils_test.rs` (new).
- Rust public API surface: table iteration helper and callback support.
- Feature gating: TF runtime or native hash table required.
- Integration points: hash table test helper equivalent.

**Implementation Steps (Detailed)**
1. Implement a Rust test that fills a table with 100 entries.
2. Iterate with small limit and shard count; accumulate total dumped entries.
3. Assert total count equals 100.

**Tests (Detailed)**
- Python tests: `monolith/native_training/hash_table_utils_test.py`.
- Rust tests: `monolith-rs/crates/monolith-hash-table/tests/hash_table_utils_test.rs`.
- Cross-language parity test: compare counts for same config.

**Gaps / Notes**
- Depends on `save_as_tensor` semantics; ensure Rust matches offset/limit behavior.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/ckpt_hooks.py`
<a id="monolith-native-training-hooks-ckpt-hooks-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 193
- Purpose/role: Checkpoint hooks for worker iterator state and barrier coordination during saves.
- Key symbols/classes/functions: `BarrierSaverListener`, `WorkerCkptHelper`, `assign_ckpt_info`, `get_ckpt_info`, `disable_iterator_save_restore`.
- External dependencies: TensorFlow, `barrier_ops`, `basic_restore_hook`, `graph_meta`, `ckpt_hooks_pb2`.
- Side effects: Creates local variables and placeholders, manipulates iterator saveables, blocks workers with barriers.

**Required Behavior (Detailed)**
- `_get_meta()`:
  - Lazily creates `WorkerCkptMetaInfo` local variable + placeholder + assign op.
- `assign_ckpt_info(session, info)`:
  - Assigns serialized `WorkerCkptInfo` to info_var.
- `get_ckpt_info(session)`:
  - Reads info_var and parses to `WorkerCkptInfo`.
- `BarrierSaverListener`:
  - On `before_save`, places a barrier and waits for workers to block (up to max_pending_seconds).
  - On `after_save`, removes barrier if it was placed by this listener.
- `WorkerCkptHelper`:
  - Creates iterator saveables (if enabled) and a Saver to save per-worker iterator state.
  - `create_save_iterator_callback` saves iterator checkpoints using current global_step from `WorkerCkptInfo`.
  - `create_restorer_hook` restores iterator state on session creation.
- `disable_iterator_save_restore()`:
  - Disables iterator save/restore globally (must be called before helper creation).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/hooks/ckpt_hooks.rs` (new).
- Rust public API surface: iterator checkpoint helper and barrier-based saver listener.
- Feature gating: TF runtime required for iterator saveables and session hooks.
- Integration points: training loops and distributed barrier coordination.

**Implementation Steps (Detailed)**
1. Implement worker checkpoint metadata storage and serialization.
2. Implement barrier saver listener with wait/remove semantics.
3. Implement iterator save/restore helper for dataset iterators.
4. Provide a global toggle to disable iterator save/restore.

**Tests (Detailed)**
- Python tests: `ckpt_hooks_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/ckpt_hooks_test.rs` (new).
- Cross-language parity test: verify iterator restore resumes from same element.

**Gaps / Notes**
- Uses TF iterator saveables and should be skipped if dataset iterators are not used in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/ckpt_hooks_test.py`
<a id="monolith-native-training-hooks-ckpt-hooks-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 181
- Purpose/role: Tests worker iterator save/restore and barrier-based saver listener behavior.
- Key symbols/classes/functions: `WorkerCkptHooksTest`, `CountCheckpointSaverListener`.
- External dependencies: TensorFlow, `ckpt_hooks`, `barrier_ops`, `save_utils`.
- Side effects: Writes checkpoints under `TEST_TMPDIR`, spawns a thread to run a monitored session.

**Required Behavior (Detailed)**
- `testIteratorSaveRestore`:
  - Saves iterator state at global_step=10 and restores; next element after restore matches expected value.
- `testNoCkpt`:
  - Restorer hook is a no-op when no checkpoint exists.
- `testNoSaveables`:
  - If no saveables, saving iterator state is skipped without error.
- `testCkptDisabled`:
  - `disable_iterator_save_restore()` prevents restore; iterator restarts from beginning.
- `test_saver_with_barrier`:
  - Verifies barrier placement during save and that saver listener callbacks are invoked in order.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/ckpt_hooks_test.rs` (new).
- Rust public API surface: barrier saver listener and worker iterator checkpoint helper.
- Feature gating: TF runtime required.
- Integration points: save_utils and barrier operations.

**Implementation Steps (Detailed)**
1. Add a Rust test dataset iterator and verify save/restore steps.
2. Add a test for disabled iterator restore.
3. Add a barrier coordination test ensuring save waits for workers.

**Tests (Detailed)**
- Python tests: `monolith/native_training/hooks/ckpt_hooks_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/ckpt_hooks_test.rs`.
- Cross-language parity test: compare iterator position after restore.

**Gaps / Notes**
- Threaded monitored session in test may be hard to replicate in Rust; can approximate with explicit calls.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/ckpt_info.py`
<a id="monolith-native-training-hooks-ckpt-info-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 98
- Purpose/role: Saves per-slot feature-id counts from hash tables into a `ckpt.info-*` file at checkpoint time.
- Key symbols/classes/functions: `FidSlotCountSaverListener`.
- External dependencies: TensorFlow, `hash_table_ops`, `hash_table_utils`, `ckpt_info_pb2`.
- Side effects: Writes `ckpt.info-<global_step>` to model_dir.

**Required Behavior (Detailed)**
- `FidSlotCountSaverListener.__init__(model_dir)`:
  - Collects hash tables from graph collections; errors if none exist.
  - Groups tables by device and allocates per-device count variables of size `_MAX_SLOT`.
  - Builds `iterate_table_and_apply` ops to accumulate slot counts.
- `before_save`:
  - Skips if multi-hash tables exist.
  - Initializes count vars, runs count op, sums counts across devices.
  - Writes `ckpt_info_pb2.CkptInfo` text to `ckpt.info-<step>`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/hooks/ckpt_info.rs` (new).
- Rust public API surface: saver listener that writes slot counts.
- Feature gating: TF runtime required for table iteration.
- Integration points: hash table iteration helper and checkpoint hooks.

**Implementation Steps (Detailed)**
1. Implement slot-count accumulation using table dump iteration.
2. Serialize `CkptInfo` via protobuf text format.
3. Write `ckpt.info-<step>` file during save.

**Tests (Detailed)**
- Python tests: `ckpt_info_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/ckpt_info_test.rs` (new).
- Cross-language parity test: compare output file contents for the same table.

**Gaps / Notes**
- `_MAX_SLOT` constant must match; slot extraction uses `extract_slot_from_entry` custom op.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/ckpt_info_test.py`
<a id="monolith-native-training-hooks-ckpt-info-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 45
- Purpose/role: Verifies that `FidSlotCountSaverListener` writes correct slot counts.
- Key symbols/classes/functions: `FidCountListener.test_basic`.
- External dependencies: TensorFlow, `hash_table_ops`, `ckpt_info`, `ckpt_info_pb2`.
- Side effects: Writes `ckpt.info-0` in temp dir.

**Required Behavior (Detailed)**
- Creates a hash table, assigns id 1, runs `before_save`.
- Reads `ckpt.info-0` and parses `CkptInfo`; `slot_counts[0] == 1`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/ckpt_info_test.rs` (new).
- Rust public API surface: slot count saver listener.
- Feature gating: TF runtime required.

**Implementation Steps (Detailed)**
1. Build a small hash table and add one entry.
2. Invoke the saver listener and read output file.
3. Parse `CkptInfo` and assert slot_counts[0] == 1.

**Tests (Detailed)**
- Python tests: `monolith/native_training/hooks/ckpt_info_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/ckpt_info_test.rs`.
- Cross-language parity test: compare text output files.

**Gaps / Notes**
- Slot extraction relies on `extract_slot_from_entry` custom op.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/controller_hooks.py`
<a id="monolith-native-training-hooks-controller-hooks-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 170
- Purpose/role: Controller hooks for stop/save signaling, barrier coordination, and file-based action queries.
- Key symbols/classes/functions: `ControllerHook`, `StopHelper`, `QueryActionHook`.
- External dependencies: TensorFlow, `barrier_ops`, `controller_hooks_pb2`, `utils.ps_device`.
- Side effects: Writes/reads action files under model_dir; places/removes barriers; triggers save callback.

**Required Behavior (Detailed)**
- `ControllerHook`:
  - Creates local `control_var=[False, False]` on ps0 device if `num_ps>0`.
  - `stop_op` assigns True to index 0; `trigger_save_op` assigns True to index 1; `reset_trigger_save_op` assigns False.
  - `before_run` requests `control_var`.
  - `after_run`:
    - If stop flag set: place barrier (action `STOP_ACTION`), wait up to 30s for all workers blocked, then remove barrier.
    - If trigger_save flag set: reset flag and call `_trigger_save` callback if provided.
- `StopHelper`:
  - Barrier callback sets internal `_should_stop` on STOP action.
  - `create_stop_hook` returns a hook that calls `request_stop` when `_should_stop`.
- `QueryActionHook`:
  - Polls `<model_dir>/monolith_action` every `QUERY_INTERVAL` seconds in a background thread.
  - Parses `ControllerHooksProto` from text; on TRIGGER_SAVE runs `hook.trigger_save_op`, on STOP runs `hook.stop_op`.
  - Writes response to `<model_dir>/monolith_action_response` and deletes query file.
  - On parse error, writes error to response.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/hooks/controller_hooks.rs` (new).
- Rust public API surface: controller hook, stop helper, file-based action polling hook.
- Feature gating: TF runtime for SessionRunHook equivalents; polling logic can be generic.
- Integration points: barrier ops, checkpoint save trigger, model_dir actions.

**Implementation Steps (Detailed)**
1. Implement a control flag shared variable (or atomic state) to signal stop/save.
2. Add barrier coordination for STOP action and wait-until-blocked logic.
3. Implement file polling for `monolith_action` and action parsing.
4. Wire trigger-save callback to the training loop/hook.

**Tests (Detailed)**
- Python tests: `controller_hooks_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/controller_hooks_test.rs` (new).
- Cross-language parity test: verify TRIGGER_SAVE and STOP paths behave as expected.

**Gaps / Notes**
- Python has a likely bug in `_write_resp` call with two args on unknown action; decide whether to fix or preserve behavior in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/controller_hooks_test.py`
<a id="monolith-native-training-hooks-controller-hooks-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 82
- Purpose/role: Tests ControllerHook stop/save behavior and QueryActionHook file polling.
- Key symbols/classes/functions: `ControllerHookTest`, `QueryActionHookTest`.
- External dependencies: TensorFlow, `barrier_ops`, `controller_hooks`.
- Side effects: Creates action files under temp model_dir.

**Required Behavior (Detailed)**
- `testStop`:
  - Uses `StopHelper` + `BarrierOp` with callback; runs `stop_op` and ensures session stops after a subsequent run if needed.
- `testSave`:
  - Trigger save op should invoke `trigger_save` exactly once.
- `QueryActionHookTest.testStop`:
  - Writes `monolith_action` file with `action: TRIGGER_SAVE` and waits for processing.
  - Confirms `trigger_save` called once.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/controller_hooks_test.rs` (new).
- Rust public API surface: controller hook and file polling hook.
- Feature gating: TF runtime if hooks are TF-based; otherwise simulate in test harness.

**Implementation Steps (Detailed)**
1. Implement a test harness to invoke stop/save flags and verify state transitions.
2. Add a file-based query test that writes `monolith_action` and checks callback execution.

**Tests (Detailed)**
- Python tests: `controller_hooks_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/controller_hooks_test.rs`.
- Cross-language parity test: compare stop/save action handling.

**Gaps / Notes**
- Timing-based file polling tests may need retries or longer timeouts in Rust CI.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/feature_engineering_hooks.py`
<a id="monolith-native-training-hooks-feature-engineering-hooks-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 99
- Purpose/role: Captures feature batches during training and dumps them as ExampleBatch protobuf files.
- Key symbols/classes/functions: `FeatureEngineeringSaveHook`.
- External dependencies: TensorFlow, `idl.matrix.proto.example_pb2` (ExampleBatch, FeatureListType).
- Side effects: Writes `.pb` files under `<model_dir>/features`.

**Required Behavior (Detailed)**
- `FeatureEngineeringSaveHook.__init__(config, nxt_elem, cap=100)`:
  - Stores config, next-element tensor, and buffer cap.
- `begin()`:
  - Initializes `_batch_list=[]` and `_steps=0`.
- `before_run`:
  - Increments step counter; returns `SessionRunArgs(nxt_elem)` after the first step (skips iterator init step).
- `after_run`:
  - Appends `run_values.results` to batch buffer; when buffer size reaches `cap`, calls `_save_features()` and clears buffer.
- `_save_features`:
  - Ensures `<model_dir>/features` exists.
  - Names output file as `chief_<uuid>.pb` for worker0, else `worker<index>_<uuid>.pb`.
  - Converts each batch dict into `ExampleBatch`:
    - Each key becomes a `named_feature_list` with type `INDIVIDUAL`.
    - RaggedTensorValue uses `to_list()`, ndarray uses `tolist()`.
    - If list entries are floats -> `float_list`, else `fid_v2_list`.
    - Sets `example_batch.batch_size` to len(lv).
  - Writes each serialized ExampleBatch preceded by two `<Q` headers: 0 (lagrange) and size.
- `end`:
  - Always attempts to save remaining batches (even empty list).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/hooks/feature_engineering_hooks.rs` (new).
- Rust public API surface: session hook that records feature batches and writes ExampleBatch files.
- Feature gating: TF runtime for SessionRunHook integration; feature serialization can be shared.
- Integration points: dataset iterators and model_dir outputs.

**Implementation Steps (Detailed)**
1. Define a hook that buffers batch feature maps and serializes to ExampleBatch protos.
2. Implement ragged vs dense conversion and output naming conventions.
3. Write files with lagrange header + size + protobuf bytes.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: add integration test with a small batch dict and validate file contents.
- Cross-language parity test: compare serialized ExampleBatch output for a fixed batch.

**Gaps / Notes**
- `end()` currently saves even when buffer is empty; decide whether to preserve this behavior in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/hook_utils.py`
<a id="monolith-native-training-hooks-hook-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 41
- Purpose/role: Thin wrappers to forward only before-save or after-save callbacks.
- Key symbols/classes/functions: `BeforeSaveListener`, `AfterSaveListener`.
- External dependencies: TensorFlow.
- Side effects: Delegates to wrapped listener.

**Required Behavior (Detailed)**
- `BeforeSaveListener`:
  - Stores a `CheckpointSaverListener` and only forwards `before_save`.
  - `__repr__` appends wrapped listener repr.
- `AfterSaveListener`:
  - Stores a listener and only forwards `after_save`.
  - `__repr__` appends wrapped listener repr.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/hooks/hook_utils.rs` (new).
- Rust public API surface: wrappers that forward only before/after save events.
- Feature gating: TF runtime only.

**Implementation Steps (Detailed)**
1. Implement wrapper types around saver listener traits.
2. Ensure only the intended callback is forwarded.

**Tests (Detailed)**
- Python tests: `hook_utils_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/hook_utils_test.rs` (new).
- Cross-language parity test: verify callbacks fire only for intended phase.

**Gaps / Notes**
- Python allows calling non-forwarded method without error (it just inherits default); Rust should match behavior or document deviations.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/hook_utils_test.py`
<a id="monolith-native-training-hooks-hook-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 35
- Purpose/role: Smoke test for BeforeSaveListener and AfterSaveListener wrappers.
- Key symbols/classes/functions: `HookUtilsTest.testBeforeAfterSaverListener`.
- External dependencies: TensorFlow, `hook_utils`.
- Side effects: None.

**Required Behavior (Detailed)**
- Wraps a base `CheckpointSaverListener` and calls before/after save methods to ensure no errors.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/hook_utils_test.rs` (new).
- Rust public API surface: hook wrapper types.
- Feature gating: TF runtime.

**Implementation Steps (Detailed)**
1. Instantiate wrapper types around a dummy listener.
2. Invoke before/after save methods and ensure no panic.

**Tests (Detailed)**
- Python tests: `hook_utils_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/hook_utils_test.rs`.
- Cross-language parity test: not required beyond smoke test.

**Gaps / Notes**
- This is a compile/smoke test only.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/ps_check_hooks.py`
<a id="monolith-native-training-hooks-ps-check-hooks-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 97
- Purpose/role: Health-check hooks for PS machines, reporting failures and placing barriers.
- Key symbols/classes/functions: `PsHealthCheckerHook`, `Config`, `get_ps_machine_info_shared_name`.
- External dependencies: TensorFlow, `logging_ops`, `barrier_ops`, `logging_ops_pb2`.
- Side effects: Spawns background thread, places barrier on failure, logs error details.

**Required Behavior (Detailed)**
- `get_ps_machine_info_shared_name(index)` returns `"ps_machine_info_<index>"`.
- `_default_report(results)`:
  - Logs per-PS MachineHealthResult using text_format one-line strings.
- `Config`:
  - Contains `barrier_op`, `num_ps`, `ps_device_fn` (default `utils.ps_device`), `report_fn` (default `_default_report`).
- `_PsHealthChecker`:
  - Builds `machine_info` and `check_machine_health` ops per PS device.
  - Runs in a daemon thread registered with coordinator.
  - If any status is non-empty, parses `MachineHealthResult`, calls report_fn, places barrier, and waits for stop.
  - Sleeps/waits via `coord.wait_for_stop(timeout=30)` in loop.
- `PsHealthCheckerHook`:
  - Creates checker on `begin()` and starts thread in `after_create_session`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/hooks/ps_check_hooks.rs` (new).
- Rust public API surface: PS health checker hook with background polling.
- Feature gating: TF runtime/custom ops required for machine health ops.
- Integration points: barrier ops and logging/alerting system.

**Implementation Steps (Detailed)**
1. Wrap logging_ops machine_info and health check ops in Rust TF runtime.
2. Spawn a background thread to poll health status.
3. On failure, call report_fn and place barrier, then request stop.

**Tests (Detailed)**
- Python tests: `ps_check_hooks_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/ps_check_hooks_test.rs` (new).
- Cross-language parity test: simulate healthy vs OOM conditions and verify report hook invocation.

**Gaps / Notes**
- Health status is encoded as serialized proto bytes; Rust must parse with `logging_ops_pb2` equivalent.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/ps_check_hooks_test.py`
<a id="monolith-native-training-hooks-ps-check-hooks-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 112
- Purpose/role: Tests PS health checker hook and error handling.
- Key symbols/classes/functions: `PsCheckHooksTest` cases.
- External dependencies: TensorFlow, `ps_check_hooks`, `logging_ops`.
- Side effects: Uses monitored sessions and sleeps briefly.

**Required Behavior (Detailed)**
- `test_basic`:
  - Healthy machine info should not trigger report.
- `test_oom`:
  - mem_limit=0 triggers report_fn once.
- `test_raise_in_after_create_session` / `test_raise_in_before_run`:
  - Raising in hooks should propagate DeadlineExceededError.
- `test_default_report`:
  - Calls `_default_report` with a MachineHealthResult for smoke coverage.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/ps_check_hooks_test.rs` (new).
- Rust public API surface: PS health checker hook.
- Feature gating: TF runtime/custom ops required.

**Implementation Steps (Detailed)**
1. Add a test harness that simulates healthy and unhealthy machine_info results.
2. Assert report function called under unhealthy case.
3. Verify exceptions propagate from hook callbacks.

**Tests (Detailed)**
- Python tests: `ps_check_hooks_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/ps_check_hooks_test.rs`.
- Cross-language parity test: compare report invocation counts.

**Gaps / Notes**
- Tests rely on custom logging_ops; may need to skip if ops unavailable.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/server/client_lib.py`
<a id="monolith-native-training-hooks-server-client-lib-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 30
- Purpose/role: gRPC client helper to connect to controller server from model_dir.
- Key symbols/classes/functions: `get_stub_from_model_dir`.
- External dependencies: `grpc`, TensorFlow gfile, generated `service_pb2_grpc`.
- Side effects: Reads controller server address file.

**Required Behavior (Detailed)**
- Reads `<model_dir>/controller_server_addr.txt`.
- Creates `grpc.insecure_channel(addr)` and returns `ControllerStub`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/hooks/server/client_lib.rs` (new).
- Rust public API surface: helper to read addr file and create gRPC client.
- Feature gating: gRPC required.

**Implementation Steps (Detailed)**
1. Read server addr file from model_dir.
2. Create gRPC channel and Controller client stub.

**Tests (Detailed)**
- Python tests: `server_lib_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/server_lib_test.rs` (integration).
- Cross-language parity test: ensure client can connect to server hook.

**Gaps / Notes**
- Assumes address file is present and readable.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/server/constants.py`
<a id="monolith-native-training-hooks-server-constants-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 15
- Purpose/role: Defines filename for controller server address.
- Key symbols/classes/functions: `SERVER_ADDR_FILENAME`.
- External dependencies: None.
- Side effects: None.

**Required Behavior (Detailed)**
- `SERVER_ADDR_FILENAME = "controller_server_addr.txt"`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/hooks/server/constants.rs` (new).
- Rust public API surface: constant for addr file name.

**Implementation Steps (Detailed)**
1. Define `SERVER_ADDR_FILENAME` constant.

**Tests (Detailed)**
- Python tests: covered indirectly in `server_lib_test.py`.
- Rust tests: none required beyond integration tests.

**Gaps / Notes**
- None.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/server/server_lib.py`
<a id="monolith-native-training-hooks-server-server-lib-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 95
- Purpose/role: gRPC controller server for training control (stop/resume/save/status).
- Key symbols/classes/functions: `ControllerServicer`, `ServerHook`.
- External dependencies: gRPC, TensorFlow, `barrier_ops`, `save_utils`, `net_utils`.
- Side effects: Starts a gRPC server, writes address file, triggers barrier ops and checkpoints.

**Required Behavior (Detailed)**
- `ControllerServicer`:
  - `StopTraining`: places barrier; if already placed, aborts with ALREADY_EXISTS.
  - `ResumeTraining`: removes barrier.
  - `GetBlockStatus`: returns blocked and unblocked indices for barrier.
  - `SaveCheckpoint`: calls `saver_hook.trigger_save`.
  - `GetTrainingStatus`: returns global_step from session.
- `ServerHook`:
  - On `after_create_session`, starts gRPC server on ephemeral port, writes addr to `<model_dir>/controller_server_addr.txt`.
  - On `end`, stops server (grace 20s).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/hooks/server/server_lib.rs` (new).
- Rust public API surface: Controller gRPC service + ServerHook.
- Feature gating: gRPC required; TF runtime for SessionRunHook lifecycle.
- Integration points: barrier ops, checkpoint saver hooks, training global_step.

**Implementation Steps (Detailed)**
1. Define gRPC service with Stop/Resume/Status/Save endpoints.
2. Start server in hook after session creation; write addr file.
3. Implement barrier operations and save trigger forwarding.

**Tests (Detailed)**
- Python tests: `server_lib_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/server_lib_test.rs`.
- Cross-language parity test: issue gRPC commands and verify barrier state changes.

**Gaps / Notes**
- Uses `net_utils.get_local_server_addr` for address formatting; Rust should mirror semantics.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/server/server_lib_test.py`
<a id="monolith-native-training-hooks-server-server-lib-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 54
- Purpose/role: Integration test for controller gRPC server and client helper.
- Key symbols/classes/functions: `ServerTest.test_basic`.
- External dependencies: TensorFlow, gRPC, `server_lib`, `client_lib`, `barrier_ops`, `save_utils`.
- Side effects: Starts server in monitored session.

**Required Behavior (Detailed)**
- Starts ServerHook and saver hook in a session.
- Uses client stub from model_dir to:
  - StopTraining (second StopTraining should raise RpcError).
  - GetBlockStatus shows blocked index then unblocked after ResumeTraining.
  - SaveCheckpoint and GetTrainingStatus succeed.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/server_lib_test.rs` (new).
- Rust public API surface: controller server hook and client stub.
- Feature gating: gRPC required.

**Implementation Steps (Detailed)**
1. Start controller server hook and saver hook in test session.
2. Use client to call gRPC methods and assert barrier behavior.

**Tests (Detailed)**
- Python tests: `server_lib_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/server_lib_test.rs`.
- Cross-language parity test: compare gRPC behavior and responses.

**Gaps / Notes**
- Needs a working saver hook to test SaveCheckpoint path.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/session_hooks.py`
<a id="monolith-native-training-hooks-session-hooks-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 44
- Purpose/role: Tracks the current TF session via a hook and provides a helper to fetch it.
- Key symbols/classes/functions: `SetCurrentSessionHook`, `get_current_session`.
- External dependencies: TensorFlow.
- Side effects: Stores session in a module-level singleton during hook lifetime.

**Required Behavior (Detailed)**
- `SetCurrentSessionHook.after_create_session` sets `_INFO.session`.
- `SetCurrentSessionHook.end` clears `_INFO.session`.
- `get_current_session()` returns `_INFO.session` if set, else `tf.compat.v1.get_default_session()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/hooks/session_hooks.rs` (new).
- Rust public API surface: session tracking hook + helper to fetch current session.
- Feature gating: TF runtime only.

**Implementation Steps (Detailed)**
1. Add a thread-safe global or TLS slot for current session.
2. Hook into session creation/end to set/clear.
3. Provide getter that falls back to default session.

**Tests (Detailed)**
- Python tests: `session_hooks_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/session_hooks_test.rs`.
- Cross-language parity test: ensure session is available inside monitored session and cleared after.

**Gaps / Notes**
- Global session state should be scoped carefully in multi-session environments.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hooks/session_hooks_test.py`
<a id="monolith-native-training-hooks-session-hooks-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 33
- Purpose/role: Smoke test for current-session tracking.
- Key symbols/classes/functions: `SessionHooksTest.testBasic`.
- External dependencies: TensorFlow, `session_hooks`.
- Side effects: None.

**Required Behavior (Detailed)**
- Asserts `get_current_session()` is None outside a session.
- Inside MonitoredSession with `SetCurrentSessionHook`, `get_current_session()` returns non-None.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/session_hooks_test.rs` (new).
- Rust public API surface: session tracking helper.
- Feature gating: TF runtime.

**Implementation Steps (Detailed)**
1. Add test that checks current session availability inside hook-managed session.
2. Ensure session is cleared after end.

**Tests (Detailed)**
- Python tests: `session_hooks_test.py`.
- Rust tests: `monolith-rs/crates/monolith-training/tests/session_hooks_test.rs`.
- Cross-language parity test: not required beyond smoke behavior.

**Gaps / Notes**
- None.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/hvd_lib.py`
<a id="monolith-native-training-hvd-lib-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 65
- Purpose/role: Lazy import wrapper for Horovod or BytePS TensorFlow libraries.
- Key symbols/classes/functions: `_Lib`, module-level `__getattr__`.
- External dependencies: `byteps.tensorflow` or `horovod.tensorflow`.
- Side effects: Imports the chosen library on first use.

**Required Behavior (Detailed)**
- `_Lib.enable_bps` reads `MONOLITH_WITH_BYTEPS` env var.
- `lib` property imports BytePS if enabled, else Horovod.
- Provides passthrough methods: `init`, `rank`, `size`, `allgather`, `broadcast`, `BroadcastGlobalVariablesHook`.
- Module-level `__getattr__` forwards to `_Lib` methods.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/hvd_lib.rs` (new).
- Rust public API surface: wrapper interface with lazy initialization for Horovod/BytePS bindings.
- Feature gating: only available under TF runtime / distributed features.

**Implementation Steps (Detailed)**
1. Implement lazy initialization with mutex/once for BytePS or Horovod bindings.
2. Expose helper methods mirroring Python names.
3. Read `MONOLITH_WITH_BYTEPS` to choose backend.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: add smoke tests to verify backend selection and lazy init.
- Cross-language parity test: ensure backend selection matches env.

**Gaps / Notes**
- Requires actual Horovod/BytePS bindings for Rust; otherwise should be stubbed with clear errors.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/input.py`
<a id="monolith-native-training-input-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 45
- Purpose/role: Utility to generate random FFM training examples.
- Key symbols/classes/functions: `slot_to_key`, `generate_ffm_example`.
- External dependencies: NumPy, TensorFlow.
- Side effects: None.

**Required Behavior (Detailed)**
- `slot_to_key(slot)` returns `"feature_<slot>"`.
- `generate_ffm_example(vocab_sizes, length=5)`:
  - Creates label feature with random int in [0,1) (effectively 0).
  - For each vocab size, samples `num_ids` in `[1, length]` and ids in a range offset by `max_vocab * i`.
  - Constructs a `tf.train.Example` and returns serialized bytes.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-data/src/input.rs` (new) or `monolith-rs/crates/monolith-examples` helpers.
- Rust public API surface: helper to generate serialized Example protos.
- Data model mapping: `monolith::io::proto::Example` or TensorFlow Example proto.

**Implementation Steps (Detailed)**
1. Implement `slot_to_key` helper.
2. Implement random example generation with identical id ranges.
3. Serialize Example protobuf to bytes.

**Tests (Detailed)**
- Python tests: used indirectly in estimator tests.
- Rust tests: add deterministic test with fixed RNG seed.
- Cross-language parity test: compare serialized Example with fixed seed.

**Gaps / Notes**
- Python uses `np.random.randint(low=0, high=1)` which always yields 0; consider whether to preserve this.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/__init__.py`
<a id="monolith-native-training-layers-init-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 46
- Purpose/role: Aggregates and re-exports Keras layers plus Monolith custom layers, then patches Keras layer classes with a `params()` helper for InstantiableParams construction.
- Key symbols/classes/functions:
  - Re-exported custom layers: `MLP`, `Dense` (custom override), `AddBias`, `LHUCTower`, `LogitCorrection`, `LayerNorm`, `GradNorm`, `SumPooling`, `AvgPooling`, `MaxPooling`, `MergeType`, `DCNType`, `MMoE`, `SNR`, plus everything from `feature_cross`, `feature_trans`, `feature_seq`, `advanced_activations`.
  - `keras_layers` dict: name → Keras layer class with `params` monkey-patched.
- External dependencies: `tensorflow` (`tf.keras.layers`, `Layer`), `types.MethodType`, `monolith.native_training.utils.params` (`params` helper).
- Side effects: Module import-time monkey-patching of Keras layer classes to inject `params()`; removal of wildcard-imported `Dense` symbol to replace with custom Dense.

**Required Behavior (Detailed)**
- Import order and namespace behavior:
  - `from tensorflow.keras.layers import *` makes all Keras layer classes available in module namespace.
  - `del globals()['Dense']` removes the Keras `Dense` symbol created by the wildcard import.
  - `from monolith.native_training.layers.dense import Dense` inserts the custom Dense in its place.
- Custom layer re-exports:
  - Re-exports layer modules so downstream Python code can do `from monolith.native_training.layers import X` for both Keras layers and custom layers.
- Keras layer patching:
  - Creates `keras_layers = {}`.
  - Iterates `dir(tf.keras.layers)`, skipping names that start with `_` or are exactly `"Layer"`.
  - For each candidate:
    - Retrieves `cls = getattr(tf.keras.layers, name)`.
    - If `issubclass(cls, Layer)` and `cls` does **not** already have `params`, then attaches `cls.params = MethodType(_params, cls)` and inserts `keras_layers[name] = cls`.
    - All errors in this process are swallowed (`except: pass`) to avoid import-time failures for non-class attributes or invalid `issubclass` checks.
  - Result: `keras_layers` includes only Keras layer classes that were patched.
- Error handling and determinism:
  - No explicit errors thrown; all reflection errors are suppressed.
  - Determinism is not relevant; import-time logic is pure reflection/patching.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/src/lib.rs` + `monolith-rs/crates/monolith-layers/src/prelude` (existing `prelude` module).
- Rust public API surface:
  - Provide a `prelude` module that re-exports Monolith layers analogous to Python's `__init__` aggregator.
  - Ensure `Dense` refers to the Monolith implementation (`monolith_layers::Dense`).
  - Expose `MergeType` and `DCNType` equivalents (`MergeType` already exists; map Python `DCNType` to Rust `DCNMode` or add an alias if necessary).
- Data model mapping:
  - Python's `params()` returns `InstantiableParams`; Rust should expose a `LayerParams`/`BuildConfig` trait that returns serializable config metadata per layer, or central registry entries.
- Feature gating:
  - If TF runtime backend is enabled (`cfg(feature = "tf-runtime")`), consider an optional registry for TensorFlow-native layers.
  - Otherwise, provide Monolith-only registry/prelude without TF/Keras dependencies.
- Integration points:
  - Downstream uses `monolith.native_training.layers` as a convenience import; in Rust this maps to `monolith_layers::prelude::*` or a `layers` module in the top-level crate.

**Implementation Steps (Detailed)**
1. Confirm `monolith_layers::prelude` exports all custom layers used in Python (`AddBias`, `MLP`, `LHUCTower`, `LogitCorrection`, `LayerNorm`, `GradNorm`, pooling layers, `MMoE`, `SNR`, feature cross/trans/seq layers).
2. Add missing exports or alias types in `monolith-rs/crates/monolith-layers/src/lib.rs` to mirror Python names (`DCNType` alias if needed).
3. Create an optional `LayerRegistry` (e.g., `HashMap<&'static str, LayerFactory>`) to mimic `keras_layers` if dynamic layer lookup is required; document that Python only registers patched Keras layers.
4. Implement a `LayerParams` trait or per-layer config builder to mirror Python `params()` (if used by config/codegen tooling).
5. Add a top-level `monolith-rs/crates/monolith/src/layers.rs` (or re-export from `monolith_layers`) to provide a single import path similar to Python.
6. Document that there is no exact Keras wildcard import equivalent; in Rust, this is replaced by explicit prelude exports and optional registry for dynamic construction.

**Tests (Detailed)**
- Python tests: None directly for `layers/__init__.py`.
- Rust tests:
  - Add a compile-time test that `monolith_layers::prelude::*` includes expected names (e.g., `Dense`, `MLP`, `AddBias`, pooling layers).
  - If a registry is added, include a test that registry contains expected layer names and no duplicates.
- Cross-language parity test:
  - (Optional) capture list of exported layer names in Python and compare to Rust prelude/registry list for overlap.

**Gaps / Notes**
- `keras_layers` is only populated for Keras classes missing `params`; if Rust introduces a registry, decide whether it should include only "patched" entries or the full set of Rust layers.
- Python explicitly suppresses all errors during reflection; Rust should avoid panicking on optional registry population.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/add_bias.py`
<a id="monolith-native-training-layers-add-bias-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 110
- Purpose/role: Keras layer that adds a learnable bias to inputs while handling `channels_first`/`channels_last` formats for 3D/4D/5D tensors.
- Key symbols/classes/functions: `AddBias(Layer)` with `build`, `call`, `get_config`.
- External dependencies: TensorFlow (`Layer`, `InputSpec`, `initializers`, `regularizers`, `tf.nn.bias_add`), Monolith utils (`get_ndim`, `int_shape`, `with_params`), layer utils (`check_dim`, `dim_size`), `monolith_export`.
- Side effects: Adds a trainable weight named `"bias"` during `build`; decorated with `@with_params` and `@monolith_export`.

**Required Behavior (Detailed)**
- Initialization:
  - `initializer = initializers.get(initializer) or tf.initializers.Zeros()`.
  - `regularizer = regularizers.get(regularizer)`.
  - `input_spec = InputSpec(min_ndim=2)`; `bias=None`.
- Build:
  - `shape = list(map(check_dim, input_shape[1:]))` (batch dim removed).
  - `check_dim(None) -> -1`, so unknown dims propagate to bias shape.
  - `self.add_weight(name='bias', shape=shape, dtype=tf.float32, initializer=initializer, regularizer=regularizer)`.
- Call:
  - `data_format = kwargs.get('data_format', 'channels_last')`.
  - Validate `data_format` is `"channels_first"` or `"channels_last"`; otherwise raise `ValueError('Unknown data_format: ' + str(data_format))`.
  - `bias_shape = int_shape(self.bias)` (tuple, `-1` for unknown dims).
  - If `len(bias_shape)` is not `1` and not `get_ndim(inputs) - 1`, raise:
    - `ValueError('Unexpected bias dimensions %d, expect to be 1 or %d dimensions' % (len(bias_shape), get_ndim(inputs)))`.
  - For `get_ndim(inputs) == 5`:
    - `channels_first`:
      - `len(bias_shape)==1`: reshape to `(1, C, 1, 1, 1)`.
      - `len(bias_shape)>1`: reshape to `(1, bias_shape[3]) + bias_shape[:3]`.
    - `channels_last`:
      - `len(bias_shape)==1`: reshape to `(1, 1, 1, C)`.
      - `len(bias_shape)>1`: reshape to `(1,) + bias_shape`.
  - For `get_ndim(inputs) == 4`:
    - `channels_first`:
      - `len(bias_shape)==1`: reshape to `(1, C, 1, 1)`.
      - `len(bias_shape)>1`: reshape to `(1, bias_shape[2]) + bias_shape[:2]`.
    - `channels_last`:
      - `len(bias_shape)==1`: `tf.nn.bias_add(inputs, bias, data_format='NHWC')`.
      - `len(bias_shape)>1`: reshape to `(1,) + bias_shape`.
  - For `get_ndim(inputs) == 3`:
    - `channels_first`:
      - `len(bias_shape)==1`: reshape to `(1, C, 1)`.
      - `len(bias_shape)>1`: reshape to `(1, bias_shape[1], bias_shape[0])`.
    - `channels_last`:
      - `len(bias_shape)==1`: reshape to `(1, 1, C)`.
      - `len(bias_shape)>1`: reshape to `(1,) + bias_shape`.
  - Else (2D or other): `tf.nn.bias_add(inputs, bias)`.
- Serialization:
  - `get_config()` serializes initializer/regularizer and merges with base config.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/src/add_bias.rs`.
- Rust public API surface: `AddBias`, `DataFormat`, `forward_with_format`, builder-style setters.
- Data model mapping:
  - Python `initializer` → Rust `Initializer`.
  - Python `regularizer` → Rust `Regularizer`.
  - Python `data_format` string → Rust `DataFormat`.
- Feature gating: None; pure Rust.
- Integration points: Re-export `AddBias` from `monolith_layers::prelude` for parity with Python `layers` aggregator.

**Implementation Steps (Detailed)**
1. Enforce Python error cases in Rust:
   - Reject bias shapes not equal to `1` or `ndim-1` with the same message text.
   - Reject invalid `data_format` string on parse.
2. Verify reshape permutations match Python for 3D/4D/5D (channels-first permutations).
3. Match `tf.nn.bias_add` semantics for 4D channels-last and 2D inputs (broadcast + dtype behavior).
4. Decide how to handle unknown dimensions (`-1` in Python) in Rust, document and test.
5. Add `LayerParams` metadata or config serialization to mirror `with_params` and `get_config`.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/add_bias_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/add_bias_test.rs` (new).
- Cross-language parity test:
  - Generate fixed bias + random tensors (3D/4D/5D) and compare outputs between Python and Rust.

**Gaps / Notes**
- Python allows `-1` dims in bias shape; Rust currently assumes known sizes.
- Python uses `tf.nn.bias_add` for 2D and 4D `channels_last`; Rust currently uses reshape + add.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/add_bias_test.py`
<a id="monolith-native-training-layers-add-bias-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 65
- Purpose/role: Unit tests for `AddBias` instantiation, serialization, and forward call in TF v1 session mode.
- Key symbols/classes/functions: `AddBiasTest.test_ab_instantiate`, `test_ab_serde`, `test_ab_call`.
- External dependencies: TensorFlow v1 session runtime, NumPy.
- Side effects: Uses `tf.compat.v1.disable_eager_execution()` in main guard; runs session initializers.

**Required Behavior (Detailed)**
- `test_ab_instantiate`:
  - Builds `layer_template = AddBias.params()`.
  - Copies params, sets `initializer = tf.initializers.Zeros()`, calls `instantiate()`.
  - Also instantiates with `AddBias(initializer=tf.initializers.Zeros())`.
  - Both constructions must succeed.
- `test_ab_serde`:
  - Instantiates via params.
  - `cfg = ins1.get_config()`, then `AddBias.from_config(cfg)`.
  - No assertion; should complete without error.
- `test_ab_call`:
  - Creates layer via params, sets `name` and `initializer`.
  - Creates variable input shape `(100, 10)` and computes `tf.reduce_sum(layer(data))`.
  - Runs in a session after `global_variables_initializer()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/add_bias_test.rs`.
- Rust public API surface: `AddBias` constructor/builder, serialization, forward call.
- Data model mapping:
  - `AddBias.params()` ↔ Rust config builder or `LayerParams` metadata.
  - `get_config`/`from_config` ↔ serde round-trip for `AddBias`.
- Feature gating: None.
- Integration points: `monolith_layers::AddBias`.

**Implementation Steps (Detailed)**
1. Add a constructor test for `AddBias::new()` and builder methods.
2. Add serde round-trip test for `AddBias` config.
3. Add forward pass test with deterministic input and bias; assert output sum.
4. Keep tests CPU-only; no TF runtime required.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/add_bias_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/add_bias_test.rs` (new).
- Cross-language parity test:
  - Compare output sum for fixed input/bias between Python and Rust.

**Gaps / Notes**
- Python tests are smoke tests without explicit assertions; Rust should add asserts for deterministic behavior.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/advanced_activations.py`
<a id="monolith-native-training-layers-advanced-activations-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 217
- Purpose/role: Defines activation layer wrappers, exports advanced activation classes, and provides `get/serialize/deserialize` helpers for activation identifiers.
- Key symbols/classes/functions:
  - Classes: `ReLU`, `LeakyReLU`, `ELU`, `Softmax`, `ThresholdedReLU`, `PReLU` (from TF), plus custom Layer wrappers `Tanh`, `Sigmoid`, `Sigmoid2`, `Linear`, `Gelu`, `Selu`, `Softsign`, `Softplus`, `Exponential`, `HardSigmoid`, `Swish`.
  - `get(identifier)`, `serialize(activation)`, `deserialize(identifier)`.
  - `__all__`, `__all_activations`, `ALL_ACTIVATION_NAMES`.
- External dependencies: TensorFlow Keras activations/layers, `types.MethodType`, Monolith `_params`, `monolith_export`.
- Side effects: Monkey-patches `params` method onto activation layer classes.

**Required Behavior (Detailed)**
- Class setup:
  - Defines lightweight `Layer` subclasses via `type(...)` for Tanh/Sigmoid/etc; each implements `call` with the corresponding TF activation function.
  - Adds `.params = MethodType(_params, cls)` for all activation classes listed (including TF-provided advanced activations).
- Export lists:
  - `__all__` includes names of all activation layers and wrappers.
  - `__all_activations` maps lowercase names (and synonyms like `hard_sigmoid`/`hardsigmoid`) to classes.
  - `ALL_ACTIVATION_NAMES = set(__all_activations.keys())`.
- `get(identifier)`:
  - `None` → `None`.
  - `str`:
    - If `identifier.lower()` in `__all_activations`: return a **new instance** of that class.
    - Else `eval(identifier)`; if dict, call `deserialize`; otherwise raise `TypeError`.
  - `dict`: call `deserialize`.
  - `callable`:
    - If has `params`, try `issubclass(identifier, Layer)`: if true, return new instance; else return identifier.
    - If `identifier` is a `Layer` instance: create new instance based on its class name.
    - Else try `identifier.__name__` and map to `__all_activations`; if not found return identifier.
  - Else: raise `TypeError('Could not interpret activation function identifier: ...')`.
- `serialize(activation)`:
  - Returns `repr(dict)` strings for known activation types; `None` otherwise.
  - Uses class-specific fields:
    - `LeakyReLU`/`ELU`: `alpha`.
    - `ReLU`: `max_value`, `negative_slope`, `threshold`.
    - `PReLU`: `alpha_initializer`, `alpha_regularizer`, `alpha_constraint`, `shared_axes` (uses `initializers.serialize` for all three in Python).
    - `Softmax`: `axis`.
    - `ThresholdedReLU`: `theta`.
- `deserialize(identifier)`:
  - Accepts dict or string repr of dict (via `eval`).
  - Requires `name` key; lowercases and looks up in `__all_activations`, pops `name`, and instantiates class with remaining kwargs.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/src/activation.rs` and `activation_layer.rs`.
- Rust public API surface:
  - Activation structs (ReLU, LeakyReLU, ELU, Softmax, ThresholdedReLU, PReLU, Tanh, Sigmoid, Sigmoid2, Linear, GELU, SELU, Softplus, Softsign, Swish, Exponential, HardSigmoid).
  - `ActivationType` enum (in `mlp.rs`) and `ActivationLayer` wrapper for dynamic dispatch.
  - Add `activation::get`, `activation::serialize`, `activation::deserialize` or a dedicated registry module to mirror Python behavior.
- Data model mapping:
  - Python string identifiers ↔ Rust `ActivationType` (case-insensitive, include synonyms).
  - Python repr(dict) ↔ Rust serde JSON/YAML or explicit struct for config; if parity requires, accept Python-style dict strings.
- Feature gating: None.
- Integration points: `MLP` and any layer configs that accept activations.

**Implementation Steps (Detailed)**
1. Add a Rust activation registry mapping lowercased names and synonyms to constructors.
2. Implement `get(identifier)` variants:
   - Accept `&str`, `ActivationType`, or config struct; return `ActivationLayer`.
3. Implement `serialize` to return a Python-compatible dict representation (or document accepted Rust-native format and add translation).
4. Implement `deserialize` that can accept Python-style `repr(dict)` if needed for cross-language parity.
5. Ensure `params`-like metadata exists for activation classes (if using config builders).

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/advanced_activations_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/advanced_activations_test.rs` (new).
- Cross-language parity test:
  - For each name in `ALL_ACTIVATION_NAMES`, call `get` and compare forward outputs on fixed input.

**Gaps / Notes**
- Python uses `eval` on identifier strings for deserialize; Rust should avoid eval and instead accept a safe format, but parity requires handling Python-style repr strings.
- Python uses `initializers.serialize` for `alpha_regularizer`/`alpha_constraint` in `PReLU` serialization; verify whether this is a bug and whether Rust should mirror it.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/advanced_activations_test.py`
<a id="monolith-native-training-layers-advanced-activations-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 84
- Purpose/role: Exercises `advanced_activations.get`/`serialize` with identifiers and ensures activation layers run in a TF session.
- Key symbols/classes/functions: `serde`, `all_acts`, `raw_acts`, `lay_acts`, `ActivationsTest`.
- External dependencies: TensorFlow Keras activations/layers, TF v1 session mode.
- Side effects: Disables eager execution in main guard; runs session with variable initialization.

**Required Behavior (Detailed)**
- `serde(act)`:
  - `_act = get(act)`, `sered_act = serialize(_act)`, then `get(sered_act)` must succeed.
- `all_acts` list defines string identifiers for names to test.
- `raw_acts` list of Keras activation functions exists but is unused in tests.
- `lay_acts` list includes Keras layer instances (`ReLU`, `PReLU`, `ThresholdedReLU`, `ELU`, `Softmax`, `LeakyReLU`).
- Tests:
  - `test_get_from_str`: calls `serde` for each name in `all_acts`.
  - `test_get_from_layers`: calls `serde` for each layer instance in `lay_acts`.
  - `test_get_from_func`: loops `lay_acts` again (likely intended `raw_acts` but uses layers).
  - `test_params`: for each name, calls `cls = get(act).__class__` then `cls.params()`.
  - `test_call`: creates input `(100, 200)`, applies `get(act)` to each name, sums and evaluates in a session.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/advanced_activations_test.rs`.
- Rust public API surface: activation registry `get`, `serialize`, config params metadata.
- Data model mapping:
  - `all_acts` strings → Rust name lookup (`ActivationType` or registry).
  - `serialize` output → Rust serialization format (must accept Python repr strings if parity required).
- Feature gating: None.
- Integration points: `monolith_layers::activation` and `ActivationLayer`.

**Implementation Steps (Detailed)**
1. Add Rust tests to cover name lookup for all `all_acts` names.
2. Add serde round-trip test for each activation type.
3. Add test to ensure `params`/config metadata exists for each activation class.
4. Add forward test applying all activations to a fixed tensor and summing outputs.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/advanced_activations_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/advanced_activations_test.rs` (new).
- Cross-language parity test:
  - Use identical inputs and compare output sums per activation name.

**Gaps / Notes**
- `test_get_from_func` likely intended to use `raw_acts` but currently uses `lay_acts`; Rust tests should mirror the current behavior, not the intent.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/agru.py`
<a id="monolith-native-training-layers-agru-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 295
- Purpose/role: Implements an Attention GRU (AGRU/AUGRU) cell and helper functions for static/dynamic RNN with attention scores.
- Key symbols/classes/functions: `AGRUCell`, `create_ta`, `static_rnn_with_attention`, `dynamic_rnn_with_attention`.
- External dependencies: TensorFlow internals (`rnn_cell_impl`, `tensor_array_ops`, `control_flow_ops`, `array_ops`, `math_ops`, `nn_ops`), Keras `Layer`, `InputSpec`, activations/initializers/regularizers, Monolith utils (`with_params`, `check_dim`, `dim_size`).
- Side effects: Creates trainable weights (`gates/*`, `candidate/*`) with initializer/regularizer; uses TensorArray and TF while_loop for dynamic RNN.

**Required Behavior (Detailed)**
- `AGRUCell` initialization:
  - `units` required; `att_type` must be `"AGRU"` or `"AUGRU"` (case-insensitive).
  - `activation = activations.get(activation or math_ops.tanh)`.
  - `initializer = tf.initializers.get(initializer) or tf.initializers.HeNormal()`.
  - `regularizer = regularizers.get(regularizer)`.
  - `input_spec` requires 3 inputs: `(x, state, att_score)`; `x` and `state` are 2D, `att_score` max_ndim=2.
- `build(inputs_shape)`:
  - `input_shape, state_shape, att_shape = inputs_shape`.
  - Assert `state_shape[-1] == units`.
  - `input_depth = check_dim(input_shape[-1])`; if `input_shape[-1] == -1`, raise `ValueError("Expected inputs.shape[-1] to be known, saw shape: ...")`.
  - Create weights:
    - `_gate_kernel`: shape `[input_depth + units, 2 * units]`.
    - `_gate_bias`: shape `[2 * units]`, initializer `Ones`.
    - `_candidate_kernel`: shape `[input_depth + units, units]`.
    - `_candidate_bias`: shape `[units]`, initializer `Ones`.
- `call((x, state, att_score))`:
  - `gate_inputs = matmul(concat([x, state], 1), _gate_kernel)`, bias add.
  - `value = sigmoid(gate_inputs)`; split into `r, u`.
  - `candidate = matmul(concat([x, r * state], 1), _candidate_kernel)`; bias add; `c = activation(candidate)`.
  - If `att_score is None`: standard GRU update: `(1 - u) * state + u * c`.
  - Else if `att_type == "AUGRU"`:
    - `u = (1 - att_score) * u`.
    - `new_h = u * state + (1 - u) * c`.
  - Else (`AGRU`):
    - `new_h = (1 - att_score) * state + att_score * c`.
  - Returns `(new_h, new_h)` (output and new state).
- `zero_state(batch_size, dtype)`:
  - In eager mode, caches last zero state to avoid recomputation.
  - Uses `_zero_state_tensors` from TF rnn_cell_impl with `backend.name_scope`.
- `get_config()` serializes `units`, `att_type`, `initializer`, `activation`, `regularizer`.
- `create_ta(name, size, dtype)` returns `TensorArray`.
- `static_rnn_with_attention(cell, inputs, att_scores, init_state=None)`:
  - `cell` must be `AGRUCell`.
  - If `init_state` is None, uses `cell.get_initial_state` if available, else `cell.zero_state`.
  - Transposes inputs to time-major, loops in Python, calls cell per time step with `att_scores[:, time]` reshaped to `(-1, 1)`.
  - Returns stacked outputs `(batch, time, hidden)` and final state.
  - Note: uses `dtype` variable in `get_initial_state` branch, but `dtype` is undefined (bug in Python).
- `dynamic_rnn_with_attention(cell, inputs, att_scores, parallel_iterations=1, swap_memory=True, init_state=None)`:
  - Same initialization rules as static (same undefined `dtype` issue).
  - Uses TensorArray + `control_flow_ops.while_loop`.
  - Outputs stacked and transposed back to batch-major.
  - Sets static shape `[None, time_steps, dim_size(outputs, -1)]`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/src/agru.rs`.
- Rust public API surface:
  - Rust `AGRU` struct exists but is not a TF-style cell; needs parity with `AGRUCell` and with sequence outputs.
  - Add `AGRUCell`-like API (`forward_step`, `zero_state`, `state_size`, `output_size`).
  - Add `static_rnn_with_attention` / `dynamic_rnn_with_attention` helpers (pure Rust loops).
- Data model mapping:
  - Python `att_type` (`AGRU`/`AUGRU`) → Rust enum.
  - Python `activation` → Rust activation layer or function.
  - Python `initializer`/`regularizer` → Rust `Initializer`/`Regularizer`.
- Feature gating: None (pure Rust implementation).
- Integration points: DIEN/sequence models that expect AGRU outputs.

**Implementation Steps (Detailed)**
1. Extend `monolith_layers::agru` to support both AGRU and AUGRU update formulas and optional `att_score=None` (standard GRU).
2. Add `AGRUCell` struct mirroring Python weight shapes and bias initialization (gate/candidate splits).
3. Implement `static_rnn_with_attention`:
   - Accept inputs `[batch, time, dim]`, attention `[batch, time]`, optional initial state.
   - Return outputs for all timesteps and final state.
4. Implement `dynamic_rnn_with_attention`:
   - In Rust, this is a loop, but preserve behavior and output shape.
5. Match error semantics for unknown input depth; enforce input_dim known at build.
6. Add config serialization for `AGRUCell` (units, att_type, initializer, activation, regularizer).

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/agru_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/agru_test.rs` (new).
- Cross-language parity test:
  - Fix weights, input, attention; compare per-timestep outputs for AGRU and AUGRU modes.

**Gaps / Notes**
- Python `static_rnn_with_attention` and `dynamic_rnn_with_attention` reference `dtype` without definition; decide whether to mimic or correct in Rust.
- Rust `AGRU` currently returns only final hidden state and uses a different attention update formula; needs alignment.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/agru_test.py`
<a id="monolith-native-training-layers-agru-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 112
- Purpose/role: Smoke tests for `AGRUCell` instantiation, serde, and static/dynamic attention RNN helpers.
- Key symbols/classes/functions: `AGRUTest` methods `test_agru_instantiate`, `test_agru_serde`, `test_agru_call`, `test_agru_static_rnn_call`, `test_agru_dynamic_rnn_call`.
- External dependencies: TensorFlow v1 session mode, NumPy.
- Side effects: Disables eager execution and v2 behavior in main guard.

**Required Behavior (Detailed)**
- `test_agru_instantiate`:
  - Uses `AGRUCell.params()` to build `InstantiableParams`.
  - Sets `units=10`, `activation=sigmoid`, `initializer=GlorotNormal`, then `instantiate()`.
  - Also constructs directly with `AGRUCell(units=10, activation=sigmoid, initializer=HeUniform)`.
  - Both instantiations must succeed.
- `test_agru_serde`:
  - `cfg = AGRUCell(...).get_config()` then `AGRUCell.from_config(cfg)` must succeed.
- `test_agru_call`:
  - Inputs: `data` shape `(100, 100)`, `state` shape `(100, 10)`, `attr` shape `(100, 1)`.
  - Calls `layer((data, state, attr))`, gets `(output, new_state)`; sums `new_state`.
  - Runs in a session after variable initialization.
- `test_agru_static_rnn_call`:
  - Inputs: `data` shape `(100, 20, 10)`, `attr` shape `(100, 20)`.
  - Calls `static_rnn_with_attention`, receives `(outputs, final_state)`.
  - Sums `final_state` (not the full outputs).
- `test_agru_dynamic_rnn_call`:
  - Inputs: random `data` shape `(100, 20, 10)` and `attr` shape `(100, 20)`.
  - Calls `dynamic_rnn_with_attention`, receives `(outputs, final_state)` and sums `final_state`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/agru_test.rs`.
- Rust public API surface: `AGRUCell` (or equivalent), `static_rnn_with_attention`, `dynamic_rnn_with_attention`.
- Data model mapping:
  - Params-based instantiation ↔ Rust config/builder.
  - `get_config`/`from_config` ↔ serde round-trip.
- Feature gating: None.
- Integration points: `monolith_layers::agru`.

**Implementation Steps (Detailed)**
1. Add Rust tests for params/builder instantiation and serde round-trip.
2. Add forward step test for `AGRUCell` with fixed inputs, compare output sum.
3. Add static and dynamic RNN tests; validate final state sum against Python.
4. Mirror test input shapes and attention shapes from Python.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/agru_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/agru_test.rs` (new).
- Cross-language parity test:
  - Fix weights and inputs and compare final state sums for static/dynamic helpers.

**Gaps / Notes**
- Python tests do not assert exact values; Rust tests should add deterministic assertions.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/dense.py`
<a id="monolith-native-training-layers-dense-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 307
- Purpose/role: Custom Dense layer with optional kernel normalization, optimizer attachment to variables, partitioned variable support, and inactive-ReLU monitoring.
- Key symbols/classes/functions: `Dense(Layer)` with `add_weight`, `get_variable`, `build`, `call`, `compute_output_shape`, `get_config`.
- External dependencies: TensorFlow/Keras internals (`core_ops.dense`, `variable_ops.PartitionedVariable`, `base_layer_utils`, `InputSpec`, `K.track_variable`, `tensor_shape`), Monolith utils (`with_params`, `get_uname`).
- Side effects:
  - Attaches `.optimizer` attribute to variables created.
  - Tracks split variables in layer trainable/non-trainable weights.
  - Emits summary histogram for inactive ReLU counts when enabled.

**Required Behavior (Detailed)**
- Initialization:
  - If `input_shape` not provided and `input_dim` is, convert to `input_shape=(input_dim,)`.
  - Sets `units`, `activation=activations.get(activation)`, `use_bias`, `kernel_initializer`, `bias_initializer`, `kernel_regularizer`, `bias_regularizer`, `allow_kernel_norm`, `kernel_norm_trainable`, `partitioner`, `inactive_relu_monitor`, `inactive_relu_monitor_decay`, `optimizer`.
  - `input_spec = InputSpec(min_ndim=2)`, `supports_masking=True`.
- `add_weight` override:
  - Calls `super().add_weight(...)`, then sets `var.optimizer = self.optimizer`.
  - If `PartitionedVariable`, set optimizer on each shard.
- `get_variable` helper:
  - Wraps `tf.compat.v1.get_variable` inside current name scope with AUTO_REUSE.
  - Sets optimizer on variable(s).
  - Manually tracks variables with `K.track_variable` and appends to `_trainable_weights` or `_non_trainable_weights`, including split/partitioned variables.
- `build(input_shape)`:
  - Ensures dtype is floating/complex; otherwise `TypeError("Unable to build `Dense` layer with non-floating point dtype %s")`.
  - Requires last dimension known; otherwise `ValueError("The last dimension of the inputs to `Dense` should be defined. Found `None`.")`.
  - Creates kernel variable using `get_variable` seeded by `kernel_initializer` output.
  - If `allow_kernel_norm`:
    - Normalize kernel with `tf.nn.l2_normalize(axis=0, epsilon=1e-6)`.
    - If `kernel_norm_trainable`, create `trainable_kernel_norm` initialized with `tf.linalg.norm(init_kernel, axis=0)` and multiply normalized kernel by it.
  - If `use_bias`, add bias weight; else `bias=None`.
  - If `inactive_relu_monitor` and activation is ReLU:
    - Create non-trainable `inactive_relu_count_moving_avg` variable under `METRIC_VARIABLES` and `GLOBAL_VARIABLES`.
- `call(inputs)`:
  - Uses `core_ops.dense(inputs, kernel, bias, activation, dtype=compute_dtype)`.
  - If `inactive_relu_monitor`:
    - `inactive_relu_count = units - count_nonzero(output, axis=0)`.
    - Logs histogram `inactive_relu_count_moving_avg`.
    - Updates moving average with decay and uses control dependencies.
- `compute_output_shape`:
  - Requires last dim defined; otherwise `ValueError("The innermost dimension of input_shape must be defined, but saw: %s")`.
  - Output shape = input_shape[:-1] + units.
- `get_config`:
  - Serializes units, activation, use_bias, initializers, regularizers, `allow_kernel_norm`, `kernel_norm_trainable`, `partitioner`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/src/dense.rs`.
- Rust public API surface:
  - `Dense` currently implements a linear layer with optional kernel norm (no activation in the layer).
  - Add a `DenseConfig` or wrapper to include activation and inactive-ReLU monitoring.
- Data model mapping:
  - Python `activation` → Rust `ActivationType` or `ActivationLayer` (in `mlp.rs`/`activation_layer.rs`).
  - Python `kernel_norm_trainable` → Rust `kernel_norm_trainable`.
  - Python `partitioner` and `optimizer` do not exist in Rust; require explicit non-TF equivalents or document omission.
- Feature gating: None.
- Integration points: MLP and any model configs that reference `Dense` directly.

**Implementation Steps (Detailed)**
1. Decide parity approach:
   - Option A: Add activation inside `Dense` (match Python call signature).
   - Option B: Keep linear `Dense` and ensure all call sites add `ActivationLayer` explicitly (document difference).
2. Add kernel norm behavior to match TF:
   - Normalize weights along axis 0, epsilon 1e-6.
   - If trainable, scale by per-output norm.
3. Add inactive ReLU monitoring equivalent (optional):
   - Track per-unit zero counts and exponential moving average; integrate with Rust metrics/logging.
4. Add config serialization to include activation, kernel/bias initializers, regularizers, allow_kernel_norm, kernel_norm_trainable.
5. Mirror error messages for invalid input dtypes and missing last dimension (as close as Rust allows).

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/dense_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/dense_test.rs` (new).
- Cross-language parity test:
  - Fix weights/bias and compare outputs for activation on/off and kernel_norm modes.

**Gaps / Notes**
- Python attaches `.optimizer` to TF variables and supports partitioned variables; Rust has no equivalent.
- Python Dense includes activation inside the layer; Rust `Dense` is linear-only today.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/dense_test.py`
<a id="monolith-native-training-layers-dense-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 147
- Purpose/role: Tests Dense instantiation, serialization, forward, kernel norm, inactive-ReLU monitoring, and variable partitioning.
- Key symbols/classes/functions: `DenseTest` methods `test_dense_instantiate`, `test_dense_serde`, `test_dense_call`, `test_dense_kernel_norm_call`, `test_inactive_relu_monitor`, `test_dense_with_explicit_partition`, `test_dense_with_implicit_partition`.
- External dependencies: TensorFlow v1 session mode, NumPy.
- Side effects: Uses graph collections and variable partitioning.

**Required Behavior (Detailed)**
- `test_dense_instantiate`:
  - Builds `Dense.params()` template, sets `units=100`, `activation=sigmoid`, `kernel_initializer=GlorotNormal`, instantiates.
  - Also constructs `Dense(...)` directly; both must succeed.
- `test_dense_serde`:
  - Instantiates via params, calls `get_config`, and `Dense.from_config(cfg)`.
- `test_dense_call`:
  - Creates Dense with sigmoid activation, input `(100, 100)` ones; sums output and runs session.
- `test_dense_kernel_norm_call`:
  - Dense with `allow_kernel_norm=True`, `kernel_norm_trainable=True`; runs forward without errors.
- `test_inactive_relu_monitor`:
  - Dense with `activation=relu` and `inactive_relu_monitor=True`.
  - After calling on a constant input, asserts graph contains node name `Dense/inactive_relu_count_moving_avg_1`.
- `test_dense_with_explicit_partition`:
  - Dense with explicit `partitioner` and kernel_norm enabled.
  - Input shape `(100, 294)`; validates output shape `(100, 1024)`.
  - Collects per-shard kernel dims (expected `[59, 59, 59, 59, 58]` but not asserted).
- `test_dense_with_implicit_partition`:
  - Uses `variable_scope` partitioner; Dense with `partitioner=None` to inherit scope.
  - Verifies kernel shard dims equal `[59, 59, 59, 59, 58]`.
  - Validates output shape `(100, 1024)`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/dense_test.rs`.
- Rust public API surface: `Dense`, activation handling, kernel norm config.
- Data model mapping:
  - Params-based instantiation ↔ Rust builder/config.
  - `get_config`/`from_config` ↔ serde round-trip.
- Feature gating: None.
- Integration points: `monolith_layers::dense`.

**Implementation Steps (Detailed)**
1. Add Rust tests for constructor + config serde round-trip.
2. Add forward tests for base Dense and kernel_norm-enabled Dense.
3. If activation is moved out of Dense in Rust, adapt tests to apply activation separately but keep parity cases documented.
4. Decide how to mirror partitioner behavior:
   - If not supported, add explicit test that documents the unsupported feature.
5. Add inactive-ReLU monitoring metrics if implemented; otherwise document absence.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/dense_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/dense_test.rs` (new).
- Cross-language parity test:
  - Fixed weights/bias and compare output sums for kernel_norm on/off.

**Gaps / Notes**
- Python partitioner behavior has no Rust equivalent; needs explicit parity plan or documented limitation.
- The expected partition shard sizes are implicit to TF partitioner; Rust may not replicate.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/feature_cross.py`
<a id="monolith-native-training-layers-feature-cross-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 805
- Purpose/role: Collection of feature-crossing layers: GroupInt/FFM, AllInt, CDot, CAN, DCN, CIN.
- Key symbols/classes/functions: `GroupInt` (alias `FFM`), `AllInt`, `CDot`, `CAN`, `DCN`, `CIN`.
- External dependencies: TensorFlow/Keras (`Layer`, `Conv1D`, activations/initializers/regularizers), Monolith layers (`MLP`), layer utils (`merge_tensor_list`, `DCNType`, `check_dim`, `dim_size`), `layer_ops.ffm`, TF internals (`variable_ops.PartitionedVariable`, `base_layer_utils`, `K.track_variable`).
- Side effects: Creates multiple trainable weights and nested Keras layers (MLP, Conv1D); uses TF variable tracking for split variables; logs via `absl.logging` (imported).

**Required Behavior (Detailed)**
- `GroupInt` (aka `FFM`):
  - Inputs: tuple `(left_fields, right_fields)` where each is list of tensors.
  - Concats left/right along axis=1, then calls `ffm(left, right, dim_size, int_type)`.
  - `interaction_type` in `{'multiply', 'dot'}`; `use_attention` only valid for `multiply`.
  - If `use_attention`: reshape to `(bs, num_feature, emb_dim)`, run MLP to get attention `(bs, num_feature, 1)`, apply elementwise weighting; output reshaped to `(bs, num_feature * emb_dim)`.
  - Returns list `[ffm_embeddings]` if `keep_list` else tensor.
  - Config includes interaction_type, attention_units, activation, initializer, regularizer, out_type, keep_list.
- `AllInt`:
  - Inputs: `embeddings` shape `[batch, num_feat, emb_size]`.
  - Builds kernel shape `(num_feat, cmp_dim)` and optional bias `(cmp_dim,)`.
  - Call: transposes embeddings to `[batch, emb_size, num_feat]`, computes `feature_comp = transposed @ kernel` (+bias), then `interaction = embeddings @ feature_comp` to get `[batch, num_feat, cmp_dim]`.
  - Returns `merge_tensor_list(interaction, merge_type=out_type, keep_list=keep_list)`.
- `CDot`:
  - Build: stores `_num_feature`, `_emd_size`, creates `project_weight` `(num_feature, project_dim)` and `compress_tower` MLP with output dims `compress_units + [emd_size * project_dim]`.
  - Call:
    - Project input: `(bs, emb_size, num_feature) @ project_weight` → `(bs, emb_size, project_dim)`.
    - Flatten and run compress MLP → `compressed` `(bs, emb_size * project_dim)`.
    - Cross: `inputs @ reshape(compressed, (bs, emb_size, project_dim))` → `(bs, num_feature, project_dim)`, flatten to `(bs, num_feature * project_dim)`.
    - Output: `concat([crossed, compressed], axis=1)`.
- `CAN`:
  - Inputs: `(user_emb, item_emb)`.
  - `item_emb` is split into alternating weight/bias tensors for `layer_num` layers; expects size `u_emb_size*(u_emb_size+1) * layer_num`.
  - Handles four shape cases based on `is_seq` and `is_stacked`, reshaping weights/bias accordingly.
  - Applies `layer_num` iterations of `user_emb = activation(user_emb @ weight + bias)` (or linear if activation is None).
  - Output reduces/squeezes based on `is_seq/is_stacked`.
- `DCN`:
  - Supports types: `Vector`, `Matrix`, `Mixed` (from `DCNType`).
  - `Vector`: kernel shape `(dim,1)` per layer; update `xl = x0 * (xl @ w) + b + xl`.
  - `Matrix`: kernel shape `(dim,dim)` per layer; update `xl = x0 * (xl @ W + b) + xl`.
  - `Mixed`: per layer, per-expert low-rank factors `U,V,C` (dims `dim x low_rank`), gating `G` (`dim x 1`), bias; computes expert outputs and softmax-gated mixture; adds residual `+ xl`.
  - Optional `allow_kernel_norm` in `get_variable`: normalizes var (axis=0, eps=1e-6) and multiplies by trainable norm initialized with `tf.norm(var_init, axis=0)`.
  - Optional dropout during `TRAIN` mode: `tf.nn.dropout(xl, rate=1-keep_prob)`.
- `CIN`:
  - Inputs: `[batch, num_feat, emb_size]`, uses `Conv1D` per layer (`hidden_uints`).
  - For each layer: compute `zl = einsum('bdh,bdm->bdhm', xl, x0)`, reshape to `(bs, emb_size, last_hidden_dim * num_feat)`, apply Conv1D.
  - Concatenate `reduce_sum` of each layer output along emb_size: `concat([sum(hi, axis=1) ...], axis=1)`.

**Rust Mapping (Detailed)**
- Target crate/module:
  - `monolith-rs/crates/monolith-layers/src/feature_cross.rs` (GroupInt/AllInt/CDot/CAN/CIN).
  - `monolith-rs/crates/monolith-layers/src/dcn.rs` (DCN).
- Rust public API surface:
  - `GroupInt`, `AllInt`, `CDot`, `CAN`, `CIN` in `feature_cross.rs`.
  - `CrossNetwork`/`CrossLayer` in `dcn.rs` for DCN variants.
- Data model mapping:
  - Python `DCNType` → Rust `DCNMode` (vector/matrix/mixed).
  - `interaction_type` → Rust `GroupIntType`.
  - `merge_tensor_list` → Rust `merge_tensor_list` / `MergeType`.
- Feature gating: GPU-accelerated paths in Rust when `cuda`/`metal` features enabled (if used).
- Integration points: MLP, merge utils, embedding and pooling layers.

**Implementation Steps (Detailed)**
1. Verify each Python layer has a Rust counterpart with matching defaults and shapes.
2. Align `GroupInt` attention path with Python’s MLP attention (last dim must be 1).
3. Ensure `AllInt` and `CDot` matmul/reshape orderings match Python exactly.
4. Implement CAN’s `is_seq` / `is_stacked` shape logic and weight/bias splitting.
5. Map DCN modes and kernel norm behavior; match gating and mixture logic for `Mixed`.
6. Ensure CIN’s einsum/reshape/Conv1D logic matches; if Conv1D is missing, emulate with 1x1 conv via matmul.
7. Add config serialization parity for each layer (activation, initializer, regularizer, units, dims).

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/feature_cross_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/feature_cross_test.rs` (new).
- Cross-language parity test:
  - Fix small inputs and compare outputs for each layer variant (GroupInt, AllInt, CDot, CAN, DCN, CIN).

**Gaps / Notes**
- Python uses TF internals for split variable tracking in DCN kernel_norm path; Rust does not have a direct analogue.
- Some layers (e.g., CDot/CIN) depend on Keras `Conv1D`; ensure Rust kernel shapes/stride/activation match.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/feature_cross_test.py`
<a id="monolith-native-training-layers-feature-cross-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 286
- Purpose/role: Smoke tests for feature crossing layers (GroupInt/AllInt/CDot/CAN/DCN/CIN).
- Key symbols/classes/functions: `FeatureCrossTest` methods for instantiate/serde/call per layer.
- External dependencies: TensorFlow v1 session mode, NumPy.
- Side effects: Disables v2 behavior in main guard; runs TF sessions.

**Required Behavior (Detailed)**
- GroupInt:
  - Instantiate via params and direct constructor.
  - `test_groupint_call`: left list of 5 tensors `(100,10)`, right list of 3 tensors `(100,10)`.
  - `test_groupint_attention_call`: same shapes with attention MLP.
- AllInt:
  - Instantiate/serde with `cmp_dim=4`.
  - Call on input `(100,10,10)`.
- CDot:
  - Instantiate/serde with `project_dim=8`, `compress_units=[128,256]`, `activation='tanh'`.
  - Call on input `(100,10,10)`.
- CAN:
  - Instantiate/serde with `layer_num=8`.
  - `test_can_seq_call`: user `(128,10,12,10)`, item `(128,220)`.
  - `test_can_call`: user `(128,10,10)`, item `(128,220)`.
- DCN:
  - Instantiate/serde for `dcn_type='matrix'`, `use_dropout=True`, `keep_prob=0.5`.
  - Call for vector/matrix/mixed modes; input `(128,10,10)`, kernel_norm enabled.
- CIN:
  - Instantiate/serde with `hidden_uints=[10,5]`, activation configured.
  - Call on input `(128,10,10)`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/feature_cross_test.rs`.
- Rust public API surface: `GroupInt`, `AllInt`, `CDot`, `CAN`, `CIN`, `DCN` equivalents.
- Data model mapping:
  - Params-based instantiation ↔ Rust config/builder.
  - `get_config`/`from_config` ↔ serde round-trip.
- Feature gating: None.
- Integration points: `monolith_layers::feature_cross` and `monolith_layers::dcn`.

**Implementation Steps (Detailed)**
1. Add Rust tests for each layer’s constructor and config serialization.
2. Add forward tests with same input shapes as Python.
3. For DCN, include tests for vector/matrix/mixed modes with kernel_norm on.
4. For CAN, enforce item size consistency with `layer_num`/`u_emb_size`.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/feature_cross_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/feature_cross_test.rs` (new).
- Cross-language parity test:
  - Fix weights and inputs; compare output sums per layer.

**Gaps / Notes**
- Python tests are smoke tests; Rust should add deterministic assertions.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/feature_seq.py`
<a id="monolith-native-training-layers-feature-seq-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 361
- Purpose/role: Sequence feature models: DIN attention, DIEN interest evolution, DMR_U2I sequence matching.
- Key symbols/classes/functions: `DIN`, `DIEN`, `DMR_U2I`.
- External dependencies: TensorFlow/Keras (`Layer`, `Dense`, `GRUCell`, activations/initializers/regularizers), Monolith layers (`MLP`, `AGRUCell`, `dynamic_rnn_with_attention`), `monolith_export`, `with_params`.
- Side effects: Adds nested layer weights/regularization losses; uses TF summary in DIN when mask provided.

**Required Behavior (Detailed)**
- `DIN`:
  - Inputs: `queries` `[B,H]`, `keys` `[B,T,H]`; optional `mask` in kwargs.
  - Builds MLP (`dense_tower`) with `hidden_units` (last dim must be 1).
  - Call:
    - Tile `queries` to `[B,T,H]`.
    - `din_all = concat([q, k, q-k, q*k], axis=-1)` -> `[B,T,4H]`.
    - `attention_weight = dense_tower(din_all)` -> `[B,T,1]`.
    - If `decay`, divide by `sqrt(H)`.
    - If `mask`: zero out masked positions and emit summary histogram `{name}_attention_outputs`.
    - If `mode == 'sum'`: `attention_out = matmul(attention_weight, keys, transpose_a=True)` -> `[B,1,H]`, squeeze to `[B,H]`.
    - Else: elementwise `keys * attention_weight` -> `[B,T,H]`.
- `DIEN`:
  - Builds:
    - GRUCell for interest extraction (`gru_cell`).
    - AGRUCell (`augru_cell`) for interest evolution (note: `att_type` argument exists but build hard-codes `att_type='AGRU'`).
    - Attention weight matrix `weight` `(num_units, num_units)`.
  - `_attention(queries, keys)`:
    - `query_weight = matmul(queries, weight, transpose_b=True)` reshape to `[B, H, 1]`.
    - `logit = squeeze(matmul(keys, query_weight))` -> `[B,T]`.
    - `softmax(logit)` returns attention scores.
  - `call`:
    - Accepts `queries` and `keys` from args/kwargs (mask optional but unused).
    - `outputs = dynamic_rnn(gru_cell, keys)` -> `[B,T,H]`.
    - `attn_scores = _attention(queries, outputs)` -> `[B,T]`.
    - `_, final_state = dynamic_rnn_with_attention(augru_cell, outputs, attn_scores)`.
    - Returns `final_state` `[B,H]`.
- `DMR_U2I`:
  - Build: `pos_emb (seq_len, cmp_dim)`, `emb_weight (ue_size, cmp_dim)`, `z_weight (cmp_dim,1)`, `bias (cmp_dim)`, `linear Dense` to `ie_size`.
  - Call:
    - `emb_cmp = user_seq @ emb_weight`.
    - `comped = pos_emb + emb_cmp + bias`.
    - `alpha = softmax(comped @ z_weight, axis=1)` -> `[B,seq_len,1]`.
    - `user_seq_merged = squeeze(transpose(user_seq) @ alpha)` -> `[B, ue_size]`.
    - `user_seq_merged = linear(user_seq_merged)` -> `[B, ie_size]`.
    - Output `user_seq_merged * items` (elementwise).

**Rust Mapping (Detailed)**
- Target crate/module:
  - `monolith-rs/crates/monolith-layers/src/din.rs` (DIN).
  - `monolith-rs/crates/monolith-layers/src/dien.rs` (DIEN).
  - `monolith-rs/crates/monolith-layers/src/dmr.rs` (DMR_U2I).
- Rust public API surface:
  - `DINAttention`/`DINConfig`, `DIENLayer`/`DIENConfig`, `DMRU2I`.
- Data model mapping:
  - Python `mode` (`sum` vs elementwise) → Rust `DINOutputMode`.
  - Activation strings → Rust `ActivationType`.
  - AGRU/GRU cell configs → Rust GRU/AUGRU implementations.
- Feature gating: None.
- Integration points: AGRU, MLP, Dense, activation registry.

**Implementation Steps (Detailed)**
1. Verify DIN attention math and mask handling match Python (including decay scaling and summary logging).
2. Ensure DIEN uses the same attention formula; decide whether to respect Python’s `att_type` parameter (currently ignored).
3. Align DIEN to use AGRU vs AUGRU to match Python behavior.
4. Implement DMR_U2I using Dense + activation as in Python, including position embeddings.
5. Add config serialization for all three layers.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/feature_seq_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/feature_seq_test.rs` (new).
- Cross-language parity test:
  - Fixed inputs and weights; compare outputs for DIN (sum/elementwise), DIEN, and DMR_U2I.

**Gaps / Notes**
- DIEN’s `att_type` argument is not used in build (hard-coded `'AGRU'`); decide whether to mirror this bug or fix with a flag.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/feature_seq_test.py`
<a id="monolith-native-training-layers-feature-seq-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 126
- Purpose/role: Smoke tests for DIN, DIEN, and DMR_U2I layers.
- Key symbols/classes/functions: `FeatureSeqTest` methods for instantiate/serde/call.
- External dependencies: TensorFlow v1 session mode, NumPy.
- Side effects: Runs TF sessions; eager disabled in main guard.

**Required Behavior (Detailed)**
- DIN:
  - Instantiate via params and direct constructor (`hidden_units=[10,1]`).
  - `test_din_call`: query `(100,10)`, keys `(100,15,10)`.
- DIEN:
  - Instantiate via params and direct constructor (`num_units=10`).
  - `test_dien_call`: query `(100,10)`, keys `(100,15,10)`.
- DMR_U2I:
  - Instantiate via params and direct constructor (`cmp_dim=10`, `activation='relu'`).
  - `test_dmr_call`: query `(100,10)`, keys `(100,15,10)`.
- All tests compute sum of outputs and run session initialization.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/feature_seq_test.rs`.
- Rust public API surface: `DINAttention`/`DINConfig`, `DIENLayer`, `DMRU2I`.
- Data model mapping:
  - Params-based instantiation ↔ Rust config/builder.
  - `get_config`/`from_config` ↔ serde round-trip.
- Feature gating: None.
- Integration points: `monolith_layers::din`, `monolith_layers::dien`, `monolith_layers::dmr`.

**Implementation Steps (Detailed)**
1. Add Rust tests for constructor and config serialization for each layer.
2. Add forward tests with the same input shapes.
3. Add deterministic assertions (output shapes and sums) for parity.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/feature_seq_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/feature_seq_test.rs` (new).
- Cross-language parity test:
  - Fix weights and inputs; compare output sums for DIN, DIEN, DMR_U2I.

**Gaps / Notes**
- Python tests do not assert numeric values; Rust should add explicit assertions.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/feature_trans.py`
<a id="monolith-native-training-layers-feature-trans-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 340
- Purpose/role: Feature transformation layers: AutoInt (self-attention), iRazor (feature/embedding dimension selection), SeNet (feature re-weighting).
- Key symbols/classes/functions: `AutoInt`, `iRazor`, `SeNet`.
- External dependencies: TensorFlow/Keras (`Layer`, `InputSpec`, initializers/regularizers), Monolith (`MLP`, `add_layer_loss`, `merge_tensor_list`, `with_params`, `check_dim`/`dim_size`).
- Side effects: Emits TF summary histogram for iRazor NAS weights; adds auxiliary loss via `add_layer_loss`.

**Required Behavior (Detailed)**
- `AutoInt`:
  - Input shape `[B, num_feat, emb_dim]` (3D).
  - For `layer_num` iterations:
    - `attn = softmax(autoint_input @ autoint_input^T)` → `[B, num_feat, num_feat]`.
    - `autoint_input = attn @ autoint_input` → `[B, num_feat, emb_dim]`.
  - Output via `merge_tensor_list` with `out_type` and `keep_list`.
- `iRazor`:
  - `nas_space` defines embedding dimension groups; `rigid_masks` is a constant `[nas_len, emb_size]` with grouped 1s.
  - Build: `nas_logits` weight shape `(num_feat, nas_len)`.
  - Call:
    - `nas_weight = softmax(nas_logits / t)`; histogram summary `"nas_weight"`.
    - `soft_masks = nas_weight @ rigid_masks` → `[num_feat, emb_size]`.
    - If `feature_weight` provided, compute `nas_loss = feature_weight @ sum(soft_masks, axis=1)` and call `add_layer_loss`.
    - `out_embeds = embeds * soft_masks`.
    - Return `merge_tensor_list(out_embeds, out_type, keep_list)`.
- `SeNet`:
  - If inputs is a tensor `[B, num_feat, emb_dim]`: `sequeeze_embedding = reduce_mean(inputs, axis=2)`.
  - If inputs is list of tensors:
    - `on_gpu=True`: use `segment_sum` on concatenated embeddings and lens to compute means.
    - Else: `sequeeze_embedding = concat([reduce_mean(embed, axis=1)] ...)`.
  - `cmp_tower` MLP outputs feature weights of shape `[B, num_feat]`.
  - For tensor input: reshape weights to `[B, num_feat, 1]` and multiply.
  - For list input: split weights and multiply per tensor.
  - Output via `merge_tensor_list` with `num_feature`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/src/feature_trans.rs` (AutoInt, IRazor, SeNet).
- Rust public API surface:
  - `AutoInt` + config, `IRazor`, `SENetLayer` (if present) or equivalent.
- Data model mapping:
  - `out_type` → `MergeType`.
  - `feature_weight` auxiliary loss → Rust loss registry or explicit return.
- Feature gating: None; GPU optimization optional.
- Integration points: merge utils, MLP, loss aggregation.

**Implementation Steps (Detailed)**
1. Align AutoInt attention math and merge semantics.
2. Implement iRazor rigid/soft mask logic and optional feature-weighted auxiliary loss.
3. Implement SeNet for both tensor and list inputs; include `on_gpu` optimization if possible.
4. Add config serialization for each layer (including `nas_space`, `t`, `cmp_dim`, `num_feature`).

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/feature_trans_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/feature_trans_test.rs` (new).
- Cross-language parity test:
  - Fix embeddings and weights; compare outputs for AutoInt, iRazor, SeNet.

**Gaps / Notes**
- Python iRazor uses `add_layer_loss` to register aux loss; Rust needs an equivalent or explicit API.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/feature_trans_test.py`
<a id="monolith-native-training-layers-feature-trans-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 140
- Purpose/role: Smoke tests for AutoInt, SeNet, and iRazor layers.
- Key symbols/classes/functions: `FeatureTransTest` methods for instantiate/serde/call.
- External dependencies: TensorFlow v1 session mode, NumPy.
- Side effects: Runs TF session after variable init.

**Required Behavior (Detailed)**
- AutoInt:
  - Instantiate via params and direct constructor (`layer_num=1`).
  - `test_autoint_call`: input `(100,10,10)`, `layer_num=2`.
- SeNet:
  - Instantiate/serde with `num_feature=10`, `cmp_dim=4`, custom initializers.
  - `test_senet_call`: input `(100,10,10)`.
- iRazor:
  - Instantiate/serde with `nas_space=[0,2,5,7,10]`, `t=0.08`.
  - `test_irazor_call`: input `(100,10,10)`.
- All tests sum outputs and run session initialization.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/feature_trans_test.rs`.
- Rust public API surface: `AutoInt`, `IRazor`, `SeNet` equivalents.
- Data model mapping:
  - Params-based instantiation ↔ Rust config/builder.
  - `get_config`/`from_config` ↔ serde round-trip.
- Feature gating: None.
- Integration points: `monolith_layers::feature_trans`.

**Implementation Steps (Detailed)**
1. Add Rust tests for constructor and config serialization for each layer.
2. Add forward tests with the same input shapes.
3. Add deterministic assertions on output shapes/sums.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/feature_trans_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/feature_trans_test.rs` (new).
- Cross-language parity test:
  - Fix weights and inputs; compare output sums for AutoInt, SeNet, iRazor.

**Gaps / Notes**
- Python tests are smoke tests without numeric assertions; Rust should add explicit checks.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/layer_ops.py`
<a id="monolith-native-training-layers-layer-ops-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 131
- Purpose/role: Python wrappers around custom TensorFlow ops (FFM, FeatureInsight, MonolithFidCounter) and their gradients.
- Key symbols/classes/functions: `ffm`, `feature_insight`, `fid_counter` and registered gradients `_ffm_grad`, `_feature_insight`, `_fid_counter_grad`.
- External dependencies: `gen_monolith_ops` custom op library, TensorFlow gradient registry.
- Side effects: Registers gradients for custom ops; uses TF summary in downstream layers.

**Required Behavior (Detailed)**
- `ffm(left, right, dim_size, int_type='multiply')`:
  - Calls `layer_ops_lib.FFM` with given attrs and returns the op output.
  - `int_type` determines multiply vs dot behavior.
- `_ffm_grad` (gradient for `FFM`):
  - Calls `layer_ops_lib.FFMGrad` with `grad`, `left`, `right`, `dim_size`, `int_type`.
  - Returns `(left_grad, right_grad)`.
- `feature_insight(input_embedding, weight, segment_sizes, aggregate=False)`:
  - Asserts `segment_sizes` provided and `input_embedding.shape[-1] == weight.shape[0]`.
  - Calls `FeatureInsight` custom op.
  - If `aggregate=True`:
    - Builds `segment_ids` of length `k * num_feature` where `k = weight.shape[-1]`.
    - Returns `transpose(segment_sum(transpose(out * out), segment_ids))`.
  - Else returns `out` directly.
- `_feature_insight` (gradient for `FeatureInsight`):
  - Calls `FeatureInsightGrad` with `grad`, `input`, `weight`, `segment_sizes`, `K`.
  - Returns gradients for input_embedding and weight.
- `fid_counter(counter, counter_threshold, step=1.0)`:
  - Calls `MonolithFidCounter` op with `counter`, `step`, `counter_threshold`.
  - Adds `step` to counter, then clamps at threshold.
  - Docstring notes counter slice should use `SgdOptimizer(1.0)` and suggests `Fp32Compressor`.
- `_fid_counter_grad`:
  - Gradient is `-step` (as a constant) until `counter >= counter_threshold`, then zero.

**Rust Mapping (Detailed)**
- Target crate/module: custom ops would live in `monolith-rs/crates/monolith-tf` (TF runtime) or in native Rust layers (for pure Rust path).
- Rust public API surface:
  - Provide equivalents for `ffm`, `feature_insight`, `fid_counter` if needed by higher-level layers.
- Data model mapping:
  - `int_type` → Rust enum for multiply/dot.
  - `segment_sizes` and `aggregate` → explicit API arguments.
- Feature gating: TF runtime only for custom ops unless reimplemented in Rust.
- Integration points: `feature_cross.GroupInt` uses `ffm`; other code may rely on `feature_insight` or `fid_counter`.

**Implementation Steps (Detailed)**
1. Decide whether to reimplement `FFM`, `FeatureInsight`, and `MonolithFidCounter` in Rust or provide TF-runtime wrappers.
2. If TF runtime is used, expose safe Rust bindings and gradient equivalents.
3. For pure Rust path, implement `ffm` and its backward, plus feature_insight/ fid_counter logic.
4. Add tests for forward and backward (if training supported).

**Tests (Detailed)**
- Python tests: None dedicated in this file (covered by layer tests).
- Rust tests: `monolith-rs/crates/monolith-layers/tests/layer_ops_test.rs` (new, if implemented).
- Cross-language parity test:
  - Compare FFM outputs/gradients and fid_counter behavior between Python and Rust.

**Gaps / Notes**
- `feature_insight` and `fid_counter` depend on custom TF ops; Rust currently lacks equivalents.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/layer_ops_test.py`
<a id="monolith-native-training-layers-layer-ops-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 232
- Purpose/role: Validates custom ops (FFM, FeatureInsight, MonolithFidCounter) across CPU/GPU and checks gradients.
- Key symbols/classes/functions: `LayerOpsTest` methods `test_ffm_mul`, `test_ffm_mul_grad`, `test_ffm_dot`, `test_ffm_dot_grad`, `test_feature_insight`, `test_feature_insight_grad`, `test_fid_counter_grad`.
- External dependencies: TensorFlow GPU test utilities, custom ops via `layer_ops`.
- Side effects: Forces GPU contexts when available; uses global `tf.random.set_seed(0)`.

**Required Behavior (Detailed)**
- `test_ffm_mul`:
  - Uses `ffm(left, right, dim_size=4)` with `left` shape `(8,40)` and `right` `(8,48)` (10*4 and 12*4).
  - Checks GPU device placement if GPU available; compares CPU and GPU outputs.
  - Expects output shape `(8, 480)` for multiply mode.
- `test_ffm_mul_grad`:
  - Computes gradients of sum of FFM output wrt left/right.
  - Expects left_grad shape `(8,40)` and right_grad `(8,48)`; CPU and GPU grads equal.
- `test_ffm_dot`:
  - Uses `int_type='dot'`.
  - Expects output shape `(8,120)`; CPU and GPU outputs equal.
- `test_ffm_dot_grad`:
  - Same gradient checks as multiply, output dims unchanged.
- `test_feature_insight`:
  - Builds expected result by splitting input/weights per `segment_sizes=[3,2,4]`, matmul per segment, then optional aggregate using segment_sum of squared outputs.
  - Calls `layer_ops.feature_insight(..., aggregate=True)` and asserts close to expected.
- `test_feature_insight_grad`:
  - Compares gradients of `feature_insight` output vs explicit matmul concatenation.
  - Asserts outputs and gradients match.
- `test_fid_counter_grad`:
  - Verifies fid_counter increments and gradient = `-step` until threshold, then 0 at threshold.
  - Checks counter values for step=1, step=0.01, and threshold case at 1000.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/layer_ops_test.rs` (if ops reimplemented) or `monolith-rs/crates/monolith-tf/tests` for TF-runtime bindings.
- Rust public API surface: FFM op (multiply/dot), FeatureInsight, fid_counter equivalents.
- Data model mapping:
  - Output shapes match Python expectations (multiply: `B * (L*R*D)` flattened, dot: `B * (L*R)`).
  - Gradients should match analytic gradients of FFM/FeatureInsight.
- Feature gating: GPU tests optional; must be skipped if GPU backend unavailable.
- Integration points: `feature_cross` uses FFM.

**Implementation Steps (Detailed)**
1. Implement Rust tests mirroring CPU/GPU parity (skip GPU when not supported).
2. Add gradient checks for FFM and FeatureInsight if backward is implemented.
3. Add fid_counter unit test that verifies saturation and gradient behavior.
4. Ensure deterministic seeding for random tensors.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/layer_ops_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/layer_ops_test.rs` (new).
- Cross-language parity test:
  - Compare outputs and gradients for FFM multiply/dot and FeatureInsight aggregate mode.

**Gaps / Notes**
- Python tests rely on custom TF ops and GPU placement; Rust needs an equivalent implementation or explicit skip/feature gate.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/lhuc.py`
<a id="monolith-native-training-layers-lhuc-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 296
- Purpose/role: LHUCTower: augments a dense tower with LHUC gating MLPs per layer; supports shared or per-layer LHUC configs, optional batch normalization, and weight norm.
- Key symbols/classes/functions: `LHUCTower`, `lhuc_params`, `build`, `call`, `get_config`, `from_config`.
- External dependencies: TensorFlow/Keras (`Layer`, `BatchNormalization`, `Sequential`, regularizers), Monolith layers (`MLP`, `Dense`), `extend_as_list`, `advanced_activations`.
- Side effects: Creates nested Dense/MLP layers and BatchNorm; collects trainable/non-trainable weights; supports LHUC-specific overrides via `lhuc_*` kwargs.

**Required Behavior (Detailed)**
- Initialization:
  - Splits kwargs into `_lhuc_kwargs` with `lhuc_` prefix; remaining kwargs passed to base Layer.
  - `output_dims` defines dense tower layers; `n_layers = len(output_dims)`.
  - `activations`:
    - None → `[relu]*(n_layers-1) + [None]`.
    - List/tuple length must match `n_layers`; maps via `ad_acts.get`.
    - Single activation string/function → same for all but last layer (None).
  - `initializers` expanded to list length `n_layers` via `extend_as_list`.
  - LHUC output dims:
    - If `lhuc_output_dims` is list of lists: each last dim must equal corresponding `output_dims[i]`.
    - If list of ints: applied to every layer and auto-append `[dim]`.
    - Else default: `[[dim] for dim in output_dims]`.
  - `lhuc_activations`: for each LHUC MLP, uses `relu` for all but last, last is `sigmoid2`.
- `build`:
  - Optional input BatchNorm if `enable_batch_normalization`.
  - For each layer:
    - Create `Sequential` block with Dense (custom monolith Dense) + optional BatchNorm + activation.
    - Dense uses weight norm options and regularizers.
    - Build LHUC MLP (`MLP`) with per-layer `lhuc_output_dims` and overrides via `lhuc_params`.
  - Extends trainable/non-trainable weights from sublayers.
- `call`:
  - If inputs is tuple/list: `(dense_input, lhuc_input)` else both are inputs.
  - Apply `extra_layers` (input BatchNorm) to dense_input.
  - For each layer and corresponding lhuc MLP:
    - `output_t = layer(dense_input) * lhuc_layer(lhuc_input)`.
    - Feed output to next layer.
- `get_config`:
  - Serializes activations via `ad_acts.serialize`, initializers via `tf.initializers.serialize`.
  - Includes batch norm settings and regularizers.
  - Adds `_lhuc_kwargs` into config.
- `from_config`:
  - Creates params via `params().copy()`, fills known keys, deserializes initializers/activations and regularizers.
  - Pops used keys from config and returns `p.instantiate()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/src/lhuc.rs`.
- Rust public API surface: `LHUCTower`, `LHUCConfig`, `LHUCOverrides`, `LHUCOutputDims`.
- Data model mapping:
  - Python activations list → Rust `ActivationType` list; last layer forced to `None`.
  - `sigmoid2` in LHUC MLP last layer → Rust `ActivationType::Sigmoid2`.
  - `lhuc_*` kwargs → `LHUCOverrides`.
- Feature gating: None.
- Integration points: `Dense`, `MLP`, `BatchNorm`.

**Implementation Steps (Detailed)**
1. Ensure Rust LHUCTower uses same activation and initializer expansion rules.
2. Mirror LHUC output dims logic (shared list vs per-layer list).
3. Add input BatchNorm and per-layer BatchNorm gating with same defaults.
4. Implement LHUCOverrides mapping to override LHUC MLP settings.
5. Ensure config serialization/deserialization matches Python (including `lhuc_*` fields).

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/lhuc_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/lhuc_test.rs` (new).
- Cross-language parity test:
  - Fix weights and inputs; compare outputs for single-input and `(dense_input, lhuc_input)` modes.

**Gaps / Notes**
- Python `from_config` ignores `kernel_regularizer` and `bias_regularizer` assignments (calls deserialize but does not set); decide whether to mirror or fix in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/lhuc_test.py`
<a id="monolith-native-training-layers-lhuc-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 73
- Purpose/role: Smoke tests for LHUCTower instantiation, config serialization, and forward call with separate dense/LHUC inputs.
- Key symbols/classes/functions: `LHUCTowerTest` methods `test_lhuc_instantiate`, `test_lhuc_serde`, `test_lhuc_call`.
- External dependencies: TensorFlow v1 session mode, NumPy.
- Side effects: Runs session after variable initialization.

**Required Behavior (Detailed)**
- `test_lhuc_instantiate`:
  - Params-based instantiate with `output_dims=[1,3,4,5]`, `activations=None`, `initializers=GlorotNormal`.
  - Direct constructor with same output dims and `initializers=HeUniform`.
- `test_lhuc_serde`:
  - Instantiates via params, `cfg = get_config()`, `LHUCTower.from_config(cfg)` should succeed.
- `test_lhuc_call`:
  - Builds LHUCTower with:
    - `output_dims=[50,20,1]`,
    - `lhuc_output_dims=[[50,50],[50,50,20],[100,1]]`,
    - `lhuc_use_bias=False`, `use_bias=True`, `activations=None`.
  - Inputs: `dense_data` `(100,100)` and `lhuc_data` `(100,50)`.
  - Calls layer with `[dense_data, lhuc_data]`, sums output, runs session.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/lhuc_test.rs`.
- Rust public API surface: `LHUCTower`, `LHUCConfig`, `LHUCOverrides`.
- Data model mapping:
  - `lhuc_*` kwargs ↔ `LHUCOverrides`.
  - Params-based instantiation ↔ Rust config builder.
- Feature gating: None.
- Integration points: `monolith_layers::lhuc`.

**Implementation Steps (Detailed)**
1. Add Rust tests for constructor/config round-trip.
2. Add forward test with separate dense/LHUC inputs and per-layer LHUC output dims.
3. Add assertions on output shape and deterministic sum.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/lhuc_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/lhuc_test.rs` (new).
- Cross-language parity test:
  - Fix weights and inputs; compare output sums.

**Gaps / Notes**
- Python uses `lhuc_use_bias` override (via `lhuc_*` kwargs); ensure Rust override wiring matches.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/logit_correction.py`
<a id="monolith-native-training-layers-logit-correction-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 88
- Purpose/role: Logit correction layer to compensate for sampling bias during training or inference.
- Key symbols/classes/functions: `LogitCorrection`, `safe_log_sigmoid`, `get_sample_logits`.
- External dependencies: TensorFlow/Keras (`Layer`, `InputSpec`, activations), `with_params`.
- Side effects: None beyond computation.

**Required Behavior (Detailed)**
- Inputs: `(logits, sample_rate)` where both are max 2D tensors.
- `call`:
  - `corrected = get_sample_logits(logits, sample_rate, sample_bias)`.
  - If `activation` is set, apply it.
- `safe_log_sigmoid(logits)`:
  - Stable computation of `log(sigmoid(logits))` using `log1p(exp(neg_abs))` trick.
- `get_sample_logits`:
  - `sample_rate is None` and `sample_bias=True`: return `safe_log_sigmoid(logits)`.
  - `sample_rate not None` and `sample_bias=False`: return `logits - log(sample_rate)`.
  - `sample_rate not None` and `sample_bias=True`: return `safe_log_sigmoid(logits) - log(sample_rate)`.
  - Else: return `logits`.
- `compute_output_shape` returns a 1D shape `([None])` (questionable but part of API).
- `get_config` serializes `activation` and `sample_bias`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/src/logit_correction.rs`.
- Rust public API surface: `LogitCorrection` with `forward_with_sample_rate`.
- Data model mapping:
  - Python activation → Rust `ActivationType`/`ActivationLayer`.
  - `sample_rate` optional tensor → `Option<&Tensor>`.
- Feature gating: None.
- Integration points: Used in training heads that correct logits for sampling.

**Implementation Steps (Detailed)**
1. Ensure `safe_log_sigmoid` matches TF numeric behavior.
2. Confirm `get_sample_logits` branch logic matches Python.
3. Add optional activation layer application.
4. Add config serialization to match `activation` and `sample_bias`.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/logit_correction_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/logit_correction_test.rs` (new).
- Cross-language parity test:
  - Compare corrected logits for combinations of sample_rate present/absent and sample_bias true/false.

**Gaps / Notes**
- Python `compute_output_shape` always returns `[None]` regardless of input; Rust may not expose shape inference.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/logit_correction_test.py`
<a id="monolith-native-training-layers-logit-correction-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 65
- Purpose/role: Smoke tests for LogitCorrection instantiation, serialization, and forward call.
- Key symbols/classes/functions: `SailSpecialTest` methods `test_sr_instantiate`, `test_sr_serde`, `test_sr_call`.
- External dependencies: TensorFlow v1 session mode, NumPy.
- Side effects: Runs session after variable init.

**Required Behavior (Detailed)**
- `test_sr_instantiate`:
  - Params-based instantiation with `activation=relu`.
  - Direct constructor with `activation=relu`.
- `test_sr_serde`:
  - Instantiate with `activation=sigmoid`, then `get_config` and `from_config`.
- `test_sr_call`:
  - Instantiate via params with `activation=tanh`.
  - Inputs: logits `x` shape `(100,10)` and sample_rate `sr` shape `(100,1)` sampled in `(1e-10, 1)`.
  - Runs `layer((x, sr))` and sums outputs in session.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/logit_correction_test.rs`.
- Rust public API surface: `LogitCorrection` with optional activation.
- Data model mapping:
  - Params-based instantiation ↔ Rust config/builder.
  - `get_config`/`from_config` ↔ serde round-trip.
- Feature gating: None.
- Integration points: `monolith_layers::logit_correction`.

**Implementation Steps (Detailed)**
1. Add Rust tests for constructor/config round-trip.
2. Add forward test with logits + sample_rate; assert output shape and deterministic sum.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/logit_correction_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/logit_correction_test.rs` (new).
- Cross-language parity test:
  - Fix logits/sample_rate and compare outputs for activation on/off.

**Gaps / Notes**
- Python test uses `activation=tanh`; ensure Rust supports this activation.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/mlp.py`
<a id="monolith-native-training-layers-mlp-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 211
- Purpose/role: Core MLP layer built from custom Dense layers with optional batch normalization, weight norm, and feature insight logging.
- Key symbols/classes/functions: `MLP`, `build`, `call`, `get_config`, `from_config`, `get_layer`.
- External dependencies: TensorFlow/Keras (`Layer`, `BatchNormalization`, regularizers), Monolith `Dense`, `extend_as_list`, `advanced_activations`, `feature_insight_data`.
- Side effects: Adds BN/Dense losses, uses `feature_insight_data` hook on first Dense when segment info provided.

**Required Behavior (Detailed)**
- Initialization:
  - `output_dims` defines layers; `use_weight_norm`, `use_learnable_weight_norm`, `use_bias`, regularizers and BN params stored.
  - `initializers` expanded to list of length `_n_layers` via `extend_as_list` and `tf.initializers.get`.
  - `activations`:
    - None → `[relu]*(n_layers-1) + [None]`.
    - List/tuple length must match `n_layers`; each mapped via `ad_acts.get`.
    - Single activation → applied to all but last (None).
- `build`:
  - If BN enabled, prepend input BN layer.
  - For each layer:
    - Create custom `Dense` with activation=None, bias/init/regularizer and kernel norm settings.
    - Optional BatchNorm between layers (not on final layer).
    - Append activation layer if not None.
  - Tracks trainable/non-trainable weights and adds sub-layer losses.
- `call`:
  - Sequentially applies `_stacked_layers`.
  - When a layer name ends with `dense_0` and kwargs provided, calls `feature_insight_data` with segment metadata and layer kernel.
- `get_config` serializes activations and initializers, regularizers, BN settings, weight norm flags.
- `from_config`:
  - Deserializes `initializers` and `activations`; others assigned directly.
  - Ignores leftover keys, then `instantiate()`.
- `get_layer(index)` returns `_stacked_layers[index]`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/src/mlp.rs`.
- Rust public API surface: `MLP`, `MLPConfig`, `ActivationType`, `ActivationLayer`.
- Data model mapping:
  - Python activations list → Rust `ActivationType` list; last layer activation None.
  - `feature_insight_data` hook → optional instrumentation in Rust.
- Feature gating: None.
- Integration points: `Dense`, `BatchNorm`, activation registry.

**Implementation Steps (Detailed)**
1. Ensure MLPConfig defaults align with Python (weight norm on, BN off).
2. Add support for per-layer initializers and activations list expansion (1 or N).
3. Add optional input BatchNorm and per-layer BN (skip last).
4. Add optional feature insight hook (or document omission).
5. Add config serialization compatible with Python `get_config`.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/mlp_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/mlp_test.rs` (new).
- Cross-language parity test:
  - Fix weights and inputs; compare output sums with/without BN and weight norm.

**Gaps / Notes**
- Python `feature_insight_data` side effect not yet mirrored in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/mlp_test.py`
<a id="monolith-native-training-layers-mlp-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 78
- Purpose/role: Smoke tests for MLP instantiation, serialization, and forward call with batch normalization and mixed activation list.
- Key symbols/classes/functions: `MLPTest` methods `test_mlp_instantiate`, `test_mlp_serde`, `test_mlp_call`.
- External dependencies: TensorFlow v1 session mode, NumPy.
- Side effects: Checks internal `_stacked_layers` length.

**Required Behavior (Detailed)**
- `test_mlp_instantiate`:
  - Params-based instantiate with `output_dims=[1,3,4,5]`, `activations=None`, `initializers=GlorotNormal`.
  - Direct constructor with same output dims and `initializers=HeUniform`.
- `test_mlp_serde`:
  - Instantiate via params, `get_config` and `MLP.from_config(cfg)` should succeed.
- `test_mlp_call`:
  - Params-based instantiate with:
    - `output_dims=[100,50,10,1]`,
    - `enable_batch_normalization=True`,
    - `activations=['relu', tanh, PReLU, None]`,
    - `initializers=GlorotNormal`.
  - Input shape `(100,100)`; sums output.
  - Asserts `len(layer._stacked_layers) == 11`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/mlp_test.rs`.
- Rust public API surface: `MLP`, `MLPConfig`, `ActivationType`.
- Data model mapping:
  - Activation list includes mixed string/function/class; Rust should accept equivalent `ActivationType`.
  - `_stacked_layers` count corresponds to Dense + optional BN + activation for each layer; ensure layering logic matches.
- Feature gating: None.
- Integration points: `monolith_layers::mlp`.

**Implementation Steps (Detailed)**
1. Add Rust tests for constructor and config serialization.
2. Add forward test with batch normalization and mixed activations; assert output shape/sum.
3. Add check on internal layer count if exposed (or infer via config).

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/mlp_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/mlp_test.rs` (new).
- Cross-language parity test:
  - Fix weights and inputs; compare output sums and layer counts.

**Gaps / Notes**
- Python uses `tf.keras.layers.PReLU` class in activations list; Rust must map to `ActivationType::PReLU` with default params.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/multi_task.py`
<a id="monolith-native-training-layers-multi-task-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 448
- Purpose/role: Multi-task learning layers: MMoE (multi-gate mixture of experts) and SNR (sub-network routing with hard-concrete gates).
- Key symbols/classes/functions: `MMoE`, `SNR`, `hard_concrete_ste`.
- External dependencies: TensorFlow/Keras (Layer, activations/initializers/regularizers/constraints), Monolith (`MLP`, `Dense`, `add_layer_loss`, `with_params`).
- Side effects: Adds loss terms for gate balancing (MMoE) and L0 penalty (SNR).

**Required Behavior (Detailed)**
- `MMoE`:
  - `gate_type` in `{softmax, topk, noise_topk}`; `top_k` default 1.
  - `num_experts` inferred from `expert_output_dims` or activations/initializers if not provided.
  - Experts are MLPs; all expert output dims must share same last dim.
  - Gate input dim inferred from `input_shape` (supports TF shape objects).
  - Gate weights shape `(gate_input_dim, num_experts * num_tasks)`; optional noise weights if `noise_topk`.
  - `calc_gate`:
    - Linear gate logits; optional noise.
    - Softmax over experts; if topk modes, zero out non-topk and renormalize.
    - Returns gates with shape `(batch, num_experts, num_tasks)`.
  - `call`:
    - If inputs is tuple, `(expert_input, gate_input)` else both are inputs.
    - `expert_outputs = stack([expert(x)], axis=2)` -> `(batch, output_dim, num_experts)`.
    - `mmoe_output = matmul(expert_outputs, gates)` -> `(batch, output_dim, num_tasks)`.
    - If gate_type != softmax: adds CV-squared loss over gate importance.
    - Returns list of per-task outputs via `unstack(axis=2)`.
- `hard_concrete_ste`:
  - Clamps to [0,1] in forward; gradient is identity (STE).
- `SNR`:
  - `snr_type` in `{trans, aver}`; `aver` requires `in_subnet_dim == out_subnet_dim`.
  - `build` infers `num_in_subnet` and `in_subnet_dim` from input list shapes.
  - `log_alpha` shape `(num_route, 1)` where `num_route = num_in_subnet * num_out_subnet`.
  - Adds L0 loss: `sum(sigmoid(log_alpha - factor))` with `factor = beta * log(-gamma/zeta)`.
  - If `snr_type=trans`: `weight` shape `(num_route, block_size)`; else identity tiled.
  - `sample`:
    - If training: sample `u`, `s = sigmoid((logit(u)+log_alpha)/beta)`.
    - Else: `s = sigmoid(log_alpha)`.
    - Stretch to `[gamma,zeta]`, then clamp to [0,1] (STE optional).
  - `call`:
    - Multiply `weight` by `z`, reshape to block matrix, matmul concat(inputs), split into outputs.

**Rust Mapping (Detailed)**
- Target crate/module:
  - `monolith-rs/crates/monolith-layers/src/mmoe.rs` (MMoE).
  - `monolith-rs/crates/monolith-layers/src/snr.rs` (SNR).
- Rust public API surface:
  - `MMoE`, `MMoEConfig`, `GateType`/`Gate`, `SNR`, `SNRConfig`, `SNRType`.
- Data model mapping:
  - Python `gate_type` → Rust enum.
  - `top_k` and noise_topk behavior → Rust gating implementation.
  - `use_bias`, batch norm flags, and initializers for experts.
- Feature gating: None.
- Integration points: MLP, Dense, activation registry.

**Implementation Steps (Detailed)**
1. Align expert construction and gate input dim inference with Python.
2. Implement noise_topk gating and CV-squared loss term for topk modes.
3. Ensure SNR sampling uses same hard-concrete bounds and STE behavior.
4. Add config serialization for MMoE and SNR (activations, initializers, gate_type, etc.).

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/multi_task_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/multi_task_test.rs` (new).
- Cross-language parity test:
  - Fix weights/inputs and compare outputs for MMoE (softmax/topk/noise_topk) and SNR (trans/aver).

**Gaps / Notes**
- Python uses `add_loss` for CV-squared and L0 penalty; Rust must decide how to expose these losses.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/multi_task_test.py`
<a id="monolith-native-training-layers-multi-task-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 128
- Purpose/role: Smoke tests for MMoE and SNR layers (instantiation, serde, forward).
- Key symbols/classes/functions: `MultiTaskTest` methods `test_mmoe_instantiate`, `test_mmoe_serde`, `test_mmoe_call`, `test_snr_instantiate`, `test_snr_serde`, `test_snr_call`.
- External dependencies: TensorFlow v1 session mode, NumPy.
- Side effects: Runs TF sessions for forward calls.

**Required Behavior (Detailed)**
- MMoE:
  - Instantiate via params with `num_tasks=2`, `num_experts=3`, `expert_output_dims=[128,64,64]`, `expert_activations='relu'`, `expert_initializers=GlorotNormal`.
  - Direct constructor with same settings.
  - `test_mmoe_call`: uses `gate_type='topk'`, `top_k=2`, expert dims list-of-lists, input `(100,128)`, sums output list.
- SNR:
  - Instantiate via params with `num_out_subnet=3`, `out_subnet_dim=128`, `use_ste=False`.
  - Direct constructor with same values.
  - `test_snr_call`: `snr_type='aver'`, `mode=PREDICT`, four inputs `(100,128)` each; sums outputs.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/multi_task_test.rs`.
- Rust public API surface: `MMoE`, `MMoEConfig`, `SNR`, `SNRConfig`.
- Data model mapping:
  - `gate_type` and `top_k` → Rust gate config.
  - `snr_type='aver'` → `SNRType::Aver`.
- Feature gating: None.
- Integration points: `monolith_layers::mmoe`, `monolith_layers::snr`.

**Implementation Steps (Detailed)**
1. Add Rust tests for config round-trip for MMoE and SNR.
2. Add forward tests with matching input shapes and configurations.
3. Add deterministic assertions on output shapes/sums.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/multi_task_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/multi_task_test.rs` (new).
- Cross-language parity test:
  - Fix weights and inputs; compare outputs for MMoE (topk) and SNR (aver).

**Gaps / Notes**
- Python uses list-of-lists expert dims for MMoE; ensure Rust supports heterogeneous expert configs.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/norms.py`
<a id="monolith-native-training-layers-norms-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 343
- Purpose/role: Normalization and multi-task gradient balancing utilities: custom BatchNorm, LayerNorm, and GradNorm.
- Key symbols/classes/functions: `BatchNorm`, `LayerNorm`, `GradNorm`.
- External dependencies: TensorFlow/Keras (`Layer`, `InputSpec`, initializers/regularizers), Monolith `add_layer_loss`.
- Side effects: Emits TF summary scalars/histograms; adds losses for moving mean/variance and GradNorm.

**Required Behavior (Detailed)**
- `BatchNorm`:
  - Tracks moving_mean and moving_variance; optional center/scale with beta/gamma weights.
  - In TRAIN mode:
    - Computes batch mean/variance (optionally stop-grad).
    - Replaces gradient for moving stats with current batch values.
    - Adds losses for moving mean/variance and logs summaries.
    - If `training_use_global_dist`: blends moving stats with current stats using `global_dist_momentum`.
  - In EVAL mode:
    - Uses stopped moving stats; logs summaries.
  - Returns `tf.nn.batch_normalization` with epsilon.
- `LayerNorm`:
  - Normalizes across last dimension per sample; applies beta/gamma with epsilon 1e-6.
  - Same logic for train/eval.
- `GradNorm`:
  - Given `losses` and `shared_inputs`, computes gradients wrt shared inputs, concatenates, and computes norms.
  - Softmax weights over tasks; computes `gnorm_loss` using absolute or relative difference vs average.
  - Returns `(gnorm_loss, weighted_loss)`; logs weights and gnorms.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/src/normalization.rs`.
- Rust public API surface: `BatchNorm`, `LayerNorm`, `GradNorm`.
- Data model mapping:
  - Momentum/epsilon/renorm settings map to Rust BatchNorm fields.
  - GradNorm computes losses from provided grads (Rust currently expects grads, not tensors).
- Feature gating: None.
- Integration points: MLP/MMoE/LHUC use BatchNorm; GradNorm used in multi-task setups.

**Implementation Steps (Detailed)**
1. Align BatchNorm behavior with Python: moving stats, training/eval paths, optional stop-grad, and global-dist blending.
2. Ensure LayerNorm uses epsilon=1e-6 and per-sample normalization.
3. Adjust GradNorm API to accept losses and grads consistent with Python (or document differences).
4. Add config serialization for BatchNorm/LayerNorm/GradNorm.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/norms_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/norms_test.rs` (new).
- Cross-language parity test:
  - Compare BatchNorm/LayerNorm outputs and GradNorm loss for fixed inputs.

**Gaps / Notes**
- Python uses TF summaries and add_layer_loss for moving stats; Rust lacks equivalent side effects.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/norms_test.py`
<a id="monolith-native-training-layers-norms-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 84
- Purpose/role: Smoke tests for LayerNorm and GradNorm instantiation and serialization; simple forward for LayerNorm.
- Key symbols/classes/functions: `NormTest` methods `test_ln_instantiate`, `test_ln_serde`, `test_ln_call`, `test_gn_instantiate`, `test_gn_serde`.
- External dependencies: TensorFlow v1 session mode, NumPy.
- Side effects: Runs session for LayerNorm forward.

**Required Behavior (Detailed)**
- LayerNorm:
  - Instantiate via params with `initializer=GlorotNormal`.
  - Direct constructor with `initializer=HeUniform`.
  - `test_ln_call`: input `(100,100)`, sum outputs, run session.
- GradNorm:
  - Instantiate via params with `loss_names=["abc","defg"]`.
  - Direct constructor with `relative_diff=True`.
  - `get_config`/`from_config` round-trip.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/norms_test.rs`.
- Rust public API surface: `LayerNorm`, `GradNorm`.
- Data model mapping:
  - Params-based instantiation ↔ Rust config/builder.
  - `get_config`/`from_config` ↔ serde round-trip.
- Feature gating: None.
- Integration points: `monolith_layers::normalization`.

**Implementation Steps (Detailed)**
1. Add Rust tests for LayerNorm config round-trip and forward output shape.
2. Add Rust tests for GradNorm config round-trip.
3. Add deterministic assertions where possible.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/norms_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/norms_test.rs` (new).
- Cross-language parity test:
  - Fix inputs and compare LayerNorm output sums.

**Gaps / Notes**
- Python tests do not cover BatchNorm; ensure BatchNorm tests are added elsewhere in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/pooling.py`
<a id="monolith-native-training-layers-pooling-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 101
- Purpose/role: Defines list-based pooling layers: base `Pooling`, `SumPooling`, `AvgPooling`, `MaxPooling`.
- Key symbols/classes/functions: `Pooling.call`, `SumPooling.pool`, `AvgPooling.pool`, `MaxPooling.pool`.
- External dependencies: TensorFlow ops (`math_ops`, `array_ops`), Monolith `check_list`.
- Side effects: None.

**Required Behavior (Detailed)**
- `Pooling.call(vec_list)`:
  - Validates list with `check_list(vec_list, lambda x: x > 0)` (ensures non-empty).
  - If list length is 1, returns first tensor.
  - Otherwise calls `self.pool`.
- `SumPooling.pool`: `math_ops.add_n(vec_list)`.
- `AvgPooling.pool`: `math_ops.add_n(vec_list) / len(vec_list)`.
- `MaxPooling.pool`: `reduce_max(stack(vec_list), axis=0)`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/src/pooling.rs`.
- Rust public API surface: `SumPooling`, `AvgPooling`, `MaxPooling` implementing `Pooling` trait.
- Data model mapping: list of tensors → slice `&[Tensor]`.
- Feature gating: None.
- Integration points: `monolith_layers::pooling`.

**Implementation Steps (Detailed)**
1. Ensure Rust pooling checks non-empty list and returns first tensor when length is 1 (Python behavior).
2. Confirm max pooling uses elementwise max (not stacking + reduce if already implemented).
3. Add config/params metadata if needed for with_params parity.

**Tests (Detailed)**
- Python tests: none specific.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/pooling_test.rs` (new) or unit tests in module.
- Cross-language parity test:
  - Compare sums/means/maxes on fixed tensors with list length 1 and >1.

**Gaps / Notes**
- Python `check_list` enforces non-empty; Rust returns error on empty list, but must also return input directly when len=1.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/pooling_test.py`
<a id="monolith-native-training-layers-pooling-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 141
- Purpose/role: Smoke tests for Sum/Max/Avg pooling instantiation, serialization, and forward call.
- Key symbols/classes/functions: `PoolingTest` methods `test_sp_*`, `test_mp_*`, `test_ap_*`.
- External dependencies: TensorFlow v1 session mode, NumPy.
- Side effects: Runs session after variable init.

**Required Behavior (Detailed)**
- SumPooling:
  - Params-based instantiate and direct constructor.
  - `test_sp_call`: list of 5 tensors `(100,10)`, sum output.
- MaxPooling:
  - Params-based instantiate and direct constructor.
  - `test_mp_call`: list of 5 tensors `(100,10)`, sum output.
- AvgPooling:
  - Params-based instantiate and direct constructor.
  - `test_ap_call`: list of 5 tensors `(100,10)`, sum output.
- Serialization:
  - `get_config` and `from_config` for each pooling type.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/tests/pooling_test.rs`.
- Rust public API surface: `SumPooling`, `MaxPooling`, `AvgPooling`.
- Data model mapping:
  - Params-based instantiation ↔ Rust config/builder (if needed).
  - No parameters beyond defaults.
- Feature gating: None.
- Integration points: `monolith_layers::pooling`.

**Implementation Steps (Detailed)**
1. Add Rust tests for pooling ops on list of tensors (len=5).
2. Add tests for len=1 list to match Python `Pooling.call`.
3. Add serialization tests if config metadata is implemented.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/pooling_test.py`.
- Rust tests: `monolith-rs/crates/monolith-layers/tests/pooling_test.rs` (new).
- Cross-language parity test:
  - Fix input tensors; compare sums for each pooling type.

**Gaps / Notes**
- Python pooling layers rely on Keras `Layer` base; Rust pooling is a trait. Map serialization accordingly.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/sparse_nas.py`
<a id="monolith-native-training-layers-sparse-nas-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 31
- Purpose/role: Placeholder module containing only imports; no classes or functions defined.
- Key symbols/classes/functions: None.
- External dependencies: TF/Keras, FeatureList, SummaryType, logging/flags; all imported but unused.
- Side effects: None.

**Required Behavior (Detailed)**
- No runtime behavior; module only defines imports.
- If later extended, define behavior for sparse NAS utilities.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (no behavior to port).
- Rust public API surface: None.
- Data model mapping: None.
- Feature gating: None.
- Integration points: None.

**Implementation Steps (Detailed)**
1. Confirm this file is a stub; if not, locate missing code or history.
2. If future Python changes add functionality, update parity mapping accordingly.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/sparse_nas_test.py`.
- Rust tests: N/A unless functionality is added.
- Cross-language parity test: N/A until implemented.

**Gaps / Notes**
- This file appears to be an empty scaffold; confirm if code was removed or moved.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/sparse_nas_test.py`
<a id="monolith-native-training-layers-sparse-nas-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 23
- Purpose/role: Empty test scaffold; no test cases defined.
- Key symbols/classes/functions: None.
- External dependencies: TensorFlow (imported), NumPy (imported), unused.
- Side effects: Runs `tf.test.main()` when executed directly.

**Required Behavior (Detailed)**
- No assertions or tests; file does not exercise any functionality.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: None.
- Data model mapping: None.
- Feature gating: None.
- Integration points: None.

**Implementation Steps (Detailed)**
1. Confirm no test coverage required unless sparse_nas gains functionality.

**Tests (Detailed)**
- Python tests: `monolith/native_training/layers/sparse_nas_test.py` (empty).
- Rust tests: N/A.
- Cross-language parity test: N/A.

**Gaps / Notes**
- If sparse_nas gains code, add corresponding tests and parity section updates.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/layers/utils.py`
<a id="monolith-native-training-layers-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 159
- Purpose/role: Shared utilities for layer code: merge semantics, shape helpers, and Gumbel-based subset sampling.
- Key symbols/classes/functions: `MergeType`, `DCNType`, `check_dim`, `dim_size`, `merge_tensor_list`, `gumbel_keys`, `continuous_topk`, `sample_subset`.
- External dependencies: TensorFlow, NumPy.
- Side effects: None.

**Required Behavior (Detailed)**
- `MergeType`: string constants `concat`, `stack`, `None`.
- `DCNType`: string constants `vector`, `matrix`, `mixed`.
- `check_dim(dim)`:
  - `None` → `-1`, `int` → itself, `tf.compat.v1.Dimension` → `.value`, else raise.
- `dim_size(inputs, axis)`:
  - Uses static shape; if unknown (`-1`), returns dynamic `array_ops.shape(inputs)[axis]`.
- `merge_tensor_list(tensor_list, merge_type='concat', num_feature=None, axis=1, keep_list=False)`:
  - Accepts tensor or list; if single tensor, uses shape to decide:
    - 3D: `stack` returns `[tensor]` or tensor; `concat` reshapes to `[B, num_feat*emb]`; `None` unstack on axis.
    - 2D with `num_feature>1`: `stack` reshapes to `[B, num_feature, emb]`; `concat` returns as-is; `None` unstack.
    - 2D without `num_feature`: returns as-is.
    - Else: raise shape error.
  - For list length >1: `stack`, `concat`, or return list.
- `gumbel_keys(w)`: samples Gumbel noise and adds to `w`.
- `continuous_topk(w, k, t, separate=False)`:
  - Iteratively computes soft top-k masks; returns sum or list.
- `sample_subset(w, k, t=0.1)`:
  - `w = gumbel_keys(w)` then `continuous_topk`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-layers/src/merge.rs` for merge utilities; DCNType maps to `monolith-layers/src/dcn.rs` (`DCNMode`).
- Rust public API surface: `MergeType`, `merge_tensor_list`, `merge_tensor_list_tensor`.
- Data model mapping:
  - `MergeType::None` corresponds to `MergeOutput::List`.
  - `check_dim`/`dim_size` are implicit in Rust shape handling; consider helper utilities.
  - Gumbel subset sampling functions not currently present in Rust.
- Feature gating: None.
- Integration points: feature_cross, feature_trans, senet, etc.

**Implementation Steps (Detailed)**
1. Verify `merge_tensor_list` semantics in Rust match Python (including single-tensor reshape/unstack cases).
2. Add Rust equivalents for `check_dim`/`dim_size` if needed for dynamic shapes.
3. Implement Gumbel subset sampling helpers if required by future layers.

**Tests (Detailed)**
- Python tests: none specific.
- Rust tests: add unit tests in `monolith-rs/crates/monolith-layers/tests/merge_test.rs` if not present.
- Cross-language parity test:
  - Compare merge outputs for 2D/3D inputs with `num_feature` and `keep_list` settings.

**Gaps / Notes**
- Gumbel subset sampling utilities are missing in Rust; add if used elsewhere.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/learning_rate_functions.py`
<a id="monolith-native-training-learning-rate-functions-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 112
- Purpose/role: Defines learning rate schedule function objects (base class + polynomial decay) for optimizers and embedding slice configs.
- Key symbols/classes/functions: `LearningRateFunction`, `PolynomialDecay`.
- External dependencies: TensorFlow v1 (`tf.compat.v1.train.polynomial_decay`, `get_or_create_global_step`), `abc`.
- Side effects: None; uses global step when called.

**Required Behavior (Detailed)**
- `LearningRateFunction`:
  - Abstract `__call__` that must be overridden.
  - `__str__` prints class name and sorted `__dict__` params.
- `PolynomialDecay`:
  - Stores init params: `initial_learning_rate`, `decay_steps`, `end_learning_rate`, `power`, `cycle`, `name`.
  - `__call__` fetches `global_step = tf.compat.v1.train.get_or_create_global_step()` and returns `tf.compat.v1.train.polynomial_decay(...)`.
  - Uses TF’s polynomial decay semantics (including `cycle`).

**Rust Mapping (Detailed)**
- Target crate/module: N/A (no Rust equivalent yet).
- Rust public API surface: None.
- Data model mapping: If implemented, use a trait + struct for polynomial decay tied to training step.
- Feature gating: None.
- Integration points: Optimizer configs and embedding slice configs.

**Implementation Steps (Detailed)**
1. Decide where to place LR schedules in Rust (optimizer module or training crate).
2. Implement `PolynomialDecay` with explicit step parameter (Rust lacks TF global_step).
3. Provide string formatting for config parity if required.

**Tests (Detailed)**
- Python tests: None in repo.
- Rust tests: Add unit tests for decay values at known steps.
- Cross-language parity test: Compare decay outputs for fixed steps.

**Gaps / Notes**
- Python relies on TF global step; Rust will need explicit step input or global trainer context.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/learning_rate_functions_test.py`
<a id="monolith-native-training-learning-rate-functions-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 76
- Purpose/role: Tests PolynomialDecay schedule and its integration with an optimizer.
- Key symbols/classes/functions: `PolynomialDecayTest.test_basic`, `test_dense_optimizer`.
- External dependencies: TensorFlow v1 session/optimizers, NumPy.
- Side effects: Uses global_step and updates variables via Adagrad.

**Required Behavior (Detailed)**
- `test_basic`:
  - Creates global_step, increments twice, and checks decay outputs.
  - With `initial_learning_rate=0.01`, `decay_steps=10`, `end_learning_rate=0.11`:
    - At global_step=1: expects 0.02.
    - At global_step=2: expects 0.03.
  - Ensures `__str__` equality between two identical PolynomialDecay instances.
- `test_dense_optimizer`:
  - Uses PolynomialDecay as `learning_rate` for `AdagradOptimizer`.
  - Applies grads to two variables for 3 steps.
  - Verifies updated values match expected arrays.

**Rust Mapping (Detailed)**
- Target crate/module: N/A until learning rate schedules are implemented.
- Rust public API surface: PolynomialDecay schedule + optimizer integration.
- Data model mapping: global_step must be explicit in Rust.
- Feature gating: None.
- Integration points: Optimizer implementations (e.g., Adagrad).

**Implementation Steps (Detailed)**
1. Implement PolynomialDecay in Rust with explicit step input.
2. Add tests validating decay values for known steps (0,1,2).
3. If an optimizer exists, add integration test similar to Adagrad update.

**Tests (Detailed)**
- Python tests: `monolith/native_training/learning_rate_functions_test.py`.
- Rust tests: `monolith-rs/crates/monolith-optim/tests/learning_rate_functions_test.rs` (new) or similar.
- Cross-language parity test:
  - Compare decay values at fixed steps.

**Gaps / Notes**
- Python uses TF global_step; Rust needs explicit step or trainer context.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/logging_ops.py`
<a id="monolith-native-training-logging-ops-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 56
- Purpose/role: Thin wrappers around custom logging/metrics TF ops (timestamps, timers, machine health).
- Key symbols/classes/functions: `tensors_timestamp`, `emit_timer`, `machine_info`, `check_machine_health`.
- External dependencies: TensorFlow, absl flags, custom ops `gen_monolith_ops`.
- Side effects: Registers a global flag `monolith_default_machine_info_mem_limit`.

**Required Behavior (Detailed)**
- `tensors_timestamp(tensors)`: returns `(tensors, timestamp)` via `monolith_tensors_timestamp`.
- `emit_timer(key, value, tags=None)`:
  - Formats tags as `"k=v|k2=v2"`, passes to `monolith_metric_v2`.
  - Returns TF op.
- `machine_info(mem_limit=None, shared_name=None)`:
  - Uses default flag if `mem_limit` is None.
  - Calls `monolith_machine_info` with `mem_limit`, `name`, `shared_name`.
- `check_machine_health(machine_info_tensor)`:
  - Returns scalar string tensor from `monolith_check_machine_health`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (custom TF ops not wired in Rust).
- Rust public API surface: None.
- Data model mapping: Would require TF runtime bindings.
- Feature gating: TF-runtime only if added.
- Integration points: metrics/logging pipeline.

**Implementation Steps (Detailed)**
1. Decide whether to expose these custom ops in Rust TF-runtime backend.
2. If yes, add FFI bindings and wrappers with identical signatures.
3. Provide a config/flag equivalent for default mem_limit.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: add once bindings exist.
- Cross-language parity test: validate emitted tags and machine health output.

**Gaps / Notes**
- Requires custom TF ops; currently no Rust bindings.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/logging_ops_test.py`
<a id="monolith-native-training-logging-ops-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 57
- Purpose/role: Tests logging ops custom TF wrappers (timestamp, timer, machine health).
- Key symbols/classes/functions: `LoggingOpsTest.test_tensors_timestamp`, `test_emit_timer`, `test_machine_health`, `test_machine_health_oom`.
- External dependencies: TensorFlow v1, absl flags, `logging_ops_pb2`.
- Side effects: Mutates global flag `monolith_default_machine_info_mem_limit`.

**Required Behavior (Detailed)**
- `test_tensors_timestamp`:
  - Calls `tensors_timestamp` twice and asserts newer timestamp >= old.
- `test_emit_timer`:
  - Calls `emit_timer("test", 0.0)` and evaluates op.
- `test_machine_health`:
  - Sets mem_limit high; `check_machine_health` returns empty bytes.
- `test_machine_health_oom`:
  - Sets mem_limit=0; `check_machine_health` returns serialized proto with status `OUT_OF_MEMORY`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (custom TF ops not implemented).
- Rust public API surface: None.
- Data model mapping: Would require TF runtime bindings and protobuf parsing.
- Feature gating: TF-runtime only.
- Integration points: logging/metrics pipeline.

**Implementation Steps (Detailed)**
1. Add Rust bindings for logging ops if TF runtime backend is enabled.
2. Add tests mirroring timestamp monotonicity and machine health outcomes.
3. Parse protobuf in Rust to validate OOM status.

**Tests (Detailed)**
- Python tests: `monolith/native_training/logging_ops_test.py`.
- Rust tests: N/A until bindings exist.
- Cross-language parity test: Validate proto outputs for machine health.

**Gaps / Notes**
- Depends on custom ops and protobufs not yet exposed in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/losses/batch_softmax_loss.py`
<a id="monolith-native-training-losses-batch-softmax-loss-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 57
- Purpose/role: Computes batch softmax loss for retrieval-style training.
- Key symbols/classes/functions: `batch_softmax_loss`.
- External dependencies: TensorFlow.
- Side effects: None.

**Required Behavior (Detailed)**
- Inputs:
  - `query` shape `(batch_size, k)`, `item` shape `(batch_size, k)`.
  - `item_step_interval` shape `(batch_size,)`.
  - `r` weights (interest) same length as batch.
  - `normalize` (default True), `temperature` (default 1.0).
- Validation:
  - `temperature` must be > 0 else raise `ValueError("temperature should be positive, while got ...")`.
- Computation:
  - Optional L2-normalize query/item along axis 1.
  - `similarity = query @ item^T / temperature`.
  - Clamp `item_step_interval` to at least 1.0, compute `item_frequency = 1 / item_step_interval`.
  - Adjust similarity: `exp(similarity - log(item_frequency))`.
  - Loss: `-sum(r * log(diag(similarity) / reduce_sum(similarity, axis=1)))`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (no Rust loss implementation yet).
- Rust public API surface: loss function in training/optimizer crate.
- Data model mapping: Tensor ops for matmul, diag, log, exp.
- Feature gating: None.
- Integration points: training loss computation.

**Implementation Steps (Detailed)**
1. Implement batch_softmax_loss in Rust with the same math and shape checks.
2. Ensure numerical stability around log/exp and item_frequency.
3. Add input normalization option.

**Tests (Detailed)**
- Python tests: `monolith/native_training/losses/batch_softmax_loss_test.py`.
- Rust tests: new test in `monolith-rs/crates/monolith-training/tests`.
- Cross-language parity test: compare loss for fixed inputs.

**Gaps / Notes**
- Requires loss module placement decision in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/losses/batch_softmax_loss_test.py`
<a id="monolith-native-training-losses-batch-softmax-loss-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 35
- Purpose/role: Single test for batch_softmax_loss numeric output.
- Key symbols/classes/functions: `BatchSoftmaxLossTest.test_batch_softmax_loss`.
- External dependencies: TensorFlow, NumPy.
- Side effects: None.

**Required Behavior (Detailed)**
- Creates random `query` and `item` tensors `(batch=4, dim=3)`.
- `item_step_interval` is random integers in `[1,10)`, `r` is ones.
- Calls `batch_softmax_loss` and asserts loss equals `6.5931373`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A until loss is implemented.
- Rust public API surface: batch_softmax_loss.
- Data model mapping: Tensor operations and RNG.
- Feature gating: None.
- Integration points: training loss module.

**Implementation Steps (Detailed)**
1. Implement loss and a deterministic test by seeding RNG or using fixed inputs.
2. Match Python numeric output if using the same fixed inputs.

**Tests (Detailed)**
- Python tests: `monolith/native_training/losses/batch_softmax_loss_test.py`.
- Rust tests: add deterministic equivalent.
- Cross-language parity test: compare loss for fixed inputs.

**Gaps / Notes**
- Python test uses random inputs without setting a seed but asserts a fixed value; likely flaky. Prefer fixing inputs in Rust and note the discrepancy.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/losses/inbatch_auc_loss.py`
<a id="monolith-native-training-losses-inbatch-auc-loss-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 41
- Purpose/role: Wrapper for custom TF op `InbatchAucLoss` and its gradient registration.
- Key symbols/classes/functions: `inbatch_auc_loss`, `_inbatch_auc_loss_grad`.
- External dependencies: `gen_monolith_ops` custom op.
- Side effects: Registers gradient for `InbatchAucLoss`.

**Required Behavior (Detailed)**
- `inbatch_auc_loss(label, logit, neg_weight=1.0)`:
  - Calls `inbatch_auc_loss_ops.inbatch_auc_loss`.
- Gradient:
  - `InbatchAucLoss` gradient returns `None` for label and computed gradient for logit via `inbatch_auc_loss_grad`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (custom TF op not bound in Rust).
- Rust public API surface: None.
- Data model mapping: Would need TF runtime binding.
- Feature gating: TF-runtime only if implemented.
- Integration points: loss computation in training.

**Implementation Steps (Detailed)**
1. Add Rust binding for `InbatchAucLoss` op if TF runtime backend is used.
2. Expose gradient or compute manually if training is supported.

**Tests (Detailed)**
- Python tests: `monolith/native_training/losses/inbatch_auc_loss_test.py`.
- Rust tests: N/A until binding exists.
- Cross-language parity test: compare loss/grad values for fixed inputs.

**Gaps / Notes**
- Depends on custom TF ops; currently missing in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/losses/inbatch_auc_loss_test.py`
<a id="monolith-native-training-losses-inbatch-auc-loss-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 71
- Purpose/role: Unit tests for inbatch_auc_loss and its gradient op.
- Key symbols/classes/functions: `InbatchAucLossTest.test_inbatch_auc_loss`, `test_inbatch_auc_loss_grad`.
- External dependencies: TensorFlow, Python math.
- Side effects: None.

**Required Behavior (Detailed)**
- `test_inbatch_auc_loss`:
  - Uses labels `[1,0,0,1]` and logits `[0.5,-0.2,-0.4,0.8]`.
  - Computes expected loss by summing `log(sigmoid(diff))` over all pos-neg pairs.
  - Asserts almost equal to TF op output.
- `test_inbatch_auc_loss_grad`:
  - Calls custom op grad with `grad=2`.
  - Computes expected gradient by pairwise contributions.
  - Asserts close.

**Rust Mapping (Detailed)**
- Target crate/module: N/A until custom op binding exists.
- Rust public API surface: inbatch_auc_loss and grad.
- Data model mapping: pairwise log-sigmoid over pos-neg pairs.
- Feature gating: TF runtime only.
- Integration points: training loss.

**Implementation Steps (Detailed)**
1. Implement loss (or bind op) and deterministic tests with the same inputs.
2. Validate gradient against manual computation.

**Tests (Detailed)**
- Python tests: `monolith/native_training/losses/inbatch_auc_loss_test.py`.
- Rust tests: add once implementation exists.
- Cross-language parity test: compare loss/grad for fixed inputs.

**Gaps / Notes**
- Depends on custom TF op; no Rust binding yet.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/losses/ltr_losses.py`
<a id="monolith-native-training-losses-ltr-losses-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 1233
- Purpose/role: Implements learning-to-rank loss functions (pairwise, listwise, approx-NDCG) and LambdaWeight models (DCG, Precision, ListMLE) based on TF-Ranking style utilities.
- Key symbols/classes/functions: `_EPSILON`, `label_valid_fn`, `sort_by_scores`, `organize_valid_indices`, `shuffle_valid_indices`, `reshape_first_ndims`, `approx_ranks`, `inverse_max_dcg`, `get_batch_idx_size`, `RankingLossKey`, `make_loss_fn`, `create_ndcg_lambda_weight`, `create_reciprocal_rank_lambda_weight`, `create_p_list_mle_lambda_weight`, `_LambdaWeight`, `IdentityLambdaWeight`, `DCGLambdaWeight`, `PrecisionLambdaWeight`, `ListMLELambdaWeight`, `_sort_and_normalize`, `_pairwise_comparison`, `_pairwise_loss`, `_pairwise_hinge_loss`, `_pairwise_logistic_loss`, `_pairwise_soft_zero_one_loss`, `_softmax_loss`, `_sigmoid_cross_entropy_loss`, `_mean_squared_loss`, `_list_mle_loss`, `_approx_ndcg_loss`.
- External dependencies: TensorFlow (core ops, math_ops, nn_ops, array_ops, random_ops, sparse_ops), `tf.losses` reduction and loss helpers, `abc`.
- Side effects: None (no I/O). Uses op-level RNG for shuffling (`random_uniform`) and TF name scopes.

**Required Behavior (Detailed)**
- Constants:
  - `_EPSILON = 1e-10` used to set invalid logits to a log probability for softmax/ListMLE.
- `label_valid_fn(labels)`:
  - Returns boolean tensor for label validity: `labels >= 0`.
  - Accepts any convertible tensor; shape preserved.
- `sort_by_scores(scores, features_list, topn=None)`:
  - `scores` must be rank-2 `[batch_size, list_size]`; asserts rank 2.
  - `features_list` must be list of tensors with same shape as `scores`.
  - `topn` defaults to `list_size`; clamped to `<= list_size`.
  - Uses `tf.nn.top_k(scores, topn, sorted=True)` and gathers each feature after flattening; output shapes `[batch_size, topn]`.
  - Preserves per-batch ordering by using list offsets (`batch_id * list_size`).
- `organize_valid_indices(is_valid, shuffle=True, seed=None)`:
  - `is_valid` must be rank-2 boolean `[batch, list]`; asserts rank 2.
  - If `shuffle=True`, uses `random_uniform` with op seed; if False, uses reversed range to preserve original order among valids.
  - Invalid entries get sentinel score `-1e-6` so they appear last.
  - Returns gather/scatter indices tensor `[batch, list, 2]` containing `(batch_id, index)` pairs.
- `shuffle_valid_indices(is_valid, seed=None)`:
  - Wrapper around `organize_valid_indices(..., shuffle=True)`.
- `reshape_first_ndims(tensor, first_ndims, new_shape)`:
  - Asserts tensor has at least `first_ndims` (if static rank known).
  - `new_shape` replaces first `first_ndims` dims; remaining dims preserved.
  - Uses `sparse_reshape` for `SparseTensor`, else `reshape`.
- `approx_ranks(logits, alpha=10.)`:
  - `logits` shape `[batch, list]`.
  - Computes approximate ranks via generalized sigmoid of pairwise differences:
    - `pairs = sigmoid(alpha * (s_j - s_i))`
    - `rank_i = sum_j pairs + 0.5`.
  - Produces `[batch, list]` tensor; O(list_size^2) memory.
- `inverse_max_dcg(labels, gain_fn=2^label-1, rank_discount_fn=1/log1p(rank), topn=None)`:
  - Sorts labels by label values (descending) using `sort_by_scores`.
  - Computes `discounted_gain = sum(gain * discount)` over ranks.
  - Returns `1/discounted_gain` when `discounted_gain > 0`, else 0; shape `[batch,1]`.
- `get_batch_idx_size(logits, labels, rank_id, name_prefix)`:
  - Expects `logits`/`labels` with leading dimension `batch_size`; `rank_id` length `batch_size`.
  - Groups examples by `rank_id` using `tf.unique_with_counts`.
  - Computes `(row, col)` indices for each example into a padded `[num_unique, max_count]` grid.
  - `logits_idx = scatter_nd(logits, indices=batch_idx, shape=[num_unique, max_count])`.
  - `label_idx = scatter_nd(labels, indices=batch_idx, shape=[num_unique, max_count]) - 1e-6`.
  - `mask_idx = scatter_nd(ones_like(logits), indices=batch_idx, shape=[batch_size, batch_size])`.
  - Returns `(logits_idx, label_idx, mask_idx, unique_idx)` where `unique_idx = argmax(list_id_mask, axis=1)`.
  - Keep exact scatter shapes and `-1e-6` offset; even if unused elsewhere.
- `RankingLossKey`: string constants for all supported loss names.
- `make_loss_fn(...)`:
  - Validates `reduction` is in `tf.losses.Reduction.all()` and not `NONE`, else raises `ValueError("Invalid reduction: ...")`.
  - Raises `ValueError` if `loss_keys` empty or `loss_weights` length mismatch.
  - Accepts `loss_keys` as string or list; normalizes to list.
  - `_loss_fn(labels, logits, features)`:
    - Optional `weights` from `features[weights_feature_name]`.
    - Builds kwargs and dispatches to loss functions; `extra_args` merged in for all losses.
    - Uses `lambda_weight` for pairwise/softmax/listmle losses; `seed` passed to ListMLE.
    - Unknown `loss_key` raises `ValueError("Invalid loss_key: ...")`.
    - Returns weighted sum (`math_ops.add_n`), optionally applying `loss_weights`.
- `create_ndcg_lambda_weight(topn=None, smooth_fraction=0.)`:
  - Returns `DCGLambdaWeight` with `gain_fn = 2^label - 1`, `rank_discount_fn = 1/log1p(rank)`, `normalized=True`.
- `create_reciprocal_rank_lambda_weight(topn=None, smooth_fraction=0.)`:
  - Returns `DCGLambdaWeight` with `gain_fn = labels`, `rank_discount_fn = 1/rank`, `normalized=True`.
- `create_p_list_mle_lambda_weight(list_size)`:
  - Returns `ListMLELambdaWeight` with `rank_discount_fn = 2^(list_size - rank) - 1`.
- `_LambdaWeight` (abstract):
  - `_get_valid_pairs_and_clean_labels(sorted_labels)`:
    - Ensures rank-2; returns `valid_pairs` mask and labels with invalids zeroed.
  - `pair_weights(sorted_labels)`: abstract; must be implemented.
  - `individual_weights(sorted_labels)`: default returns `sorted_labels` (no transform).
- `IdentityLambdaWeight`:
  - `pair_weights` returns scalar `1.0`.
- `DCGLambdaWeight`:
  - `__init__` stores `topn`, `gain_fn`, `rank_discount_fn`, `normalized`, `smooth_fraction`; asserts `smooth_fraction` in `[0,1]`.
  - `pair_weights(sorted_labels)`:
    - Computes `gain = gain_fn(labels)` and optionally normalizes by `inverse_max_dcg`.
    - `pair_gain = gain_i - gain_j` masked to valid pairs.
    - Computes discount `u` (relative rank diff) and `v` (absolute rank) per LambdaLoss/LambdaMART.
    - `pair_weight = |pair_gain| * ((1-smooth_fraction)*u + smooth_fraction*v)`.
    - If `topn` set, masks pairs where either i or j is within topn (OR mask).
  - `individual_weights(sorted_labels)`:
    - Cleans invalid labels to 0.
    - Computes `gain_fn(labels)` and optional normalization.
    - Returns `gain * rank_discount_fn(rank)`.
- `PrecisionLambdaWeight`:
  - `__init__(topn, positive_fn=label>=1.0)`.
  - `pair_weights`:
    - Computes binary labels via `positive_fn`.
    - `label_diff = |b_i - b_j|` for valid pairs.
    - Masks pairs where exactly one of i/j is in topn (xor).
    - Returns `label_diff * rank_mask`.
- `ListMLELambdaWeight`:
  - `pair_weights` returns `sorted_labels` (pass-through).
  - `individual_weights` returns `rank_discount_fn(rank)` broadcast to `[batch, list]`.
- `_sort_and_normalize(labels, logits, weights=None)`:
  - `logits` rank-2 and shape-compatible with `labels`.
  - `weights` can be scalar, `[batch,1]`, or `[batch, list]`; broadcast to labels.
  - Invalid labels (`<0`) get score `min(logits) - 1e-6` to force sorting to end.
  - Returns sorted `(labels, logits, weights)` using `sort_by_scores`.
- `_pairwise_comparison(sorted_labels, sorted_logits, sorted_weights, lambda_weight=None)`:
  - Computes pairwise label diffs, logits diffs, and `pairwise_labels = 1{l_i > l_j}`.
  - Invalid labels mask pairs; weights apply to `i` dimension (`w_i` only).
  - If `lambda_weight` provided, multiply by `lambda_weight.pair_weights`; else multiply by `|l_i - l_j|`.
  - Uses `stop_gradient` on pairwise weights.
- `_pairwise_loss(loss_fn, labels, logits, weights=None, lambda_weight=None, lambda_scale=True, reduction=SUM_BY_NONZERO_WEIGHTS)`:
  - Sorts and builds pairwise comparisons.
  - If `lambda_weight` and `lambda_scale`, scales weights by `list_size` to counteract shrinkage.
  - Uses `core_losses.compute_weighted_loss(loss_fn(pairwise_logits), weights=pairwise_weights, reduction=...)`.
- `_pairwise_hinge_loss(...)`:
  - Loss: `relu(1 - (s_i - s_j))` for pairs with `l_i > l_j`.
  - Default `lambda_scale=True`.
- `_pairwise_logistic_loss(...)`:
  - Loss: `log(1 + exp(-logits))` via stable formulation `relu(-x) + log1p(exp(-abs(x)))`.
- `_pairwise_soft_zero_one_loss(...)`:
  - Loss: `1 - sigmoid(logits)` if logits>0 else `sigmoid(-logits)` (smooth 0-1 loss).
- `_softmax_loss(labels, logits, weights=None, lambda_weight=None, reduction=SUM_BY_NONZERO_WEIGHTS)`:
  - Sorts labels/logits/weights, masks invalid labels to 0 and invalid logits to `log(_EPSILON)`.
  - If `lambda_weight` is `DCGLambdaWeight`, replaces labels with `lambda_weight.individual_weights`.
  - Multiplies labels by weights, computes `label_sum` per list.
  - Filters out lists with `label_sum == 0` using `boolean_mask`.
  - Computes `softmax_cross_entropy(labels/label_sum, logits, weights=label_sum, reduction=...)`.
- `_sigmoid_cross_entropy_loss(...)`:
  - Flattens and filters to valid labels (`>=0`) only.
  - Broadcasts weights to labels; passes vectors to `core_losses.sigmoid_cross_entropy`.
- `_mean_squared_loss(...)`:
  - Same filtering as sigmoid; uses `core_losses.mean_squared_error`.
- `_list_mle_loss(labels, logits, weights=None, lambda_weight=None, reduction=SUM_BY_NONZERO_WEIGHTS, seed=None)`:
  - Invalid labels -> 0; invalid logits -> `log(_EPSILON)`.
  - `weights` broadcast to labels, then `squeeze`d.
  - Shuffles valid entries per list using `shuffle_valid_indices(is_label_valid, seed)`; gather labels/logits.
  - Sorts by shuffled labels (descending) to form ground-truth permutation.
  - Applies max-shift for stability; `sums = log(cumsum(exp(sorted_logits), reverse=True)) - sorted_logits`.
  - If `lambda_weight` is `ListMLELambdaWeight`, multiplies `sums` by per-rank discounts.
  - Loss = `sum(sums, axis=1)`; reduced via `compute_weighted_loss`.
- `_approx_ndcg_loss(labels, logits, weights=None, reduction=SUM, alpha=10.)`:
  - Invalid labels -> 0; invalid logits -> `min(logits) - 1e3`.
  - `label_sum` per list; lists with `label_sum == 0` removed.
  - If weights is None, code uses `ones_like(label_sum)` (note: docstring says label sum; code uses 1.0).
  - Computes gains `2^label - 1`, ranks `approx_ranks(logits, alpha)`, discounts `1/log1p(ranks)`.
  - DCG = sum(gains * discounts); cost = `-dcg * inverse_max_dcg(labels)`.
  - Uses `compute_weighted_loss(cost, weights, reduction=SUM by default)`.
- Threading/concurrency: none; pure tensor ops.
- Determinism: only nondeterminism is in `shuffle_valid_indices` (seeded via op/global seed).
- Performance: pairwise and approx-ranks are O(batch * list_size^2) memory/compute; avoid large list_size or use batching.
- Logging/metrics: none.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (no ranking loss module yet). Likely add `monolith-rs/crates/monolith-training/src/losses/ltr.rs` or a new `monolith-losses` crate.
- Rust public API surface: loss functions and LambdaWeight structs mirroring Python names; a `make_loss_fn` factory for composing losses.
- Data model mapping:
  - Tensor ops required: sort-by-score, top-k, gather, boolean mask, pairwise diffs, softmax, sigmoid.
  - Shape handling must preserve `[batch, list]` contracts and broadcasted weights.
  - RNG seed input for ListMLE shuffling (explicit seed param; no global TF seed).
- Feature gating: none for Candle path; optional TF-runtime backend for parity with TF ops.
- Integration points: training loss computation in `monolith-training`, possibly `monolith-layers` heads or `monolith-native_training` equivalents.

**Implementation Steps (Detailed)**
1. Choose Rust location for ranking losses (training crate vs new `losses` crate) and define a public API mirroring Python names.
2. Implement helpers: `label_valid_fn`, `sort_by_scores`, `organize_valid_indices`, `reshape_first_ndims`, `approx_ranks`, `inverse_max_dcg`.
3. Implement LambdaWeight hierarchy (`DCGLambdaWeight`, `PrecisionLambdaWeight`, `ListMLELambdaWeight`) with the same default gains/discounts and smooth_fraction behavior.
4. Implement core loss kernels: `_pairwise_comparison`, `_pairwise_loss`, `_softmax_loss`, `_sigmoid_cross_entropy_loss`, `_mean_squared_loss`, `_list_mle_loss`, `_approx_ndcg_loss`.
5. Preserve exact masking semantics for invalid labels (<0) and the invalid-logit sentinel behavior (`log(_EPSILON)` and `min(logits)-1e3`).
6. Add `make_loss_fn` factory that accepts loss keys/weights, feature-based weights, `extra_args`, and `lambda_weight`/`seed`.
7. Decide how to represent reductions (`SUM_BY_NONZERO_WEIGHTS`, `SUM`) and match Python reductions for each loss.
8. Add tests for each loss with fixed small tensors; validate against Python outputs.
9. Document and resolve discrepancies (e.g., approx_ndcg docstring vs code for weights).

**Tests (Detailed)**
- Python tests: none in repo for `ltr_losses.py`.
- Rust tests: add new unit tests per loss in `monolith-rs/crates/monolith-training/tests/ltr_losses_test.rs`.
- Cross-language parity test:
  - Use fixed small tensors (e.g., batch=2, list=3) and compare loss values against Python TF output.
  - Include seeded ListMLE shuffling to keep deterministic comparisons.

**Gaps / Notes**
- No Rust implementation of TF-Ranking losses yet; all of these are missing.
- `get_batch_idx_size` is unused in Python file but must still be ported for parity.
- `_approx_ndcg_loss` docstring says weights default to label sum; code uses `ones_like(label_sum)`. Preserve code behavior.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/metric/cli.py`
<a id="monolith-native-training-metric-cli-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 28
- Purpose/role: Stub/no-op CLI client placeholder; provides a `Client` with no-op methods to satisfy callers.
- Key symbols/classes/functions: `Client`, `get_cli`.
- External dependencies: `absl.logging`, `threading` (imported but unused).
- Side effects: None.

**Required Behavior (Detailed)**
- `Client.__init__(*args, **kwargs)`:
  - No-op constructor; ignores all args/kwargs.
- `Client.__getattr__(name)`:
  - Returns a function `method(*args, **kwargs)` that does nothing and returns `None`.
  - Allows arbitrary attribute access without raising `AttributeError`.
- `get_cli(*args, **kwargs)`:
  - Returns a new `Client()`; ignores args/kwargs.
- No logging, no threads, no I/O.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (stub).
- Rust public API surface: optional `NoopClient` with methods that accept any inputs and do nothing.
- Data model mapping: none.
- Feature gating: none.
- Integration points: callers expecting a CLI client can receive a stub.

**Implementation Steps (Detailed)**
1. If Rust needs a CLI client, add a no-op struct with methods used by callers.
2. Ensure missing method calls do not panic (mirror Python `__getattr__` permissiveness).

**Tests (Detailed)**
- Python tests: none.
- Rust tests: optional smoke test that unknown method calls are no-ops if implemented.
- Cross-language parity test: not needed (stub behavior only).

**Gaps / Notes**
- This is a pure stub; threading/logging imports are unused.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/metric/deep_insight_ops.py`
<a id="monolith-native-training-metric-deep-insight-ops-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 134
- Purpose/role: Thin wrappers around custom Monolith Deep Insight TF ops (create client, write metrics).
- Key symbols/classes/functions: `deep_insight_client`, `write_deep_insight`, `write_deep_insight_v2`, `deep_insight_ops`.
- External dependencies: TensorFlow, `gen_monolith_ops` (custom ops), `socket.gethostname()`.
- Side effects: Custom ops create a Deep Insight client resource and emit metrics to a databus; may dump to file if `dump_filename` is provided by the op.

**Required Behavior (Detailed)**
- Constants: `_FEATURE_REQ_TIME`, `_SAMPLE_RATE`, `_UID` are defined but unused in this file.
- `deep_insight_client(enable_metrics_counter=False, is_fake=False, dump_filename=None, container=socket.gethostname())`:
  - Default `container` is evaluated at import time (module load), not at call time.
  - Calls `deep_insight_ops.monolith_create_deep_insight_client(enable_metrics_counter, is_fake, dump_filename, container)`.
  - Returns a `tf.Tensor` handle to the client resource.
- `write_deep_insight(...)`:
  - Args:
    - `deep_insight_client_tensor`: handle from `deep_insight_client`.
    - `uids`: 1-D int64 tensor.
    - `req_times`: 1-D int64 tensor.
    - `labels`, `preds`, `sample_rates`: 1-D float tensors.
    - `model_name`, `target`: strings.
    - `sample_ratio` float (default 0.01).
    - `return_msgs` bool: whether op returns serialized messages.
    - `use_zero_train_time` bool: if True uses 0 as train time (tests).
  - Calls `monolith_write_deep_insight` op with named args and returns 1-D string tensor.
- `write_deep_insight_v2(...)`:
  - Args:
    - `req_times`: 1-D int64 tensor (batch_size).
    - `labels`, `preds`, `sample_rates`: 2-D float tensors of shape `(num_targets, batch_size)`.
    - `extra_fields_values`: list of 1-D tensors (each batch_size).
    - `extra_fields_keys`: list of strings, same length as `extra_fields_values`.
    - `targets`: list of strings (num_targets).
  - Calls `monolith_write_deep_insight_v2` op with named args and returns 1-D string tensor.
- No Python-side validation of shapes/dtypes; relies on op validation.
- Threading/concurrency: op-level; Python wrapper is pure.
- Determinism: depends on external Deep Insight system; no RNG here.
- Logging/metrics: metrics emission happens inside the custom op.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (custom TF ops not bound in Rust).
- Rust public API surface: optional wrappers when TF-runtime backend is present.
- Data model mapping: Tensor handles and string vectors must map to TF runtime types.
- Feature gating: TF-runtime + custom ops only.
- Integration points: training metrics pipeline, databus output.

**Implementation Steps (Detailed)**
1. Expose `monolith_create_deep_insight_client` and write ops in Rust TF-runtime backend (FFI).
2. Mirror function signatures and defaults (especially container default semantics).
3. Add validation if desired, but preserve op behavior for parity.
4. Add tests using fake client mode (`is_fake=True`) to avoid external dependencies.

**Tests (Detailed)**
- Python tests: `monolith/native_training/metric/deep_insight_ops_test.py`.
- Rust tests: N/A until TF custom ops are bound.
- Cross-language parity test: compare returned messages (if `return_msgs=True`) and shape/dtype.

**Gaps / Notes**
- Requires custom TF ops; no Rust bindings exist today.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/metric/deep_insight_ops_test.py`
<a id="monolith-native-training-metric-deep-insight-ops-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 33
- Purpose/role: Placeholder test module for Deep Insight ops; currently no assertions.
- Key symbols/classes/functions: `DeepInsightOpsTest.dummy_test`.
- External dependencies: TensorFlow test harness, `absl.logging`, `json`, `time` (unused).
- Side effects: None.

**Required Behavior (Detailed)**
- `DeepInsightOpsTest.dummy_test`: no-op; test does nothing.
- `__main__` block disables eager execution and runs `tf.test.main()`.
- No validation of deep insight ops; effectively a stub.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (empty test).
- Rust public API surface: none.
- Data model mapping: none.
- Feature gating: none.
- Integration points: none.

**Implementation Steps (Detailed)**
1. If Deep Insight ops are implemented in Rust, add real tests; otherwise keep as stub-equivalent.

**Tests (Detailed)**
- Python tests: `monolith/native_training/metric/deep_insight_ops_test.py` (no assertions).
- Rust tests: none.
- Cross-language parity test: not applicable until ops exist.

**Gaps / Notes**
- Tests are effectively empty; add real assertions when ops are implemented.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/metric/exit_hook.py`
<a id="monolith-native-training-metric-exit-hook-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 48
- Purpose/role: Installs signal handlers and an `atexit` hook that emits an "exit_hook" counter when the process exits due to a signal.
- Key symbols/classes/functions: `sig_no`, `sig_handler`, `exit_hook`.
- External dependencies: `atexit`, `signal`, `sys`, `monolith.native_training.utils`, `native_task_context`, `metric.cli`.
- Side effects: Registers signal handlers on import; registers an `atexit` handler; may call `sys.exit` on signal.

**Required Behavior (Detailed)**
- Module global `sig_no` initialized to `None`.
- `sig_handler(signo, frame)`:
  - Sets global `sig_no = signo`.
  - Calls `sys.exit(signo)` to terminate process.
- On import, installs handlers:
  - `signal.signal(signal.SIGHUP, sig_handler)`
  - `signal.signal(signal.SIGINT, sig_handler)`
  - `signal.signal(signal.SIGTERM, sig_handler)`
- `exit_hook()` (decorated with `@atexit.register`):
  - Fetches context via `native_task_context.get()`.
  - Builds metric client: `cli.get_cli(utils.get_metric_prefix())`.
  - `index = ctx.worker_index` if `ctx.server_type == 'worker'`, else `ctx.ps_index`.
  - Builds tags: `server_type`, `index` (string), `sig` (stringified `sig_no`).
  - Only emits counter if `sig_no is not None`: `mcli.emit_counter("exit_hook", 1, tags)`.
- No explicit error handling; depends on `native_task_context` and `cli` behavior.
- Determinism: signal arrival timing; otherwise deterministic.
- Logging/metrics: emits a counter metric when terminating due to signal.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (no Rust signal/exit hook yet).
- Rust public API surface: optional `exit_hook` module that registers signal handlers + exit hook.
- Data model mapping: tag map `server_type/index/sig` -> metrics client.
- Feature gating: none; only needed if Rust training/runtime needs parity.
- Integration points: metrics client (Rust equivalent of `cli.get_cli`), task context.

**Implementation Steps (Detailed)**
1. Implement signal handling in Rust (e.g., `signal-hook`) for HUP/INT/TERM.
2. Record the signal number in a global and trigger process exit.
3. Register an exit hook that emits `exit_hook` counter with identical tags.
4. Mirror `server_type`/index selection from `native_task_context`.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: optional integration test using signal simulation.
- Cross-language parity test: validate emitted tags and counter name.

**Gaps / Notes**
- Python `cli.get_cli` is a stub; metric emission may be a no-op unless replaced.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/metric/kafka_utils.py`
<a id="monolith-native-training-metric-kafka-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 119
- Purpose/role: Simple Kafka producer wrapper with a background thread + queue; tracks send counters.
- Key symbols/classes/functions: `KProducer`, `KProducer.send`, `KProducer.close`.
- External dependencies: `kafka.KafkaProducer`, `queue.Queue`, `threading.Thread/RLock`, `absl.logging`, `time`.
- Side effects: Starts a background thread on init; sends messages to Kafka.

**Required Behavior (Detailed)**
- `KProducer.__init__(brokers, topic)`:
  - Stores `brokers` and `topic`.
  - Creates `KafkaProducer(bootstrap_servers=brokers)`.
  - Initializes `_lock` (RLock), `_has_stopped=False`, `_msg_queue=Queue()`.
  - Initializes counters `_total`, `_success`, `_failed` to 0.
  - Spawns background thread targeting `_poll` and starts it.
- `send(msgs)`:
  - If `msgs` is `None` or empty, returns immediately.
  - If `msgs` is `str` or `bytes`, wraps into a list.
  - Else filters iterable to only non-`None` entries with `len(msg) > 0`.
  - If resulting list is non-empty:
    - Logs first message up to 10 times via `logging.log_first_n(INFO, msgs[0], n=10)`.
    - Increments `_total` by `len(msgs)`.
    - Enqueues the list into `_msg_queue`.
  - No encoding/conversion; message passed to KafkaProducer as-is.
- `_poll()` (background thread):
  - Loop: `msg_batch = _msg_queue.get(timeout=1)`.
  - On any exception (e.g., timeout), checks `_has_stopped` under lock:
    - If stopped: break; else continue.
  - If `msg_batch` non-empty: sends each message via `producer.send(topic, msg)` and attaches callbacks:
    - `_send_success` for success, `_send_failed` for error.
  - After processing a batch, exits if `_has_stopped` is True.
- `total()`, `success()`, `failed()`:
  - Return counters; not synchronized across threads (may race).
- `_flush()`:
  - Asserts `_has_stopped` is True.
  - Drains `_msg_queue` (timeout=1) until empty or exception.
  - Sends queued messages with callbacks (same as `_poll`).
- `close()`:
  - Sets `_has_stopped=True` under lock.
  - Joins background thread.
  - Calls `_flush()` and then `producer.close(timeout=1)`.
  - Logs warnings on any exception.
- `_send_success(...)`: increments `_success`.
- `_send_failed(...)`: sleeps 2 seconds, logs warning, increments `_failed`.
- Threading/concurrency: background thread + queue; counters are not locked.
- Determinism: none; dependent on Kafka/network.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (no Rust Kafka wrapper).
- Rust public API surface: optional Kafka producer wrapper with background worker.
- Data model mapping: messages as `Vec<u8>` or `String`; track counters.
- Feature gating: requires a Rust Kafka client (e.g., `rdkafka`).
- Integration points: metrics emission pipeline.

**Implementation Steps (Detailed)**
1. Choose Kafka client crate and implement a background worker with channel/queue.
2. Mirror `send` behavior (filtering, logging first message, counters).
3. Implement graceful shutdown (stop flag, join thread, flush queue).
4. Provide success/failure counters and expose them.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: add unit tests with a mocked producer or test broker.
- Cross-language parity test: compare counters and send filtering behavior.

**Gaps / Notes**
- Python implementation is not thread-safe for counters; preserve semantics unless explicitly improved.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/metric/metric_hook.py`
<a id="monolith-native-training-metric-metric-hook-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 563
- Purpose/role: Collection of TensorFlow Estimator hooks for metrics, profiling, Kafka/file logging, and telemetry.
- Key symbols/classes/functions: `ThroughputMetricHook`, `StepLossMetricHook`, `CustomMetricHook`, `Tf2ProfilerHook`, `ByteCCLTelemetryHook`, `NVProfilerHook`, `KafkaMetricHook`, `FileMetricHook`, `WriteOnlyFileAndStat`, helper functions (`default_parse_fn`, `default_layout_fn`, `vepfs_layout_fn`, `vepfs_key_fn`).
- External dependencies: TensorFlow Estimator hooks, TF profiler, BytePS telemetry, Kafka (via `KProducer`), `tf.io.gfile`, `absl.flags/logging`, `alert_manager`, `alert_pb2`.
- Side effects: Registers exit hook via import (`exit_hook`), starts background threads, writes to Kafka/files, may start TF profiler server on port 6666.

**Required Behavior (Detailed)**
- Module globals:
  - `FLAGS = flags.FLAGS` used by `Tf2ProfilerHook`.
  - Importing `exit_hook` executes signal/atexit registration for metrics.
- `ThroughputMetricHook`:
  - `__init__(model_name, start_time_secs, cluster_type="stable", run_every_n_secs=30)`:
    - Initializes counters and `self._mcli = cli.get_cli(utils.get_metric_prefix())`.
    - If alert manager exists, creates `AlertProto` and registers rules with prefix.
  - `begin()`: sets `self._global_step_tensor = tf.compat.v1.train.get_global_step()`.
  - `before_run(run_context)`:
    - On first step, reads `global_step` via `session.run`.
    - Records `emit_time` (int seconds).
    - If `start_time_secs` provided, emits timer `run_start_elapsed_time.all` with tags `{model_name, cluster_type}`.
    - Returns `SessionRunArgs({"global_step": global_step_tensor})`.
  - `after_run(run_context, run_values)`:
    - If elapsed wall time >= `run_every_n_secs`, emits:
      - `run_steps.all` counter (step interval).
      - `run_steps_elapsed_time.all` timer (elapsed_time / step_interval).
    - Updates emit step/time. (No guard against `step_interval == 0`.)
- `StepLossMetricHook`:
  - `__init__(loss_tensor)` stores tensor and mcli.
  - `before_run`: requests loss tensor.
  - `after_run`: emits `step_loss` store with loss value.
- `CustomMetricHook`:
  - `__init__(metric_tensors)`:
    - Validates each tensor is scalar (rank 0) and dtype in `{tf.float32, tf.int32}`.
    - Raises `ValueError` if invalid or if metric list empty.
  - `before_run`: requests all metric tensors.
  - `after_run`: emits each metric as float via `emit_store`.
- `Tf2ProfilerHook`:
  - `__init__(logdir, init_step_range, save_steps=None, save_secs=None, options=None)`:
    - Validates `end_step > start_step` when provided.
    - Sets `delta = end_step - start_step` or default 10.
    - If `save_steps` provided and `<= delta`, raises `ValueError`.
    - Creates `SecondOrStepTimer(every_steps=save_steps, every_secs=save_secs)`.
  - `begin()`:
    - If `FLAGS.enable_sync_training` tries `tf.profiler.experimental.server.start(6666)`; logs warning on failure.
  - `before_run`:
    - If profiling, creates `_pywrap_traceme.TraceMe("TraceContext", graph_type="train", step_num=current_step)` for step-time graph fix.
    - Returns `SessionRunArgs(fetches=None)`.
  - `after_run`:
    - Increments `current_step`.
    - Stops TraceMe if active.
    - If `start_step` is None, defers profiling to `current_step + 500` with default delta.
    - Stops profiling when `current_step >= end_step`.
    - If timer triggers, starts profiling and sets new `[start_step, end_step)` window.
  - `end(sess)`: stops profiling if active.
  - `_start_profiling()`: `tf.profiler.experimental.start(logdir, options)`; ignores `AlreadyExistsError`.
  - `_stop_profiling()`: calls `tf.profiler.experimental.stop()`; ignores `UnavailableError`.
- `ByteCCLTelemetryHook`:
  - Requires global step tensor (`training_util._get_or_create_global_step_read()`), else `RuntimeError`.
  - Logs telemetry every `interval` steps by sampling BytePS ops on rank 0.
  - `_log_telemetry()` filters ops containing `alltoall` or first 3 `PushPull` entries.
- `NVProfilerHook`:
  - Subclass of `Tf2ProfilerHook` with `logdir=None`.
  - Loads `libcudart.so` and calls `cudaProfilerStart/Stop`.
- `KafkaMetricHook` (singleton):
  - Uses `KAFKA_BROKER_LIST` and `KAFKA_TOPIC_NAME` env vars to create `KProducer`.
  - `__init__`: loads `deep_insight_op` from TF collection if not provided; stores as tensor dict.
  - `after_run`: sends `deep_insight_op` messages to Kafka if any.
  - `end`: closes producer, logs success/failed counts.
- Helper functions:
  - `default_parse_fn`: JSON-decodes strings/bytes; otherwise returns input.
  - `default_layout_fn`: returns string or JSON dump; falls back to `repr` on error.
  - `vepfs_layout_fn`: formats deep insight record as `req_time;gid;uid;predict_scores;labels`.
  - `vepfs_key_fn`: builds path `base/model_name/date/worker_{id}`.
- `WriteOnlyFileAndStat`:
  - Holds buffered output; rotates partitions after `partition_size` lines (default 1e6).
  - Uses `tf.io.gfile` to write `part_XXXXXX.{file_ext}` under `key` directory.
  - `write()` buffers formatted strings; `flush()` writes and rotates; `close()` closes stream.
  - `is_available()` returns True if updated within last 24 hours.
  - Note: uses `List`/`Dict` typing annotations without importing them (potential NameError at runtime).
- `FileMetricHook` (singleton):
  - Initializes from `deep_insight_op` collection if not provided.
  - Requires `key_fn` for routing items; if `None`, `_send` will fail when called.
  - Spawns background thread on first `after_run`.
  - Enqueues messages (handles list/tuple/np.ndarray or scalar).
  - `_send` parses items, writes to per-key `WriteOnlyFileAndStat`, and cleans up inactive files every 10 minutes.
  - `end` waits for queue to drain, stops thread, closes open files.
- Threading/concurrency: multiple background threads; queue for metrics, RLock in file writer.
- Determinism: depends on timing, Kafka/network, filesystem.
- Logging/metrics: uses `mcli.emit_*`, absl logging, Kafka/file outputs.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (TF Estimator hooks and Kafka/file hooks not present in Rust).
- Rust public API surface: if needed, add a training hooks module with metrics, profiling, and output sinks.
- Data model mapping: map TF hooks to Rust training loop callbacks; map `deep_insight_op` outputs to Rust equivalents.
- Feature gating: Kafka and profiler hooks should be optional (feature flags).
- Integration points: training loop, metrics client, optional BytePS/collective telemetry.

**Implementation Steps (Detailed)**
1. Decide which hooks are needed in Rust training (throughput, loss, custom metrics).
2. Implement throughput/loss hooks as callbacks in Rust training loop.
3. Provide profiling hooks only if profiling support exists (TF2/NV profilers likely N/A).
4. Implement Kafka/File output sinks if required; reuse Rust Kafka + filesystem abstractions.
5. Match environment-variable configuration for Kafka (`KAFKA_BROKER_LIST`, `KAFKA_TOPIC_NAME`).
6. Preserve thread/queue behavior and file partitioning semantics.
7. Add tests for validation errors (CustomMetricHook), file rotation, and queue draining.

**Tests (Detailed)**
- Python tests: `monolith/native_training/metric/metric_hook_test.py`.
- Rust tests: N/A until hooks exist.
- Cross-language parity test: validate emitted metrics names/tags and file output formatting.

**Gaps / Notes**
- `List`/`Dict` are used in annotations without import; may require adding `from typing import List, Dict` in Python for runtime use.
- `FileMetricHook` will fail if `key_fn` is not provided; ensure callers pass `vepfs_key_fn` or a custom function.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/metric/metric_hook_test.py`
<a id="monolith-native-training-metric-metric-hook-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 189
- Purpose/role: Tests for `Tf2ProfilerHook` and `FileMetricHook` behaviors.
- Key symbols/classes/functions: `Tf2ProfilerHookTest`, `FileMetricHookTest`.
- External dependencies: TensorFlow, `os`, `time`, `json`, `random`, `datetime`.
- Side effects: Writes profiling data under `TEST_TMPDIR` and file metrics under `$HOME/tmp/file_metric_hook`.

**Required Behavior (Detailed)**
- `Tf2ProfilerHookTest`:
  - `setUp`:
    - `logdir = $TEST_TMPDIR/<test_name>`.
    - `filepattern = logdir/plugins/profile/*`.
    - Creates a graph with global_step and train_op (`assign_add` by 1).
  - `_count_files()` returns count of files matching pattern.
  - `test_steps`:
    - Hook: `Tf2ProfilerHook(logdir, init_step_range=[0,10], save_steps=50)`.
    - Runs one step in `SingularMonitoredSession`.
    - Expects exactly 1 profile file.
  - `test_multiple_steps_1`:
    - Hook with `save_steps=30`, runs 30 steps with 0.15s sleep.
    - Expects 1 file (profile only at 0~9).
  - `test_multiple_steps_2`:
    - Same hook, runs 31 steps with 0.15s sleep.
    - Expects 2 files (0~9 and step 30).
  - `test_secs_1`:
    - Hook with `save_secs=1`, runs 10 steps with 0.15s sleep.
    - Expects at least 1 file.
  - `test_secs_2`:
    - Hook with `save_secs=3`, runs 21 steps with 0.15s sleep.
    - Expects at least 2 files.
- `FileMetricHookTest`:
  - `setUpClass`:
    - `model_name='test_model'`, `base_name=$HOME/tmp/file_metric_hook`.
    - Creates `FileMetricHook(worker_id=0, key_fn=vepfs_key_fn, layout_fn=vepfs_layout_fn, batch_size=8, partition_size=32)`.
  - `tearDownClass`:
    - Calls `hook.end(None)` to flush/close.
    - For each of last 8 days, asserts:
      - date directory exists under `base_name/model_name/<YYYYMMDD>/worker_0/`.
      - exactly 2 files exist; each has 32 lines.
  - `test_vepfs_key_fn`:
    - Asserts path formatting for fixed data.
  - `test_vepfs_layout_fn`:
    - Asserts formatted string with predict/label JSON and fallback `gid`.
  - `test_after_run`:
    - Builds `RunValue` wrapper with `results={'deep_insight_op':[json.dumps(rv)]}`.
    - For last 8 days, sends 64 records/day with random predict/label values.
    - Calls `hook.after_run` to enqueue metrics; file writing validated in `tearDownClass`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (TF profiler and FileMetricHook not implemented in Rust).
- Rust public API surface: if implemented, provide equivalent tests for profiling triggers and file partitioning.
- Data model mapping: file output format must match `vepfs_layout_fn` and `vepfs_key_fn`.
- Feature gating: profiling/Kafka/file outputs should be optional.
- Integration points: Rust training hook system.

**Implementation Steps (Detailed)**
1. If Rust supports profiling hooks, add tests for step/second trigger behavior.
2. If file output hook is implemented, port these tests with deterministic data (no randomness).
3. Ensure file partitioning at 32 lines and 2 files per day for 64 records.

**Tests (Detailed)**
- Python tests: `monolith/native_training/metric/metric_hook_test.py`.
- Rust tests: N/A until hooks are implemented.
- Cross-language parity test: compare file outputs and profile dump counts if available.

**Gaps / Notes**
- Tests rely on filesystem and time sleeps; may be flaky or slow.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/metric/utils.py`
<a id="monolith-native-training-metric-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 104
- Purpose/role: Convenience wrapper to emit Deep Insight metrics using custom TF ops (v1 or v2).
- Key symbols/classes/functions: `write_deep_insight`.
- External dependencies: TensorFlow, `deep_insight_ops`, Python `logging`.
- Side effects: Calls custom ops that create clients and emit metrics; logs when disabling.

**Required Behavior (Detailed)**
- `write_deep_insight(features, sample_ratio, model_name, labels=None, preds=None, target=None, targets=None, labels_list=None, preds_list=None, sample_rates_list=None, extra_fields_keys=[], enable_deep_insight_metrics=True, enable_kafka_metrics=False, dump_filename=None)`:
  - Requires `features["req_time"]`; if missing:
    - Logs "Disabling deep_insight because req_time is absent".
    - Returns `tf.no_op()`.
  - `is_fake = enable_kafka_metrics or (dump_filename is not None and len(dump_filename) > 0)`.
  - Creates client: `deep_insight_ops.deep_insight_client(enable_deep_insight_metrics, is_fake, dump_filename)`.
  - `req_times = reshape(features["req_time"], [-1])`.
  - **Single-target path** (`not targets`):
    - `uids = reshape(features["uid"], [-1])`.
    - `sample_rates = reshape(features["sample_rate"], [-1])`.
    - Calls `deep_insight_ops.write_deep_insight` with `labels`, `preds`, `model_name`, `target`, `sample_ratio`, `return_msgs=is_fake`.
  - **Multi-target path** (`targets` truthy):
    - `labels = stack([label if rank==1 else reshape(label, (-1,)) for label in labels_list if label is not None])`.
    - `preds = stack([pred if rank==1 else reshape(pred, (-1,)) for pred in preds_list if pred is not None])`.
    - `sample_rates_list` handling:
      - If falsy: uses `features["sample_rate"]` reshaped to [-1] and repeats `len(targets)` times.
      - If list/tuple: reshapes each to rank 1; filters out None.
      - Else raises `Exception("sample_rates_list error!")`.
    - `sample_rates = stack(sample_rates_list)`.
    - Ensures `"uid"` in `extra_fields_keys` (mutates list default).
    - Builds `extra_fields_values` by reshaping each `features[key]` to [-1].
    - Calls `deep_insight_ops.write_deep_insight_v2` with `targets`, `extra_fields_*`, `return_msgs=is_fake`.
  - Returns the op tensor from deep_insight ops.
- Error cases:
  - Missing `uid`/`sample_rate`/extra fields -> `KeyError`.
  - Empty `labels_list`/`preds_list` -> `tf.stack` error.
  - `sample_rates_list` non-list and truthy -> raises generic `Exception`.
- Mutability note: `extra_fields_keys` default list is mutated when adding `"uid"`.
- No threading; deterministic aside from op behavior.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (custom TF ops not bound in Rust).
- Rust public API surface: optional wrapper around TF runtime deep insight ops.
- Data model mapping: feature tensors, string targets, list-of-tensors for multi-target.
- Feature gating: TF runtime + custom ops only.
- Integration points: metrics pipeline in training.

**Implementation Steps (Detailed)**
1. Add TF runtime bindings for deep insight ops if needed.
2. Mirror single-target vs multi-target branching.
3. Preserve `is_fake` semantics and `return_msgs`.
4. Avoid mutable default pitfalls if porting (but keep behavior if parity requires).

**Tests (Detailed)**
- Python tests: `monolith/native_training/metric/utils_test.py`.
- Rust tests: N/A until ops are bound.
- Cross-language parity test: verify that v1/v2 calls receive the same tensors/flags.

**Gaps / Notes**
- `extra_fields_keys` uses a mutable default list; repeated calls may accumulate `"uid"`.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/metric/utils_test.py`
<a id="monolith-native-training-metric-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 50
- Purpose/role: Tests basic call path for `utils.write_deep_insight` using a mocked op.
- Key symbols/classes/functions: `DeepInsightTest.test_basic`.
- External dependencies: TensorFlow, `unittest.mock`.
- Side effects: None (deep insight op is mocked).

**Required Behavior (Detailed)**
- `test_basic`:
  - Patches `deep_insight_ops.write_deep_insight` and sets a side-effect function.
  - `fake_call` evaluates `uids` tensor in a session and asserts it equals `[1,2,3]`.
  - Constructs `features` with `uid`, `req_time`, `sample_rate` tensors.
  - Creates `labels`, `preds`, `model_name`, `target`.
  - Calls `utils.write_deep_insight(...)`.
  - Note: Call uses positional arguments in a non-obvious order; still exercises `uids` extraction.
- `__main__` disables eager execution and runs `tf.test.main()`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: none.
- Feature gating: TF runtime only.
- Integration points: deep insight wrapper tests if implemented.

**Implementation Steps (Detailed)**
1. If deep insight ops are bound in Rust, add a unit test to ensure `uid` extraction and reshape behavior.
2. Prefer explicit keyword arguments to avoid positional mis-ordering.

**Tests (Detailed)**
- Python tests: `monolith/native_training/metric/utils_test.py`.
- Rust tests: N/A until implementation exists.
- Cross-language parity test: verify tensor shapes/values passed to op.

**Gaps / Notes**
- The test passes arguments positionally in a confusing order relative to the function signature; keep behavior but consider fixing in Python if allowed.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/mlp_utils.py`
<a id="monolith-native-training-mlp-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 444
- Purpose/role: Utilities for MLP/YARN distributed training setup, tf.data service orchestration, MPI exception handling, and TF profiler control.
- Key symbols/classes/functions: `check_port`, `MLPEnv`, `add_mpi_exception_hook`, `mlp_pass`, `begin`, `after_create_session`, `EXTRA_DSWORKERS`.
- External dependencies: TensorFlow, tf.data service (`dsvc`), MPI/Horovod/BytePS, `yarn_runtime`, `distribution_utils`, `absl.flags/logging`, `socket`, `signal`, `subprocess`.
- Side effects: Opens sockets, starts TF DataService dispatcher/worker servers, starts TF profiler server, sets `sys.excepthook`, calls `os._exit(0)`, monkeypatches `_DatasetInitializerHook` methods.

**Required Behavior (Detailed)**
- `check_port(host, port, timeout=1)`:
  - Opens IPv4 or IPv6 socket based on host string (`':' in host.strip('[]')`).
  - Attempts to connect within `timeout` seconds; returns True if connected.
  - On timeout returns False.
  - On socket error, retries until timeout expires; returns False otherwise.
- `MLPEnv.__init__()`:
  - Collects env vars starting with `MLP_` or `MPI_`.
  - Reads `MLP_FRAMEWORK`, `MLP_SSH_PORT`, `MLP_LOG_PATH`, `MLP_DEBUG_PORT`, `MLP_ENTRYPOINT_DIR`, `MLP_TASK_CMD`, `MLP_ROLE`.
  - Builds `all_roles` from env vars like `MLP_<ROLE>_NUM`.
  - If `enable_mpi` (OMPI rank exists and role is WORKER):
    - `index = get_mpi_rank()`, `all_roles['WORKER'] = get_mpi_size()`.
    - `port = int(MLP_PORT) + index`.
  - Else uses `MLP_ROLE_INDEX` and `MLP_PORT`.
  - `avaiable` is True if MLP env and roles exist.
  - Records `cpu`, `gpu`, `gpu_type`, `mem` from env.
  - `host = yarn_runtime.get_local_host()`.
- `MLPEnv.enable_mpi`:
  - True when `OMPI_COMM_WORLD_RANK` exists and role is WORKER.
- `MLPEnv._get(name, default=None)`:
  - Reads from `_mlp_env`, strips quotes; returns default if missing.
- `num_replicas(role=None)`:
  - If MPI worker, returns `get_mpi_size()`; else reads `MLP_<ROLE>_NUM`.
- `get_all_host(role=None, is_primary=True)`:
  - Returns `MLP_<ROLE>_ALL_PRIMARY_HOSTS` or `MLP_<ROLE>_ALL_HOSTS` value.
- `get_all_addrs(role=None, is_primary=True)`:
  - Reads `MLP_<ROLE>_ALL_PRIMARY_ADDRS`/`ALL_ADDRS`, splits by comma, else [].
- `get_host(role=None, index=None, is_primary=True)`:
  - Uses MPI/local index logic; returns `MLP_<ROLE>_<index>_PRIMARY_HOST` or `_HOST`.
- `get_addr(role=None, index=None, is_primary=True)`:
  - Computes host and port:
    - MPI worker uses `MLP_<ROLE>_0_PORT` + index.
    - Otherwise `MLP_<ROLE>_<index>_PORT`.
  - Returns `"host:port"` or None.
- `get_port(role=None, index=None)`:
  - MPI worker: `MLP_<ROLE>_0_PORT` (default 2222) + index.
  - Else: `MLP_<ROLE>_<index>_PORT` (default 2222).
  - Note: `_get` returns strings; may require int conversion for correctness.
- `dispatcher_target()`:
  - Returns `grpc://{dispatcher_addr}` or `grpc://localhost:5050`.
- `dispatcher_addr(role=None)`:
  - Uses role default `'dispatcher'` and `get_addr`.
- `wait(role=None, index=0, timeout=-1, use_ssh=True)`:
  - Repeatedly calls `check_port` on `host:port`, sleeping 5s until ready or timeout.
  - Uses `ssh_port` when `use_ssh=True`.
- `join(role='worker', index=0, use_ssh=True)`:
  - Waits for host to come up, then loops until port stops responding (timeout=60 per check).
  - Stops TF profiler if started and calls `os._exit(0)`.
- `queue_device` property:
  - Returns `/job:ps/task:0/device:CPU:0` if PS exists; else worker CPU or local CPU.
- `start_profiler(port=6666)`:
  - Starts `tf.profiler.experimental.server` (port offset by MPI index).
- `profiler_trace(...)`:
  - Builds `ProfilerOptions` and calls `tf.profiler.experimental.client.trace`.
  - Uses all addresses if `index < 0`, otherwise specific address.
- `add_mpi_exception_hook()`:
  - If OMPI rank not set, returns.
  - Installs `sys.excepthook` that prints error details and calls `mpi4py.MPI.COMM_WORLD.Abort(1)`.
- `EXTRA_DSWORKERS`: global list of extra WorkerServer handles.
- `mlp_pass(dispatcher_role='dispatcher', dsworker_role='dsworker', worker_role='worker', ps_role='ps')`:
  - Uppercases roles; if `FLAGS.dataset_use_dataservice`, monkeypatches `_DatasetInitializerHook.begin` and `.after_create_session`.
  - Creates `MLPEnv`. If available:
    - Dispatcher role: starts `dsvc.DispatchServer` on `mlp_env.port`, then `mlp_env.join()`.
    - Dsworker role: waits for dispatcher, starts `dsvc.WorkerServer`, starts profiler, then join.
    - Worker role:
      - If dataset service enabled, waits for dispatcher and dsworkers, sets `FLAGS.data_service_dispatcher`.
      - Starts extra dsworkers on GPU worker based on `FLAGS.num_extra_dsworker_on_gpu_worker`.
      - Logs worker start info and roles.
- `begin(self)` (monkeypatched into `_DatasetInitializerHook`):
  - Stores iterator initializer; sets `_broadcast_dataset_id=None`.
  - If sync training enabled, tries to import BytePS or Horovod (based on `MONOLITH_WITH_BYTEPS`).
  - If `registed_dataset_id` collection exists, broadcasts dataset_id from rank 0 and stores `_broadcast_dataset_id`.
  - Calls `graph.clear_collection(...)` (graph is undefined; potential bug).
- `after_create_session(self, session, coord)`:
  - If `_broadcast_dataset_id` present, runs it to log dataset ids and clears it.
  - Runs iterator initializer.
- Threading/concurrency: uses tf.data service servers and background processes; no explicit threads here.
- Determinism: depends on environment, networking, MPI rank.
- Logging/metrics: extensive absl logging.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (TF data service + MPI orchestration not in Rust).
- Rust public API surface: if needed, separate runtime module for distributed training env/launcher.
- Data model mapping: env var parsing, host/port resolution, dataset service endpoints.
- Feature gating: MPI/BytePS/TF data service features optional.
- Integration points: training launcher, profiler integration.

**Implementation Steps (Detailed)**
1. Determine whether Rust training needs MLP/YARN orchestration features.
2. If yes, implement env parsing and host/port logic, plus dispatcher/worker lifecycle.
3. Implement MPI exception handling with `mpi` crate equivalents.
4. Provide profiler server/trace integration if applicable.
5. Mirror dataset initializer hook behavior in Rust data input pipeline.

**Tests (Detailed)**
- Python tests: none in repo.
- Rust tests: add unit tests for env parsing and host/port computation.
- Cross-language parity test: compare computed addresses/ports for fixed env maps.

**Gaps / Notes**
- `begin()` references `graph.clear_collection` but `graph` is undefined (likely bug).
- `get_port` uses `_get` without int conversion; may return strings.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model.py`
<a id="monolith-native-training-model-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 182
- Purpose/role: Defines a small FFM (field-aware factorization machine) test model and input pipeline for native training.
- Key symbols/classes/functions: `_parse_example`, `TestFFMModel`, `FFMParams`, constants `_NUM_SLOTS`, `_FFM_SLOT`, `_VOCAB_SIZES`, `_NUM_EXAMPLES`.
- External dependencies: TensorFlow, NumPy, Monolith feature/entry APIs, `deep_insight_ops`.
- Side effects: Sets NumPy seed in input function; emits deep insight metrics in training; logs model info.

**Required Behavior (Detailed)**
- Constants:
  - `_NUM_SLOTS = 6`, `_VOCAB_SIZES = [5,5,5,5,5,5]`, `_NUM_EXAMPLES = 64`.
  - `_FFM_SLOT` defines tuple pairs and embedding dim for dot products.
- `_parse_example(example: str) -> Dict[str, tf.Tensor]`:
  - Builds feature map with fixed label and VarLenFeature for each slot.
  - Parses examples with `tf.io.parse_example`.
  - Converts any `SparseTensor` to `RaggedTensor`.
  - Returns feature dict including `"label"` and slot features.
- `TestFFMModel(NativeTask)`:
  - `create_input_fn(mode)` returns `input_fn()`:
    - Sets `np.random.seed(0)` for deterministic example generation.
    - Generates `_NUM_EXAMPLES` examples via `generate_ffm_example(_VOCAB_SIZES)`.
    - Builds dataset: batch to `per_replica_batch_size`, map `_parse_example`, cache, repeat, prefetch.
  - `create_model_fn()` returns `model_fn(features, mode, config)`:
    - Creates feature slots/columns for each slot with FTRL bias + SGD default optimizer.
    - Collects bias embeddings for first half of slots.
    - Computes FFM dot products for each `(user,item,dim)` in `_FFM_SLOT`.
    - `ffm_out = add_n(dot_res) + sum_bias`, `pred = sigmoid(ffm_out)`.
    - If `mode == PREDICT`: returns `EstimatorSpec(predictions=pred)`.
    - Loss: `reduce_sum(binary_crossentropy(features["label"], pred))`.
    - If deep insight metrics enabled and sample ratio > 0:
      - Creates deep insight client, builds uids/req_times/sample_rates, calls `write_deep_insight`.
      - Logs model_name/target.
    - Else uses `tf.no_op()`.
    - Increments global step and applies embedding gradients via `ctx.apply_embedding_gradients`.
    - Returns `EstimatorSpec(loss=loss, train_op=group(apply_grads, deep_insight_op))`.
  - `create_serving_input_receiver_fn()`:
    - Creates string placeholder `instances`, parses via `_parse_example`, returns `ServingInputReceiver`.
- `FFMParams(SingleTaskModelParams)`:
  - `task()` returns `TestFFMModel.params()` with `per_replica_batch_size = 64`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (test model; no Rust equivalent).
- Rust public API surface: none.
- Data model mapping: if ported, map to Rust embedding feature slots and FFM dot products.
- Feature gating: none.
- Integration points: training pipeline and deep insight metrics.

**Implementation Steps (Detailed)**
1. If needed for parity demos, implement a Rust FFM example model and parser.
2. Mirror dataset generation determinism (`np.random.seed(0)`) with fixed RNG.
3. Match bias/slot construction and FFM dot-product structure.
4. Add deep insight metrics only if TF runtime backend exists.

**Tests (Detailed)**
- Python tests: none in repo.
- Rust tests: optional integration test for forward pass + loss.
- Cross-language parity test: compare forward outputs on fixed synthetic batch.

**Gaps / Notes**
- This is a test/sample model; may not be required for production parity.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_comp_test.py`
<a id="monolith-native-training-model-comp-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 183
- Purpose/role: Integration test comparing TF embedding updates vs Monolith embedding updates under sync training (Horovod).
- Key symbols/classes/functions: `EmbeddingUpdateTask`, `CpuSyncTrainTest`, `lookup_tf_embedding`.
- External dependencies: TensorFlow, Horovod, Monolith CPU training stack, Keras layers.
- Side effects: Sets environment variables at import; runs distributed training; writes model checkpoints under `/tmp/<user>/monolith_test/...`.

**Required Behavior (Detailed)**
- Module-level env vars (set before TF/Horovod import):
  - `MONOLITH_WITH_HOROVOD=True`, `HOROVOD_AUTOTUNE=1`, `HOROVOD_CYCLE_TIME=0.1`,
    `MONOLITH_SYNC_EMPTY_RANK0_PS_SHARD=0`, `MONOLITH_WITH_ALLREDUCE_FUSION=one`,
    `MONOLITH_ROOT_LOG_INTERVAL=10`.
- Sets TF v1 random seed to 42.
- Global constants: `num_features=17`, `batch_size=455`, `emb_dim=15`, `fid_max_val=100000`.
- `lookup_tf_embedding(features, f_name, dim)`:
  - Builds `RaggedTensor` from `tf_<f_name>_p1`/`p2`.
  - Embedding lookup on a zeros-initialized variable.
  - Returns `segment_sum` by row ids.
- `EmbeddingUpdateTask(MonolithModel)`:
  - `__init__`: sets `train.max_steps=50`, `train.per_replica_batch_size=batch_size`.
  - `input_fn`:
    - Generates random feature vectors with variable length per feature (1..24).
    - Uses `dense_to_ragged_batch` with batch_size and `advanced_parse`.
    - Adds `tf_feature{i}_p1/p2` to features for TF embedding lookup.
  - `model_fn`:
    - Creates embedding feature columns and Monolith embeddings.
    - Computes TF embeddings via `lookup_tf_embedding`.
    - Asserts Monolith embeddings equal TF embeddings.
    - Builds parallel Keras MLPs for both embedding sets, computes MSE losses.
    - Returns `EstimatorSpec` with combined loss, predictions, labels, head names, and optimizer.
  - `serving_input_receiver_fn`: unimplemented (`pass`).
- `CpuSyncTrainTest`:
  - `_create_config(gpu, multi_hash_table)` builds `DistributedCpuTrainingConfig` with sync training enabled.
  - `test_embedding_update`:
    - Initializes Horovod, runs distributed sync training in 2 configurations (cpu/multi-hash on/off).
    - If GPU available, repeats with GPU enabled.
- `__main__`: disables eager execution and runs `tf.test.main()`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (TF/Horovod integration test only).
- Rust public API surface: none.
- Data model mapping: none.
- Feature gating: Horovod/TF runtime only.
- Integration points: training loop parity for embedding updates.

**Implementation Steps (Detailed)**
1. If Rust aims to match embedding update semantics, port the comparison into Rust unit tests using Candle/TF backend.
2. Provide deterministic random feature generation for repeatability.
3. Mirror embedding lookup + segment sum behavior.

**Tests (Detailed)**
- Python tests: `monolith/native_training/model_comp_test.py`.
- Rust tests: none.
- Cross-language parity test: compare embedding tensors and loss values on fixed seeds.

**Gaps / Notes**
- `serving_input_receiver_fn` is unimplemented.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_dump/dump_utils.py`
<a id="monolith-native-training-model-dump-dump-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 757
- Purpose/role: Central model dump utility that records feature/slice metadata, input/output tensors, signatures, and graph defs into `ModelDump` protobufs.
- Key symbols/classes/functions: `DumpUtils`, `parse_input_fn_result`, wrappers `record_feature`, `record_slice`, `record_receiver`.
- External dependencies: TensorFlow graph/ops internals, protobufs (`model_dump` protos), pickle, `tf.io.gfile`, export context, data parsers.
- Side effects: Monkeypatches `util.parse_input_fn_result`; stores `ProtoModel` on default graph; writes/reads dump files.

**Required Behavior (Detailed)**
- `DumpUtils` is a singleton (`_instance`), `__init__` initializes fields only once:
  - `enable`, `_params`, `_run_config`, `_user_params`, train/infer `ProtoModel` + graph defs, sub-model caches, table configs, slice dims, feature combiners.
- `model_dump` property:
  - Attaches `ProtoModel()` to default graph as `graph.monolith_model_dump`.
- `update_kwargs_with_default(func, kwargs)`:
  - Fills `kwargs` entries with function default values when `None`.
- `record_feature(func)` wrapper:
  - When `need_record`, appends to `model_dump.features`.
  - Copies args/kwargs into proto, converting integer `feature_name` via `get_feature_name_and_slot`.
  - Logs warnings if field missing in proto.
- `record_slice(func)` wrapper:
  - Forbids `learning_rate_fn` (raises `Exception`).
  - Records slice config into `model_dump.emb_slices` including initializer/optimizer/compressor protos.
  - Calls wrapped function and appends output tensor names.
- `record_receiver(func)` wrapper:
  - Records serving input receiver features and receiver tensors into `serving_input_receiver_fn`.
  - Stores ragged tensors as `{values,row_splits,is_ragged}`; dense tensors include dtype/last_dim.
- `record_params(model)`:
  - Captures non-callable, non-private attrs except a skip list into `_params`.
- `get_params_bytes(model)`:
  - Pickles model attributes (including deep-copied `p` and serialized `_layout_dict`).
  - Returns pickled bytes; used by `add_model_fn`.
- `add_signature` / `restore_signature`:
  - Syncs signatures with current export context, mapping tensor names.
- `add_model_fn(model, mode, features, label, loss, pred, head_name, is_classification)`:
  - Fills `model_fn` proto with labels, loss, predictions, head names, classification flags.
  - Adds user summaries from `GraphKeys.SUMMARIES`.
  - Records non-ragged features not already registered.
  - Records extra losses from graph `__losses`.
  - Records `export_outputs` as `extra_output.fetch_dict`.
  - Enforces that only `ItemPoolSaveRestoreHook` is allowed in `__training_hooks`.
  - Stores signatures and SaveSliceInfo for variables.
  - Snapshots graph_def for TRAIN vs INFER into `train_graph`/`infer_graph`.
- `add_input_fn(results)`:
  - Records input feature tensor names and ragged flags; records label if present.
  - Stores parser type and item pool name.
- `add_sub_model(sub_model_type, name, graph)` / `restore_sub_model(sub_model_type)`:
  - Stores/restore sub-graph defs for PS or dense submodels via export context subgraphs.
- `add_optimizer(optimizer)`:
  - Pickles optimizer into `model_dump.optimizer`.
- `dump(fname)`:
  - Builds `ModelDump` proto with run config, user params, train/infer graphs, sub-models, table configs, slice dims, combiners.
  - Writes serialized bytes to `fname` using `tf.io.gfile`.
- `load(fname)`:
  - Reads `ModelDump` and reconstructs train/infer graph defs, table configs, feature slices, combiners, user params, sub-models.
- `get_graph_helper(mode)`:
  - Builds `GraphDefHelper` with SaveSliceInfo from train/infer model dump.
  - Caches on graph as `graph.graph_def_helper`.
- `restore_params()`:
  - Unpickles model params; rebuilds `_layout_dict` from `OutConfig` proto bytes.
  - Deletes `_training_hooks` key if present; raises if layout_dict missing.
- `need_record`:
  - True when `enable` and graph does not have `DRY_RUN` attribute.
- `table_configs` property/setter:
  - Converts between proto configs and `entry.HashTableConfigInstance`.
  - Setter disallows non-numeric `learning_rate_fns`.
- `feature_slice_dims` property/setter:
  - Converts between proto list and dict of dims.
- `feature_combiners` property/setter:
  - Maps `ReduceSum/ReduceMean/FirstN` to/from proto enum `Combiner`.
- `get_slot_to_occurrence_threshold` / `get_slot_to_expire_time`:
  - Builds slot->value maps; warns if slot resolution fails.
- `has_collected`:
  - True if table configs, slice dims, combiners all non-empty; otherwise asserts they are empty.
- `parse_input_fn_result(result)`:
  - If `DatasetV2`, makes iterator + `_DatasetInitializerHook`.
  - Else uses `DatasetInitHook` from collection `mkiter`.
  - Calls `DumpUtils().add_input_fn` and returns parsed iterator result + input hooks.
  - Monkeypatches `util.parse_input_fn_result`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (TF graph/proto dump infrastructure not in Rust).
- Rust public API surface: if parity required, add a model-dump module capturing graph metadata and feature configs.
- Data model mapping: Protobuf `ModelDump` -> Rust structs; tensor names as strings.
- Feature gating: TF runtime only if applicable.
- Integration points: model export, serving input receivers, embedding table configs.

**Implementation Steps (Detailed)**
1. Decide whether Rust needs model dump/export parity; define equivalent data structures.
2. Implement feature/slice recording and tensor-name capture.
3. Implement save/load of graph metadata and signatures.
4. Mirror validation (learning_rate_fn disallow, training hooks restrictions).

**Tests (Detailed)**
- Python tests: none in repo.
- Rust tests: add serialization/deserialization tests if implemented.
- Cross-language parity test: compare dumped proto fields for a simple model.

**Gaps / Notes**
- `export_outputs` branch uses `ts` variable that may be undefined when outputs are not dict (possible bug).
- `parse_input_fn_result` monkeypatch changes global TF estimator behavior.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_dump/graph_utils.py`
<a id="monolith-native-training-model-dump-graph-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 845
- Purpose/role: GraphDef utilities for reconstructing variables, importing subgraphs, and rebuilding input/model/receiver functions from dumped metadata.
- Key symbols/classes/functions: `DatasetInitHook`, `_node_name`, `_colocated_node_name`, `EchoInitializer`, `VariableDef`, `PartitionVariableDef`, `GraphDefHelper`.
- External dependencies: TensorFlow GraphDef/ops internals, protobufs (`LineId`, `FeatureConfigs`), `tf.keras`, `tf.io`, flags.
- Side effects: Mutates graph node attrs (`_class`), clears devices, adds to TF collections, imports graph defs into default graph.

**Required Behavior (Detailed)**
- Globals:
  - `DRY_RUN = 'dry_run'`, `FLAGS = flags.FLAGS`.
- `DatasetInitHook`: session hook that runs initializer in `after_create_session`.
- `_node_name(name)`:
  - Strips leading `^` and output suffix `:0`; returns node base name.
- `_colocated_node_name(name)`:
  - Decodes bytes to string; strips `loc:@` prefix if present.
- `EchoInitializer`:
  - Accepts an init value; if list/tuple uses first element.
  - Returns tensor directly if `tf.Tensor`, else returns op output (expects single output).
- `VariableDef`:
  - Wraps a `VarHandleOp` node and helper.
  - `initializer`: finds `<var>/Assign` node, determines initializer input, builds subgraph, imports it, and returns `EchoInitializer`.
  - `variable`: creates a `tf.get_variable` with dtype/shape/initializer on proper device, temporarily disabling partitioner; returns first partition if `PartitionedVariable`.
  - Tracks associated `ReadVariableOp` nodes via `add_read`.
- `PartitionVariableDef`:
  - Handles partitioned variables (`/part_N`); tracks partitions and read ops.
  - `get_base_name` extracts base variable name from VarHandleOp or ReadVariableOp inputs.
  - `initializer`: finds PartitionedInitializer slice nodes or Assign initializer nodes, imports subgraph, returns list of `EchoInitializer`.
  - `variable`: creates variables for each partition (uses first device as group_device), sets `save_slice_info`, and builds a `PartitionedVariable` for validation if multiple partitions.
- `GraphDefHelper.__init__(graph_def, save_slice_info)`:
  - Validates GraphDef type.
  - Clears node device and `_class` colocation hints (adds colocated names to input set).
  - Builds name-to-node, seq mapping, and tracks variables/readers into `VariableDef`/`PartitionVariableDef`.
  - Records PBDataset file_name const node if present.
- `_check_invalidate_node(graph_def, input_map)`:
  - Removes input_map entries not referenced by graph inputs; logs warnings.
- `_create_variables(variables)`:
  - Recreates variable read tensors using `read_variable_op` for all variable defs in subgraph.
  - Skips canonical `/Read/ReadVariableOp` nodes.
- `sub_graph(dest_nodes, source_nodes=None, with_library=True)`:
  - BFS from dest nodes through inputs; stops at source_nodes.
  - Builds a GraphDef containing non-variable nodes and collects variable names separately.
  - If `with_library`, copies required functions (including Dataset functions).
- `import_input_fn(input_conf, file_name)`:
  - Constructs dest_nodes from recorded output features and label; includes iterator ops unless DRY_RUN.
  - Updates PBDataset/file_name Const value to `file_name`.
  - Optionally updates PBDataset/input_pb_type based on `FLAGS.data_type`.
  - Imports subgraph; adds iterator/mkiter to collections.
  - Rebuilds features dict (ragged or dense) and adds label.
- `import_model_fn(input_map, proto_model)`:
  - Collects outputs from predict, extra outputs, loss, labels, extra_losses, signatures, summaries.
  - Builds subgraph, prunes input_map, recreates variable reads, imports graph_def.
  - Adds sparse feature names to collections by scanning ShardingSparseFids nodes.
  - Restores summaries to GraphKeys.SUMMARIES.
  - Validates signature inputs exist in graph.
  - Returns `(label, loss, predict, head_name, extra_output_dict, is_classification)`.
- `import_receiver_fn(receiver_conf)`:
  - Builds dest_nodes for ragged values/row_splits and dense features.
  - Populates collections: sparse_features, dense_features/types/shapes, extra_features/shapes, variant_type.
  - Imports subgraph and reconstructs feature tensors + receiver_tensors.
- `get_optimizer(proto_model)`:
  - Unpickles optimizer from proto bytes or returns None.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (TF GraphDef import is Python-specific).
- Rust public API surface: only relevant if a TF runtime backend is used for model import.
- Data model mapping: GraphDef + metadata + tensor names.
- Feature gating: TF runtime only.
- Integration points: model loader, serving input reconstruction.

**Implementation Steps (Detailed)**
1. If Rust needs TF graph import, wrap GraphDef parsing and node filtering.
2. Implement variable recreation and read-op mapping analogous to `_create_variables`.
3. Implement sub-graph extraction with function library filtering.
4. Recreate input/model/receiver functions using stored metadata and collections.

**Tests (Detailed)**
- Python tests: none in repo.
- Rust tests: N/A unless TF GraphDef import is added.
- Cross-language parity test: verify imported outputs match original graph outputs.

**Gaps / Notes**
- Uses `eval` on serialized feature representations (security risk if untrusted).
- Clears node device assignments; placement is not preserved.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_dump/graph_utils_test.py`
<a id="monolith-native-training-model-dump-graph-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 86
- Purpose/role: Tests `GraphDefHelper` input/receiver reconstruction using a saved model dump.
- Key symbols/classes/functions: `GraphUtilsTest.test_load_input_fn`, `test_load_receiver`, `test_load_mode`.
- External dependencies: TensorFlow, `DumpUtils`, `GraphDefHelper`.
- Side effects: Loads model dump from `model_dump/test_data/model_dump`.

**Required Behavior (Detailed)**
- `setUpClass`:
  - Sets `FLAGS.data_type = 'examplebatch'`.
  - Loads dump via `DumpUtils().load(...)`.
- `test_load_input_fn`:
  - Calls `import_input_fn` with `file_name`.
  - Verifies each output feature returns `tf.RaggedTensor` if flagged ragged, else `tf.Tensor`.
- `test_load_receiver`:
  - Calls `import_receiver_fn`.
  - Verifies feature tensor types and that receiver_tensors length is 1.
- `test_load_mode`:
  - `get_graph_helper` returns `GraphDefHelper` for TRAIN, TRAIN with `graph.dry_run=True`, and PREDICT.
- `__main__`: disables eager execution and runs tests.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: none.
- Feature gating: TF runtime only.
- Integration points: model dump loader.

**Implementation Steps (Detailed)**
1. If graph import is implemented in Rust, add tests for ragged/dense reconstruction.
2. Use fixed dump artifacts to avoid nondeterminism.

**Tests (Detailed)**
- Python tests: `monolith/native_training/model_dump/graph_utils_test.py`.
- Rust tests: none.
- Cross-language parity test: compare reconstructed tensors and types.

**Gaps / Notes**
- Test depends on external dump artifacts in the repo.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/__init__.py`
<a id="monolith-native-training-model-export-init-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 22
- Purpose/role: Re-exports model export modules under legacy module paths for backward compatibility.
- Key symbols/classes/functions: module aliasing via `sys.modules`.
- External dependencies: `export_context`, `saved_model_exporters`.
- Side effects: Inserts entries into `sys.modules` and deletes local `_sys`.

**Required Behavior (Detailed)**
- Imports `monolith.native_training.model_export.export_context` and `saved_model_exporters`.
- Registers aliases:
  - `'monolith.model_export.export_context'` → `export_context` module.
  - `'monolith.model_export.saved_model_exporters'` → `saved_model_exporters` module.
- Deletes `_sys` name after aliasing.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: none.
- Feature gating: none.
- Integration points: Python import compatibility only.

**Implementation Steps (Detailed)**
1. If Rust wrappers need to mirror Python module paths, document the aliasing behavior in docs.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: none.
- Cross-language parity test: not applicable.

**Gaps / Notes**
- Pure import aliasing; no functional logic.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/data_gen_utils.py`
<a id="monolith-native-training-model-export-data-gen-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 732
- Purpose/role: Generates synthetic Example/Instance/ExampleBatch data and PredictionLogs for model export, warmup, and testing.
- Key symbols/classes/functions: `FeatureMeta`, `ParserArgs`, `gen_fids_v1`, `gen_fids_v2`, `fill_features`, `fill_line_id`, `gen_example`, `gen_instance`, `gen_example_batch`, `gen_prediction_log`, `gen_warmup_file`, `gen_random_data_file`.
- External dependencies: TensorFlow, TF Serving protos, Monolith feature list, proto types (`Example`, `Instance`, `LineId`).
- Side effects: Writes TFRecord warmup files and binary data files.

**Required Behavior (Detailed)**
- Constants: `MASK_V1/MAX_SLOT_V1`, `MASK_V2/MAX_SLOT_V2` control fid encoding.
- `FeatureMeta`:
  - Infers dtype from LineId field descriptors or slot (defaults to float32 for dense, int64 for sparse).
  - `shape` defaults to 1 for dense, -1 for sparse.
- `ParserArgs` dataclass:
  - Reads defaults from TF collections via `get_collection`.
  - Ensures `DEFAULT_SERVING_SIGNATURE_DEF_KEY` is present in `signature_name`.
  - Attempts `FeatureList.parse()` if no feature_list provided.
- `gen_fids_v1(slot, size)` / `gen_fids_v2(slot, size)`:
  - Encodes slot in high bits and random low bits; v1 logs when slot > max, v2 asserts slot range.
- `fill_features` (singledispatch):
  - For `EFeature` (Example):
    - Sparse: generates fid_v2_list with drop_rate logic and slot-specific handling.
    - Dense: fills float/double/int64 lists with random values.
  - For `IFeature` (Instance):
    - Similar logic; uses `feature.fid`, `float_value`, `int64_value`.
- `fill_line_id(line_id, features, hash_len=48, actions=None)`:
  - Fills LineId fields based on metadata or defaults; handles repeated vs scalar fields.
- `lg_header(source)` / `sort_header(sort_id, kafka_dump, kafka_dump_prefix)`:
  - Produce binary headers for data files, including Java hash computation.
- `gen_example(...)`:
  - Builds Example with named_feature entries; uses `FeatureList` or slot lookup; fills labels and LineId.
- `gen_instance(...)`:
  - Builds Instance proto using fidv1/fidv2 features; fills labels and LineId.
- `gen_example_batch(...)`:
  - Builds ExampleBatch with per-feature lists, LineId list (`__LINE_ID__`), and labels (`__LABEL__`).
- `gen_prediction_log(args)`:
  - Generates PredictRequest logs using the appropriate variant type.
  - Supports multiple signatures; may emit multiple requests for multi-head outputs.
- `gen_warmup_file(warmup_file, drop_rate)`:
  - Builds ParserArgs, removes dense label if present, writes PredictionLog TFRecord to file.
  - Creates directories if needed; returns file path or None.
- `gen_random_data_file(...)`:
  - Writes binary file with headers and serialized instances for `num_batch`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (Python proto generators).
- Rust public API surface: if needed, add a data-gen utility module for tests/warmup.
- Data model mapping: map proto types (Example/Instance/ExampleBatch) to Rust protobufs.
- Feature gating: TF Serving protos required for PredictionLog generation.
- Integration points: model export/warmup pipeline.

**Implementation Steps (Detailed)**
1. Implement fid encoding and feature filling logic in Rust.
2. Mirror ParserArgs collection-based defaults if Rust uses similar collections.
3. Implement PredictionLog generation and TFRecord writing for warmup data.
4. Port binary data file format (headers + length + payload).

**Tests (Detailed)**
- Python tests: none (`data_gen_utils_test.py` is empty).
- Rust tests: add unit tests for fid encoding and example generation.
- Cross-language parity test: compare serialized outputs for fixed RNG seed.

**Gaps / Notes**
- Uses `eval` on stored representations and random data generation; not deterministic without seeding.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/data_gen_utils_test.py`
<a id="monolith-native-training-model-export-data-gen-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 0
- Purpose/role: Empty test placeholder.
- Key symbols/classes/functions: none.
- External dependencies: none.
- Side effects: none.

**Required Behavior (Detailed)**
- File is empty; no tests executed.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: none.
- Feature gating: none.
- Integration points: none.

**Implementation Steps (Detailed)**
1. Add tests if/when data generation is ported to Rust.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: none.
- Cross-language parity test: N/A.

**Gaps / Notes**
- No coverage for data generation utilities.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/demo_export.py`
<a id="monolith-native-training-model-export-demo-export-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 100
- Purpose/role: CLI demo that exports a saved model from the TestFFMModel using standalone or distributed exporter.
- Key symbols/classes/functions: `export_saved_model`, `main`.
- External dependencies: TensorFlow, Monolith CPU training, `parse_instances`, `StandaloneExporter`, `DistributedExporter`.
- Side effects: Writes SavedModel to disk under `export_base`; uses flags; disables eager execution.

**Required Behavior (Detailed)**
- Defines flags:
  - `num_ps` (default 5) for CPU training config.
  - `model_dir` and `export_base` default to `/tmp/<user>/monolith/native_training/demo/...`.
  - `export_mode` enum (Standalone or Distributed).
- `export_saved_model(model_dir, export_base, num_ps, export_mode)`:
  - Disables eager execution; sets TF logging verbosity to INFO.
  - Instantiates `TestFFMModel` params with name `"demo_export"` and batch size 64.
  - Creates `cpu_training.CpuTraining` with `CpuTrainingConfig(num_ps=num_ps)`.
  - Chooses exporter:
    - `StandaloneExporter` or `DistributedExporter` (with `shared_embedding=False`).
  - Defines `serving_input_receiver_fn`:
    - `instances` placeholder of dtype `tf.string` with shape `(None,)`.
    - Parses instances via `parse_instances`, with fidv1 features 0.._NUM_SLOTS-1.
    - Builds `features` dict with keys `feature_i` from `slot_i`.
    - Returns `tf.estimator.export.ServingInputReceiver`.
  - Calls `exporter.export_saved_model(serving_input_receiver_fn)`.
- `main(_)` calls `export_saved_model` with flags.
- `__main__` uses `absl.app.run`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (Python TF export demo).
- Rust public API surface: none.
- Data model mapping: if exporting in Rust, define equivalent serving input receiver.
- Feature gating: TF runtime only.
- Integration points: export pipeline.

**Implementation Steps (Detailed)**
1. If Rust export is desired, implement a demo exporter that mirrors TestFFMModel inputs.
2. Map parsing logic for FID v1 features to Rust serving inputs.
3. Preserve default paths and batch size for parity tests.

**Tests (Detailed)**
- Python tests: `monolith/native_training/model_export/demo_export_test.py`.
- Rust tests: none.
- Cross-language parity test: compare exported SavedModel signatures and input names.

**Gaps / Notes**
- Demo only; depends on TestFFMModel and CPU training stack.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/demo_export_test.py`
<a id="monolith-native-training-model-export-demo-export-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 48
- Purpose/role: Integration test that trains TestFFMModel and verifies standalone/distributed export paths run without error.
- Key symbols/classes/functions: `DemoExportTest.test_demo_export`.
- External dependencies: TensorFlow, Monolith CPU training, `demo_export.export_saved_model`.
- Side effects: Creates training checkpoints and two SavedModel export directories under `TEST_TMPDIR`.

**Required Behavior (Detailed)**
- Disables eager execution at import time.
- `test_demo_export`:
  - Creates `model_dir = $TEST_TMPDIR/test_ffm_model`.
  - Trains TestFFMModel with `cpu_training.local_train(params, num_ps=5, model_dir=...)`.
  - Calls `demo_export.export_saved_model` twice:
    - Standalone export to `$TEST_TMPDIR/standalone_saved_model`.
    - Distributed export to `$TEST_TMPDIR/distributed_saved_model`.
  - Uses `ExportMode.STANDALONE` and `ExportMode.DISTRIBUTED`.
- No explicit assertions on contents; success is absence of errors.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: none.
- Feature gating: TF runtime only.
- Integration points: export pipeline.

**Implementation Steps (Detailed)**
1. If Rust adds export support, add a smoke test for export outputs.
2. Ensure deterministic temp paths and cleanup.

**Tests (Detailed)**
- Python tests: `monolith/native_training/model_export/demo_export_test.py`.
- Rust tests: none.
- Cross-language parity test: compare exported SavedModel signatures.

**Gaps / Notes**
- Test is heavy (trains and exports); may be slow.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/demo_predictor.py`
<a id="monolith-native-training-model-export-demo-predictor-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 110
- Purpose/role: CLI demo to load a SavedModel and run prediction with randomly generated inputs.
- Key symbols/classes/functions: `make_fid_v1`, `generate_demo_instance`, `random_generate_instances`, `random_generate_int`, `random_generate_float`, `predict`, `main`.
- External dependencies: TensorFlow SavedModel, NumPy, `proto_parser_pb2.Instance`, TestFFMModel constants.
- Side effects: Loads SavedModel from disk; logs prediction outputs.

**Required Behavior (Detailed)**
- Flags:
  - `saved_model_path` (required path), `tag_set` (default "serve"), `signature` (default "serving_default"), `batch_size` (default 128).
- `make_fid_v1(slot_id, fid)`:
  - Encodes FID v1 as `(slot_id << 54) | fid`.
- `generate_demo_instance()`:
  - Creates `Instance` proto.
  - For each slot in `model._NUM_SLOTS`, generates 5 random fids in that slot based on `max_vocab`.
  - Returns serialized bytes.
- `random_generate_instances(bs)`:
  - Returns list of `bs` serialized Instance bytes.
- `random_generate_examples(bs)` (unused):
  - Returns list of serialized Example bytes using `model.generate_ffm_example`.
- `random_generate_int(shape)`:
  - Returns int64 array in `[0, max_vocab)` where `max_vocab = max(_VOCAB_SIZES) * _NUM_SLOTS`.
- `random_generate_float(shape)`:
  - Returns float array of `uniform(0,1)` values.
- `predict()`:
  - Loads SavedModel with `tf.compat.v1.saved_model.load`.
  - Reads signature inputs/outputs for `FLAGS.signature`.
  - For each input, builds a feed tensor based on dtype:
    - string -> list of serialized instances, shape length must be 1.
    - int64 -> random ints.
    - float32 -> random floats.
    - else raises `ValueError`.
  - Runs session and logs outputs.
- `main` calls `predict`; `__main__` sets logging verbosity to INFO and runs via absl.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: if implementing predictor in Rust, map SavedModel signature I/O to random data generation.
- Feature gating: TF runtime only.
- Integration points: serving validation / smoke tests.

**Implementation Steps (Detailed)**
1. If Rust can load SavedModels, implement a CLI to sample inputs and run predictions.
2. Mirror dtype-based generation (string -> serialized Instance, int64/float32 random arrays).
3. Match FID v1 encoding for feature IDs.

**Tests (Detailed)**
- Python tests: `demo_predictor_client.py` (manual) or none.
- Rust tests: none.
- Cross-language parity test: compare output shapes for identical inputs.

**Gaps / Notes**
- `random_generate_examples` is unused.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/demo_predictor_client.py`
<a id="monolith-native-training-model-export-demo-predictor-client-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 93
- Purpose/role: gRPC client for TensorFlow Serving PredictionService using random inputs derived from SavedModel signature.
- Key symbols/classes/functions: `get_signature_def`, `main`.
- External dependencies: gRPC, TensorFlow Serving protos, TensorFlow.
- Side effects: Sends Predict RPC to remote server.

**Required Behavior (Detailed)**
- Flags:
  - `server` (default "localhost:8500"), `model_name` ("default"), `signature_name` ("serving_default"), `use_example` (bool).
  - Note: code references `FLAGS.batch_size` but flag is not defined in this file (bug).
- `get_signature_def(stub)`:
  - Requests signature_def metadata via `GetModelMetadata`.
  - Unpacks `SignatureDefMap` and returns signature by `FLAGS.signature_name`.
  - Prints available signature names.
- `main`:
  - Creates insecure gRPC channel and PredictionService stub.
  - Builds PredictRequest with model spec.
  - For each input in signature:
    - Computes shape, substituting `FLAGS.batch_size` for -1 dims.
    - Generates example/instance bytes for string inputs.
    - Generates random ints/floats for int64/float32 inputs.
    - Raises `ValueError` for unsupported dtype.
  - Calls `stub.Predict(request, timeout=30)` and logs result.
- Logging verbosity set to INFO in `__main__`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: if implementing, use TF Serving gRPC protos in Rust.
- Feature gating: gRPC + TF Serving protos.
- Integration points: serving smoke tests.

**Implementation Steps (Detailed)**
1. Define missing `batch_size` flag or pass as CLI arg.
2. If Rust needs a client, implement signature discovery and random input generation.
3. Mirror example/instance encoding logic using demo_predictor helpers.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: none.
- Cross-language parity test: compare request shapes and dtype handling.

**Gaps / Notes**
- `FLAGS.batch_size` is referenced but never defined (likely a bug).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/export_context.py`
<a id="monolith-native-training-model-export-export-context-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 141
- Purpose/role: Manages export mode state and signatures for model export; provides context manager for export mode.
- Key symbols/classes/functions: `ExportMode`, `ExportContext`, `enter_export_mode`, `is_exporting*`, `get_current_export_ctx`, `is_dry_run_or_exporting`.
- External dependencies: TensorFlow, `tf_contextlib`, `monolith_export` decorator.
- Side effects: Global export mode state; stores signatures in TF collections.

**Required Behavior (Detailed)**
- `ExportMode` enum: `NONE`, `STANDALONE`, `DISTRIBUTED`.
- `SavedModelSignature` namedtuple (`name`, `inputs`, `outputs`).
- `ExportContext`:
  - Maintains `sub_graphs` and `dense_sub_graphs` as `defaultdict(tf.Graph)`.
  - Maintains `_signatures` keyed by graph id; each entry maps name -> SavedModelSignature.
  - `add_signature` adds to TF collection `signature_name` and stores signature.
  - `merge_signature` updates existing signature inputs/outputs or creates empty.
  - `signatures(graph)` returns signature values for given graph id.
  - `with_remote_gpu` property returns constructor flag.
  - `sub_graph_num` returns count of sub_graphs.
- Globals:
  - `EXPORT_MODE` starts as `NONE`.
  - `EXPORT_CTX` starts as `None`.
- `is_exporting` / `is_exporting_standalone` / `is_exporting_distributed`:
  - Compares `EXPORT_MODE` to enum values.
- `get_current_export_ctx`:
  - Returns `EXPORT_CTX`.
- `enter_export_mode(mode, export_ctx=None)`:
  - Asserts no nested export (`EXPORT_MODE is NONE` and `EXPORT_CTX is None`).
  - Creates new `ExportContext()` if not provided.
  - Sets globals, yields `export_ctx`, then resets globals to defaults in `finally`.
- `is_dry_run_or_exporting()`:
  - Returns True if export mode active or default graph has `dry_run` attribute.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: optional export context struct with thread-local state.
- Data model mapping: signatures map, subgraph registry.
- Feature gating: export-only.
- Integration points: model export pipeline.

**Implementation Steps (Detailed)**
1. Implement export context state in Rust (thread-local/global).
2. Provide RAII guard for entering/exiting export mode.
3. Mirror signature tracking and graph association logic if needed.

**Tests (Detailed)**
- Python tests: `export_context` is exercised by export hooks and demo exporters.
- Rust tests: add unit tests for mode nesting and signature registry.
- Cross-language parity test: ensure signature names collected match.

**Gaps / Notes**
- Uses global mutable state; not thread-safe for parallel exports.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/export_hooks.py`
<a id="monolith-native-training-model-export-export-hooks-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 137
- Purpose/role: Checkpoint saver listener that exports SavedModel on each checkpoint and prunes old exports.
- Key symbols/classes/functions: `get_global_step`, `ExportSaverListener`.
- External dependencies: TensorFlow, Monolith save_utils/export_state_utils, custom metrics client.
- Side effects: Writes export directories, deletes old ones, emits metrics.

**Required Behavior (Detailed)**
- `get_global_step(checkpoint_path)`:
  - Regex `model.ckpt-(\d+)`; asserts match; returns int.
- `ExportSaverListener`:
  - `__init__(save_path, serving_input_receiver_fn, exporter, exempt_checkpoint_paths=None, dense_only=False)`:
    - Stores serving_input_receiver_fn and exporter.
    - `self._helper = save_utils.SaveHelper(save_path)`.
    - Builds `self._exempt_checkpoint_steps` from `exempt_checkpoint_paths` by parsing global steps.
    - `dense_only` toggles special deletion logic.
    - Creates metric client via `cli.get_cli(utils.get_metric_prefix())`.
  - `after_save(session, global_step_value)`:
    - Uses SaveHelper to get checkpoint prefix for step.
    - Calls `exporter.export_saved_model(...)`.
    - Accepts export_dirs as bytes, list, or dict of values.
    - For each export_dir:
      - Adds entry to export state and prunes old entries.
  - `_add_entry_to_state(export_dir, global_step_value)`:
    - Decodes bytes; computes base/version.
    - Appends `ServingEntry(export_dir, global_step)` to state and overwrites state file.
    - Calls `_update_metrics`.
  - `_maybe_delete_old_entries(export_dir)`:
    - Loads existing state; computes `existing_steps` from current checkpoints plus exempt steps.
    - If `dense_only`, also loads full checkpoint state from model_dir and includes all steps.
    - Removes entries not in `existing_steps`, deleting directories via `tf.io.gfile.rmtree`.
  - `_update_metrics(export_dir_base, version)`:
    - Emits `export_models.latest_version` as int if version is numeric.
    - `version` uses `split(".")[0]` to handle float-like names.
    - Logs warning on exceptions every 1200 seconds.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: if implementing export hooks, add a checkpoint listener trait.
- Data model mapping: export state protobufs -> Rust structs.
- Feature gating: export-only.
- Integration points: checkpoint saving, export pipeline, metrics client.

**Implementation Steps (Detailed)**
1. Implement checkpoint listener in Rust training loop if needed.
2. Mirror export directory state tracking and pruning rules.
3. Add metrics emission for latest export version.

**Tests (Detailed)**
- Python tests: `export_hooks_test.py`.
- Rust tests: add filesystem tests for pruning behavior.
- Cross-language parity test: compare export state entries after simulated checkpoints.

**Gaps / Notes**
- `get_global_step` asserts regex match; invalid checkpoint paths will raise AssertionError.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/export_hooks_test.py`
<a id="monolith-native-training-model-export-export-hooks-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 141
- Purpose/role: Tests ExportSaverListener behavior (state updates, dict export outputs, deletion of old exports).
- Key symbols/classes/functions: `ExportHookTest.testBasic`, `testExporterReturnsDict`, `testDeleted`.
- External dependencies: TensorFlow, `save_utils`, `export_state_utils`, `unittest.mock`.
- Side effects: Creates model/export dirs under `TEST_TMPDIR`.

**Required Behavior (Detailed)**
- `testBasic`:
  - Mocks exporter to return `export_dir` bytes and asserts checkpoint path format.
  - Runs `NoFirstSaveCheckpointSaverHook` and sets global_step to 10.
  - Verifies export state has one entry with correct dir and step.
- `testExporterReturnsDict`:
  - Mocks exporter to return dict of model names to export dirs.
  - Ensures no errors during export and state update.
- `testDeleted`:
  - Mocks exporter to create unique export dirs per step.
  - Uses `PartialRecoverySaver` with `max_to_keep=1`.
  - After two steps, verifies only latest export remains and old one deleted.
- Tests rely on `export_state_utils.get_export_saver_listener_state` and filesystem cleanup.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: export state proto to Rust struct if implemented.
- Feature gating: export-only.
- Integration points: export listener and checkpoint saver.

**Implementation Steps (Detailed)**
1. If Rust implements export hooks, add tests mirroring state entries and deletion.
2. Mock exporter to return bytes or dict.
3. Validate pruning when `max_to_keep` is 1.

**Tests (Detailed)**
- Python tests: `monolith/native_training/model_export/export_hooks_test.py`.
- Rust tests: none.
- Cross-language parity test: compare state entries after simulated checkpoints.

**Gaps / Notes**
- Uses real filesystem; may need cleanup to avoid test leakage.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/export_state_utils.py`
<a id="monolith-native-training-model-export-export-state-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 46
- Purpose/role: Reads/writes export state file (`ExportSaverListenerState`) containing `ServingModelState` proto.
- Key symbols/classes/functions: `get_export_saver_listener_state`, `overwrite_export_saver_listener_state`.
- External dependencies: TensorFlow gfile, protobuf text_format, `export_pb2`.
- Side effects: Reads and writes state files in export directory.

**Required Behavior (Detailed)**
- `_ExportSaverListenerStateFile = "ExportSaverListenerState"`.
- `get_export_saver_listener_state(export_dir_base)`:
  - Reads `<export_dir_base>/ExportSaverListenerState` as text proto.
  - If file missing, returns empty `ServingModelState`.
  - Parses using `text_format.Merge`.
- `overwrite_export_saver_listener_state(export_dir_base, state)`:
  - Ensures `export_dir_base` exists (`gfile.makedirs`).
  - Writes text proto to temp file `<filename>-tmp`.
  - Atomically renames temp to final file (overwrite=True).

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: if implemented, add export state read/write helpers.
- Data model mapping: `ServingModelState` proto in Rust.
- Feature gating: export-only.
- Integration points: export hooks.

**Implementation Steps (Detailed)**
1. Implement read/write of text-format proto in Rust (or switch to binary with parity notes).
2. Preserve temp-file rename semantics for atomic updates.

**Tests (Detailed)**
- Python tests: `export_state_utils_test.py`.
- Rust tests: add read/write round-trip tests.
- Cross-language parity test: compare serialized text output.

**Gaps / Notes**
- Uses text-format proto; any Rust implementation must match formatting if parity is required.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/export_state_utils_test.py`
<a id="monolith-native-training-model-export-export-state-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 36
- Purpose/role: Round-trip test for export state read/write.
- Key symbols/classes/functions: `ExportStateUtilsTest.test_basic`.
- External dependencies: `export_state_utils`, `export_pb2`, filesystem.
- Side effects: Writes state file under `TEST_TMPDIR`.

**Required Behavior (Detailed)**
- `test_basic`:
  - Creates `ServingModelState` with one entry (`export_dir="a"`, `global_step=1`).
  - Writes state to temp dir via `overwrite_export_saver_listener_state`.
  - Reads state back and asserts equality with original.
- Uses `unittest.TestCase`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: ServingModelState proto.
- Feature gating: export-only.
- Integration points: export state utilities.

**Implementation Steps (Detailed)**
1. Implement Rust read/write helpers and add a round-trip test.
2. Ensure protobuf equality holds after text-format serialization.

**Tests (Detailed)**
- Python tests: `export_state_utils_test.py`.
- Rust tests: add round-trip test if implemented.
- Cross-language parity test: compare text serialization output.

**Gaps / Notes**
- Uses deprecated `assertEquals` (should be `assertEqual`).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/export_utils.py`
<a id="monolith-native-training-model-export-export-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 98
- Purpose/role: Helper for defining and invoking remote prediction signatures via `distributed_serving_ops.remote_predict`.
- Key symbols/classes/functions: `RemotePredictHelper`, `_get_tensor_signature_name`.
- External dependencies: TensorFlow, `nested_tensors`, `export_context`, `distributed_serving_ops`.
- Side effects: Registers SavedModel signatures with `ExportContext`.

**Required Behavior (Detailed)**
- `_get_tensor_signature_name(t)`:
  - Returns tensor name with ":" replaced by "_" (e.g., "foo:0" -> "foo_0").
- `RemotePredictHelper.__init__(name, input_tensors, remote_func)`:
  - Wraps inputs in `NestedTensors`, stores remote func, calls `_define_remote_func`.
- `_define_remote_func()`:
  - Creates placeholders matching flat input tensors (dtype/shape) with suffix `_remote_input_ph`.
  - Builds nested input structure from placeholders and calls `remote_func`.
  - Wraps outputs in `NestedTensors`.
  - Builds signature input/output dicts keyed by `_get_tensor_signature_name`.
  - Asserts no name conflicts (lengths match).
  - Registers signature in current `ExportContext` via `add_signature`.
- `call_remote_predict(model_name, input_tensors=None, old_model_name=None, task=0)`:
  - Uses provided `input_tensors` or original input tensors.
  - Calls `distributed_serving_ops.remote_predict` with signature name and I/O names.
  - Passes `output_types` from output tensor dtypes and `signature_name=self._name`.
  - Returns outputs in original nested structure.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: if remote predict exists, implement a helper mirroring signature registration.
- Data model mapping: nested tensor structure + signature names.
- Feature gating: remote serving only.
- Integration points: distributed serving ops.

**Implementation Steps (Detailed)**
1. Implement nested tensor flattening and placeholder generation if Rust supports graph export.
2. Ensure signature name mapping replaces ":" with "_".
3. Provide remote predict wrapper matching arg ordering and output types.

**Tests (Detailed)**
- Python tests: `export_utils_test.py`.
- Rust tests: add unit test for signature name mapping and nested output reconstruction.
- Cross-language parity test: compare signature I/O names and remote_predict call args.

**Gaps / Notes**
- Relies on global export context; ensure one is active when constructing helper.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/export_utils_test.py`
<a id="monolith-native-training-model-export-export-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 43
- Purpose/role: Basic test for `RemotePredictHelper` signature definition and call path.
- Key symbols/classes/functions: `ExportUtilsTest.testBasic`.
- External dependencies: TensorFlow, `export_context`, `export_utils`.
- Side effects: Enters export mode (standalone).

**Required Behavior (Detailed)**
- `testBasic`:
  - Enters `export_context.enter_export_mode(EXPORT_MODE.STANDALONE)`.
  - Defines `remote_func(d)` returning `d["a"] * 3 + d["b"] * 4`.
  - Instantiates `RemotePredictHelper("test_func", {"a": tf.constant(1), "b": tf.constant(2)}, remote_func)`.
  - Calls `helper.call_remote_predict("model_name")`.
  - Asserts result is a `tf.Tensor`.
- Note: test intentionally only checks grammar due to missing TF Serving compilation.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: none.
- Feature gating: export-only.
- Integration points: remote predict.

**Implementation Steps (Detailed)**
1. If Rust implements RemotePredictHelper, add a similar smoke test.
2. Ensure export mode context is active for signature registration.

**Tests (Detailed)**
- Python tests: `export_utils_test.py`.
- Rust tests: none.
- Cross-language parity test: verify signature registration.

**Gaps / Notes**
- No actual remote serving is exercised.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/saved_model_exporters.py`
<a id="monolith-native-training-model-export-saved-model-exporters-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 739
- Purpose/role: Implements SavedModel exporters for standalone and distributed export, including hashtable restore/assign signatures and warmup assets.
- Key symbols/classes/functions: `BaseExporter`, `StandaloneExporter`, `DistributedExporter`.
- External dependencies: TensorFlow SavedModel internals, Monolith hash table ops, export_context, DumpUtils.
- Side effects: Writes SavedModel directories, copies assets, modifies TF collections, restores variables/hashtables.

**Required Behavior (Detailed)**
- `BaseExporter`:
  - Stores model_fn, model_dir, export_dir_base, shared_embedding, warmup_file, and optional export_context_list.
  - `create_asset_base()`:
    - Adds a `ASSET_BASE` tensor and AssetFileDef to assets collection if not already.
    - Returns tensor with value `"./"`.
  - `add_ckpt_to_assets(ckpt_to_export, pattern="*")`:
    - Adds all matching ckpt asset files to `ASSET_FILEPATHS` collection.
  - `build_signature(input_tensor_dict, output_tensor_dict)`:
    - Wraps tensors or TensorInfo into `SignatureDef` for PREDICT.
  - `_freeze_dense_graph(graph_def, signature_def_map, session)`:
    - Collects all input/output nodes from signatures and uses `convert_variables_to_constants`.
    - Restores device placement in frozen graph.
  - `_export_saved_model_from_graph(...)`:
    - Requires export_dir or export_dir_base.
    - Builds signatures from export_ctx.
    - Optionally adds hashtable assign signatures.
    - Creates Session with soft placement + GPU updates.
    - Restores variables and hashtables (restore ops and assign ops).
    - Writes SavedModel via `Builder` to temp dir then renames.
    - Copies `assets_extra` to `assets.extra` if provided.
  - `_export_frozen_saved_model_from_graph(...)`:
    - Similar but freezes graph and re-imports into a new graph before export.
  - `create_hashtable_restore_ops` / `create_multi_hashtable_restore_ops`:
    - For each (multi) hash table in graph collections, builds restore ops.
    - If not shared_embedding, adds ckpt files to assets and uses asset base; else uses ckpt asset dir.
  - `build_hashtable_assign_inputs_outputs`:
    - Creates placeholder-based assign tensors for hashtable update signature.
  - `add_multi_hashtable_assign_signatures`:
    - Adds raw_assign signatures for multi-hash tables (ragged id + flat values).
  - `_model_fn_with_input_reveiver`:
    - Runs model_fn in PREDICT mode and registers signatures in export_context.
  - `export_saved_model(...)`:
    - Abstract.
  - `gen_warmup_assets()`:
    - Generates warmup TFRecord via `gen_warmup_file` if not present and returns assets dict.
- `StandaloneExporter.export_saved_model(...)`:
  - Enters export mode STANDALONE; clears `TF_CONFIG` temporarily.
  - Builds graph, runs model_fn, exports SavedModel with warmup assets.
  - Restores `TF_CONFIG` on exit.
- `DistributedExporter.export_saved_model(...)`:
  - Creates ExportContext with `with_remote_gpu` flag.
  - Enters DISTRIBUTED export mode; clears `TF_CONFIG` temporarily.
  - Exports entry graph (optionally with GPU device placement).
  - Exports dense subgraphs and ps subgraphs stored in export_ctx.
  - Supports `dense_only`, `include_graphs`, `global_step_as_timestamp`, `freeze_variable`.
  - Skips exporting if target dir already exists.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: if implementing SavedModel export, mirror BaseExporter and specialized exporters.
- Data model mapping: SavedModel signatures, assets, and hashtable metadata.
- Feature gating: TF runtime + monolith hash table ops required.
- Integration points: export pipeline, checkpoint loader, hash table restore.

**Implementation Steps (Detailed)**
1. Implement export context signature collection in Rust if needed.
2. Add SavedModel builder wrappers and asset copying.
3. Implement hash table restore/assign signatures in Rust or document lack.
4. Mirror distributed export layout (entry + dense + ps submodels).

**Tests (Detailed)**
- Python tests: `saved_model_exporters_test.py`.
- Rust tests: add export smoke tests if implemented.
- Cross-language parity test: compare exported signature defs and asset layout.

**Gaps / Notes**
- `_model_fn_with_input_reveiver` typo in name (receiver misspelled) but used internally.
- Uses TF internal APIs; may be brittle across TF versions.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/saved_model_exporters_test.py`
<a id="monolith-native-training-model-export-saved-model-exporters-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 153
- Purpose/role: Tests StandaloneExporter with hash tables and multi-hash tables, including shared embedding mode.
- Key symbols/classes/functions: `ModelFnCreator`, `SavedModelExportersTest`.
- External dependencies: TensorFlow Estimator, Monolith hash table ops, SavedModel exporter.
- Side effects: Creates checkpoints and exports SavedModels under `TEST_TMPDIR`.

**Required Behavior (Detailed)**
- `ModelFnCreator.create_model_fn()`:
  - Sets `_called_in_exported_mode` if `export_context.EXPORT_MODE != None`.
  - Builds hash table and multi-hash table.
  - In PREDICT mode:
    - Exports outputs for default signature, "table/lookup", and "mtable/lookup".
  - In TRAIN mode:
    - Adds assign_add ops for tables.
    - Adds `CheckpointSaverHook` with hash table saver listeners.
    - Returns `EstimatorSpec` with train_op and loss=0.
- `dummy_input_receiver_fn` returns empty features with a string placeholder.
- `SavedModelExportersTest`:
  - `run_pred(export_path, key=DEFAULT)` loads SavedModel and runs output tensor.
  - `testBasic`:
    - Trains one step to create checkpoint.
    - Exports SavedModel and asserts predictions for table and mtable lookups.
    - Asserts model_fn was called in export mode.
  - `testSharedEmebdding`:
    - Exports with `shared_embedding=True` and asserts predictions.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: hash table ops and SavedModel exports.
- Feature gating: TF runtime + hash table ops.
- Integration points: export pipeline and hash table checkpointing.

**Implementation Steps (Detailed)**
1. If Rust supports hash-table-backed exports, add equivalent tests.
2. Verify lookup outputs after export match expected values.
3. Cover shared embedding behavior if implemented.

**Tests (Detailed)**
- Python tests: `saved_model_exporters_test.py`.
- Rust tests: none.
- Cross-language parity test: compare exported predictions for fixed inputs.

**Gaps / Notes**
- Misspelling `testSharedEmebdding` in test name.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/saved_model_visulizer.py`
<a id="monolith-native-training-model-export-saved-model-visulizer-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 89
- Purpose/role: CLI utility to import a SavedModel protobuf and write it to TensorBoard for visualization.
- Key symbols/classes/functions: `import_to_tensorboard`, `main`.
- External dependencies: TensorFlow SavedModel proto, TensorBoard summary writer.
- Side effects: Reads a SavedModel file, writes TensorBoard logdir.

**Required Behavior (Detailed)**
- `import_to_tensorboard(model_dir, log_dir)`:
  - Opens SavedModel file at `model_dir` as bytes.
  - Parses `SavedModel` proto; if more than one meta_graph, prints message and exits with code 1.
  - Imports the first graph_def into a new graph.
  - Writes graph to `log_dir` using `summary.FileWriter`.
  - Prints TensorBoard command.
- `main` invokes `import_to_tensorboard` with CLI flags.
- CLI parsing via `argparse`, requires `--model_dir` and `--log_dir`.
- Uses `app.run` from TF platform with parsed args.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: SavedModel proto parsing.
- Feature gating: TF runtime only.
- Integration points: tooling/debugging.

**Implementation Steps (Detailed)**
1. If Rust needs similar tool, parse SavedModel protobuf and emit graph for visualization.
2. Provide CLI for model_dir/log_dir.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: none.
- Cross-language parity test: not applicable.

**Gaps / Notes**
- Uses TF internal APIs; depends on TensorFlow installation.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/warmup_data_decoder.py`
<a id="monolith-native-training-model-export-warmup-data-decoder-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 55
- Purpose/role: CLI tool to decode TF Serving warmup TFRecord files and print sanitized requests.
- Key symbols/classes/functions: `main`.
- External dependencies: TensorFlow, TF Serving PredictionLog proto, `env_utils`.
- Side effects: Reads TFRecord file, logs decoded model specs.

**Required Behavior (Detailed)**
- Flag `file_name` specifies input TFRecord path.
- `main`:
  - Attempts `env_utils.setup_hdfs_env()`; ignores errors.
  - Enables eager execution and sets TF logging verbosity.
  - Defines `decode_fn` to parse `PredictionLog` from record bytes.
  - Iterates TFRecordDataset over `file_name`, decodes each log.
  - Extracts PredictRequest, replaces `string_val:.*` with `string_val: ...` in printed output.
  - Logs index and sanitized request string.
- Uses `app.run(main)`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: TF Serving PredictionLog proto.
- Feature gating: TF Serving protos required.
- Integration points: tooling for warmup verification.

**Implementation Steps (Detailed)**
1. If Rust needs a decoder, parse TFRecord PredictionLogs and print sanitized requests.
2. Mirror regex sanitization for `string_val` fields.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: none.
- Cross-language parity test: not applicable.

**Gaps / Notes**
- Eager execution is required; script is for inspection only.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/warmup_data_gen.py`
<a id="monolith-native-training-model-export-warmup-data-gen-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 253
- Purpose/role: CLI tool to generate TF Serving warmup PredictionLog data from existing pb data or random generation.
- Key symbols/classes/functions: `PBReader`, `gen_prediction_log_from_file`, `tf_dtype`, `main`.
- External dependencies: TensorFlow, TF Serving protos, `data_gen_utils.gen_prediction_log`, `env_utils`.
- Side effects: Reads input files, writes TFRecord warmup data to `output_path`.

**Required Behavior (Detailed)**
- CLI flags cover file input, batch sizes, feature lists/types, and generation mode (`gen_type` = file/random).
- `PBReader`:
  - Iterates over binary input stream (stdin or file), reading size-prefixed records.
  - Supports lagrangex header, sort_id, kafka_dump_prefix/kafka_dump.
  - For `example_batch`, reads one record per batch; otherwise reads `batch_size` records.
  - `set_max_iter(max_records)` sets max iterations based on variant type.
- `gen_prediction_log_from_file(...)`:
  - Chooses input name based on variant_type (`instances`, `examples`, `example_batch`).
  - Ensures `serving_default` signature included.
  - Yields `PredictionLog` entries with `PredictRequest` containing the batch tensor.
- `tf_dtype(dtype: str)`:
  - Maps string/int aliases to TF dtypes; **bug**: returns `tf.int46` for int64 cases (invalid dtype).
- `main`:
  - Calls `env_utils.setup_hdfs_env()`.
  - Writes PredictionLog records to `FLAGS.output_path` using TFRecordWriter.
  - If `gen_type == 'file'`, uses `gen_prediction_log_from_file`.
  - Else constructs feature specs from CLI flags and calls `data_gen_utils.gen_prediction_log(...)`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: TF Serving PredictionLog proto and tensor encoding.
- Feature gating: TF Serving protos required.
- Integration points: warmup data generation tooling.

**Implementation Steps (Detailed)**
1. Fix `tf_dtype` mapping if porting (use `tf.int64` for int64/long).
2. Clarify `gen_prediction_log` API usage; current call signature appears outdated.
3. If porting to Rust, implement size-prefixed reader and PredictionLog writer.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: add unit tests for PBReader and dtype mapping.
- Cross-language parity test: compare generated TFRecord entries for fixed inputs.

**Gaps / Notes**
- `tf_dtype` uses `tf.int46` (typo).
- `gen_prediction_log` call signature likely mismatched with current `data_gen_utils` (uses ParserArgs now).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/model_export/warmup_example_batch.py`
<a id="monolith-native-training-model-export-warmup-example-batch-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 57
- Purpose/role: Converts saved example-batch files into TF Serving PredictionLog warmup records.
- Key symbols/classes/functions: `gen_prediction_log`, `main`.
- External dependencies: TensorFlow, TF Serving protos, `env_utils`.
- Side effects: Reads input folder, writes TFRecord output.

**Required Behavior (Detailed)**
- Flags: `input_folder`, `output_path` (both required for use).
- `gen_prediction_log(input_folder)`:
  - Iterates files in input folder.
  - Reads file bytes and parses into `PredictRequest`.
  - Sets `model_spec.name="default"` and `signature_name="serving_default"`.
  - Wraps in `PredictionLog` and yields.
  - Prints parse result (debug).
- `main`:
  - Writes logs to TFRecord at `output_path`.
- `__main__` calls `env_utils.setup_hdfs_env()` then runs app.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: TF Serving PredictionLog.
- Feature gating: TF Serving protos.
- Integration points: warmup data generation.

**Implementation Steps (Detailed)**
1. If porting, read binary example-batch files and wrap into PredictionLog.
2. Preserve default model/signature names.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: none.
- Cross-language parity test: compare serialized output logs.

**Gaps / Notes**
- No validation of input file format; assumes each file is a serialized PredictRequest.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/monolith_export.py`
<a id="monolith-native-training-monolith-export-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 18
- Purpose/role: No-op decorator used to mark classes/functions for export.
- Key symbols/classes/functions: `monolith_export`.
- External dependencies: none.
- Side effects: Adds `__monolith_doc` attribute set to `None` on the object.

**Required Behavior (Detailed)**
- `monolith_export(obj)`:
  - Sets `obj.__monolith_doc = None`.
  - Returns the original object.
  - Used as decorator on classes/functions.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: none.
- Feature gating: none.
- Integration points: documentation/export tooling only.

**Implementation Steps (Detailed)**
1. If Rust needs similar tagging, add a marker trait or attribute macro (optional).

**Tests (Detailed)**
- Python tests: none.
- Rust tests: none.
- Cross-language parity test: not applicable.

**Gaps / Notes**
- Pure annotation; no runtime behavior beyond attribute set.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/multi_hash_table_ops.py`
<a id="monolith-native-training-multi-hash-table-ops-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 695
- Purpose/role: Implements multi-hash-table ops wrapper around custom TF ops, including lookup/assign/optimize and checkpoint save/restore hooks.
- Key symbols/classes/functions: `CachedConfig`, `MultiHashTable`, `MultiHashTableCheckpointSaverListener`, `MultiHashTableCheckpointRestorerListener`, `MultiHashTableRestorerSaverListener`.
- External dependencies: TensorFlow custom ops (`gen_monolith_ops`), hash table protobufs, save_utils, distributed_serving_ops.
- Side effects: Registers proto functions, adds tables to TF collections, writes ckpt info files.

**Required Behavior (Detailed)**
- Constants: `_TIMEOUT_IN_MS` (1 hour), `_MULTI_HASH_TABLE_GRAPH_KEY`.
- `CachedConfig`:
  - Stores configs, table_names, serialized mconfig, tensor, dims, slot_expire_time_config.
- `infer_dims`/`convert_to_cached_config`:
  - Builds `MultiEmbeddingHashTableConfig`, sets entry_type=SERVING when exporting.
  - Serializes config and returns `CachedConfig`.
- `MultiHashTable`:
  - Creates/reads multi hash table handle via custom ops, registers resource, adds to collection.
  - `from_cached_config` sets device based on table type (gpucuco -> GPU).
  - Lookup/assign/add/optimize operations delegate to custom ops.
  - `raw_lookup`, `raw_assign`, `raw_apply_gradients` use ragged ids and flat values.
  - Provides fused lookup/optimize for sync training.
  - `save`/`restore` use custom ops with basename.
  - `to_proto`/`from_proto` allow graph serialization.
- Helpers: ragged concatenation and flattening utilities for input/outputs.
- Checkpoint listeners:
  - `MultiHashTableCheckpointSaverListener` saves tables before saver, optionally writes `ckpt.info-<step>` with feature counts.
  - `MultiHashTableCheckpointRestorerListener` restores tables before restore, with optional PS monitor skip.
  - `MultiHashTableRestorerSaverListener` triggers restore after save.
- Registers proto functions on `_MULTI_HASH_TABLE_GRAPH_KEY` and marks `IsHashTableInitialized` as not differentiable.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (TF custom ops).
- Rust public API surface: would require binding the custom multi-hash-table ops.
- Data model mapping: protobuf configs, table handles, ragged IDs.
- Feature gating: TF runtime + custom ops.
- Integration points: embedding tables, checkpointing, distributed serving.

**Implementation Steps (Detailed)**
1. Bind custom ops for multi-hash-table if TF runtime backend is enabled.
2. Mirror ragged-id flattening and value slicing for embeddings.
3. Implement checkpoint listeners or hook equivalents for save/restore.
4. Match proto serialization for export/import.

**Tests (Detailed)**
- Python tests: `multi_hash_table_ops_test.py`.
- Rust tests: none.
- Cross-language parity test: compare lookup/assign outputs and checkpoint restores.

**Gaps / Notes**
- Requires custom ops and protobuf definitions; no Rust equivalent today.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/multi_hash_table_ops_test.py`
<a id="monolith-native-training-multi-hash-table-ops-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 249
- Purpose/role: Tests for MultiHashTable operations (lookup, assign_add, reinitialize, apply_gradients, save/restore, hooks).
- Key symbols/classes/functions: `MultiTypeHashTableTest.*`.
- External dependencies: TensorFlow, multi_hash_table_ops custom ops, save_utils.
- Side effects: Writes checkpoint and asset files under `TEST_TMPDIR`.

**Required Behavior (Detailed)**
- `test_lookup_assign_add_reinitialize`:
  - Builds table with slots; assign_add values; lookup matches expected.
  - Reinitializes slot2 and slot3; slot3 returns status -1.
- `test_apply_gradients`:
  - Applies gradients and checks embeddings updated (negative values).
- `test_save_restore`:
  - Saves table to basename, restores into new graph with different slots.
  - Restored values for overlapping slots match expected.
- `test_save_restore_hook`:
  - Uses saver and restorer hooks; ensures restore overwrites sub_op updates.
  - Verifies values match initial add_op.
- `test_meta_graph_export`:
  - Ensures multi hash table collection appears in exported meta_graph.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: custom ops and checkpoint assets.
- Feature gating: TF runtime + custom ops.
- Integration points: embedding table subsystem.

**Implementation Steps (Detailed)**
1. If Rust binds multi hash table ops, add tests for assign/lookup/save/restore.
2. Mirror hook behavior in Rust checkpointing.

**Tests (Detailed)**
- Python tests: `multi_hash_table_ops_test.py`.
- Rust tests: none.
- Cross-language parity test: compare lookup outputs and restore behavior.

**Gaps / Notes**
- Heavily depends on custom ops; cannot run without TF runtime.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/multi_type_hash_table.py`
<a id="monolith-native-training-multi-type-hash-table-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 435
- Purpose/role: Abstractions for multi-type hash tables and a merged-table wrapper that deduplicates configs across slots.
- Key symbols/classes/functions: `BaseMultiTypeHashTable`, `MultiTypeHashTable`, `MergedMultiTypeHashTable`.
- External dependencies: TensorFlow, hash_table_ops, distribution_ops, prefetch_queue.
- Side effects: Uses device placement; may register queue hooks for pipelined execution.

**Required Behavior (Detailed)**
- `BaseMultiTypeHashTable`:
  - Abstract API: `lookup`, `assign`, `assign_add`, `reinitialize`, `apply_gradients`, `as_op`, `get_table_dim_sizes`.
  - Supports queue hook aggregation via `add_queue_hook` and `get_queue_hooks`.
- `MultiTypeHashTable`:
  - Builds per-slot hash tables using a factory; maintains resources and learning_rate tensors.
  - `lookup` returns per-slot embeddings.
  - `assign` / `assign_add` / `apply_gradients` delegate to per-slot tables and return updated copy.
  - `as_op` returns no-op dependent on all table ops.
  - Supports fused lookup/optimize via custom ops using flattened learning rate tensor.
  - `reinitialize` not supported (raises NotImplementedError).
- `_IndexedValues` dataclass: records merged slots, index, and value tensor for merged operations.
- `MergedMultiTypeHashTable`:
  - Deduplicates slots with identical config (stringified config as key).
  - Builds merged slot names using MD5; tracks slot->merged_slot mapping.
  - If old naming mismatch, adds `extra_restore_names`.
  - `lookup`:
    - Merges slot ids by merged slot, calls underlying table lookup.
    - Splits embeddings back to original slots using sizes.
    - Supports optional early reorder results via `auxiliary_bundle`.
  - `assign` / `assign_add` / `apply_gradients`:
    - Merges ids and values before delegating.
    - `skip_merge_id` option in `_update` to bypass merge for certain paths.
  - `reinitialize` not supported.
  - `get_table_dim_sizes` returns inferred sizes for merged configs.

**Rust Mapping (Detailed)**
- Target crate/module: N/A (TF hash table ops).
- Rust public API surface: if embedding tables are implemented in Rust, add multi-type table abstraction and merged wrapper.
- Data model mapping: slot->embedding tensors, per-slot configs.
- Feature gating: embedding/hash table feature only.
- Integration points: embedding lookup and optimizer updates.

**Implementation Steps (Detailed)**
1. Implement BaseMultiTypeHashTable trait in Rust with lookup/assign APIs.
2. Add merged-table wrapper to reduce redundant configs.
3. Preserve slot ordering and size-based splitting for merged lookups.

**Tests (Detailed)**
- Python tests: `multi_type_hash_table_test.py`.
- Rust tests: add tests for merged slot mapping and lookup correctness.
- Cross-language parity test: compare embeddings for identical configs.

**Gaps / Notes**
- Merging uses `str(config)`; any changes in string representation alter merge behavior.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/multi_type_hash_table_test.py`
<a id="monolith-native-training-multi-type-hash-table-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 326
- Purpose/role: Tests MultiTypeHashTable and MergedMultiTypeHashTable behaviors, including fused ops and name stability.
- Key symbols/classes/functions: `MultiTypeHashTableTest.*`, `MergedMultiTypeHashTable.*`.
- External dependencies: TensorFlow, hash_table_ops custom ops.
- Side effects: None beyond TF session operations.

**Required Behavior (Detailed)**
- `test_basic`: assign_add + lookup values per slot.
- `test_apply_gradients`: applies gradients; expected negative embeddings.
- `test_apply_gradients_with_learning_rate_decay`: uses PolynomialDecay learning rate; checks scaled updates.
- `test_apply_gradients_without_lookup`: gradient updates without prior lookup.
- `test_fused_lookup` / `test_fused_lookup_multi_shards`:
  - Validate fused lookup outputs (embeddings, splits, offsets).
- `test_fused_apply_gradients` / `test_fused_apply_gradients_missing_tables`:
  - Validate fused optimize updates and resulting embeddings.
- `MergedMultiTypeHashTable.testBasic`:
  - Merges slots 1/2; verifies combined updates and gradients.
- `testNameStability`:
  - Ensures merged slot name (MD5) deterministic; factory called with single merged key.
- `testRestoreName`:
  - Verifies `extra_restore_names` for old naming convention `fc_slot_*`.

**Rust Mapping (Detailed)**
- Target crate/module: N/A.
- Rust public API surface: none.
- Data model mapping: custom ops for hash tables.
- Feature gating: TF runtime + custom ops.
- Integration points: embedding table implementation.

**Implementation Steps (Detailed)**
1. If Rust binds custom ops, port tests for assign/lookup and fused ops.
2. Validate merged slot mapping and restore name behavior.

**Tests (Detailed)**
- Python tests: `multi_type_hash_table_test.py`.
- Rust tests: none.
- Cross-language parity test: compare embeddings and offsets for fused ops.

**Gaps / Notes**
- Tests rely on custom ops and fixed learning rate semantics.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/native_model.py`
<a id="monolith-native-training-native-model-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 1109
- Purpose/role: Core TF-native model base classes and helpers for loss/prediction, embedding slice management, device placement, file output, metrics (AUC/MSE + deep insight), and export hooks.
- Key symbols/classes/functions:
  - `get_sigmoid_loss_and_pred`, `get_softmax_loss_and_pred`
  - `DeviceCtxType`, `MonolithDeviceCtx`
  - `MonolithBaseModel`, `MonolithModel`
  - Key methods: `create_model_fn`, `create_input_fn`, `create_serving_input_receiver_fn`,
    `create_embedding_feature_column`, `lookup_embedding_slice`, `share_slot`, `add_extra_output`, `add_training_hook`
- External dependencies: TensorFlow (graph/Estimator APIs, metrics, summaries, SavedModel), `absl.flags/logging`,
  monolith internal modules (`feature`, `feature_utils`, `file_ops`, `metric_utils`, `export_context`,
  `layers.LogitCorrection`, `dump_utils`, `device_utils`, `distribution_utils`, `embedding_combiners`, etc.),
  `OutConfig/OutType/TensorShape` proto from `idl.matrix.proto.example_pb2`.
- Side effects:
  - Writes prediction/eval outputs to per-worker files.
  - Writes item embedding cache table files in resource-constrained roughsort predict mode.
  - Mutates TF graph collections and graph-attached lists (`__losses`, `__training_hooks`, `__export_outputs`).
  - Registers slots and switches feature slots (`register_slots`, `switch_slot`, `switch_slot_batch`).
  - Adds TF summary scalars and `tf.print` ops for metrics.
  - Adds custom hooks for Kafka/file metrics.
  - Enables TOB env (`enable_tob_env`) on init.

**Required Behavior (Detailed)**
- **`get_sigmoid_loss_and_pred(name, logits, label, batch_size, sample_rate=1.0, sample_bias=False, mode=TRAIN, instance_weight=None, mask=None, logit_clip_threshold=None, predict_before_correction=True)`**
  - Reshapes `logits` to `(-1,)` and **overrides** `batch_size` using `dim_size(logits, 0)` regardless of the caller-supplied `batch_size`.
  - If `mode != PREDICT`:
    - `sample_rate` handling:
      - If `float`, fill tensor of shape `(batch_size,)`.
      - If `None`, fill tensor with `1.0`.
    - Instantiate `LogitCorrection(activation=None, sample_bias=sample_bias, name='sample_rate_correction')`.
    - Compute `logits_biased = src((logits, sample_rate))`.
    - `pred` is sigmoid of **raw** `logits` when `predict_before_correction=True`, else sigmoid of `logits_biased`.
    - If `logit_clip_threshold` set:
      - Assert `0 < logit_clip_threshold < 1`.
      - Compute `threshold = log((1 - p) / p)` and clip `logits_biased` to `[-threshold, threshold]`.
    - Compute `losses = sigmoid_cross_entropy_with_logits(labels=label.reshape(-1), logits=logits_biased)`.
    - If `instance_weight` present, reshape to `(-1,)`.
    - If `mask` present, reshape to `(-1,)` and `boolean_mask` both `losses` and `instance_weight` (if present).
    - If `instance_weight` present, multiply `losses *= instance_weight`.
    - Final `loss = reduce_sum(losses)`.
  - If `mode == PREDICT`: `loss=None`, `pred = sigmoid(logits)` (no correction and no clipping).
  - Returns `(loss, pred)` with op names using `{name}_sigmoid_*`.
- **`get_softmax_loss_and_pred(name, logits, label, mode)`**
  - `pred = argmax(softmax(logits, name='{name}_softmax_pred'), axis=1)`.
  - If `mode != PREDICT`, `loss = softmax_cross_entropy_with_logits(labels=label, logits=logits, name='{name}_softmax_loss')`.
  - Else `loss=None`. Returns `(loss, pred)`.
- **`DeviceCtxType`**
  - Constants: `INPUT_FN`, `MODEL_FN`, `INPUT_RECEIVER_FN`, `OTHERS`.
  - `all_types()` returns set of all constants.
- **`MonolithDeviceCtx(ctx_type)`**
  - Context manager for device placement; asserts `ctx_type` in `DeviceCtxType.all_types()`.
  - `__enter__`:
    - No-op if `enable_sync_training()` is false or `export_context.is_exporting()` is true.
    - Selects device function:
      - `INPUT_FN` → `input_device_fn`
      - `MODEL_FN` → `model_device_fn`
      - `INPUT_RECEIVER_FN` → `serving_input_device_fn`
      - Otherwise no-op.
    - Calls `tf.compat.v1.device(self._device_fn)` and enters it.
  - `__exit__`:
    - If `ctx_type == MODEL_FN`, calls `ensure_variables_in_device()` before exiting device scope.
    - Resets `_current` and `_device_fn`.
  - `ensure_variables_in_device()`:
    - Iterates `graph.get_operations()` and for ops whose name starts with `global_step`, calls `graph._apply_device_functions(op)` (private TF API).
- **`MonolithBaseModel(NativeTask, ABC)`**
  - `params()` defines:
    - `output_path`, `output_fields`, `delimiter` (default `\t`), `file_name`,
      `enable_grads_and_vars_summary`, `dense_weight_decay`, `clip_norm` (default `1000.0`),
      `sparse_norm_warmup_steps`, `default_occurrence_threshold`.
  - `__init__`:
    - Calls `enable_tob_env()`.
    - Initializes dicts: `fs_dict`, `fc_dict`, `slice_dict`, `_layout_dict`, `_occurrence_threshold`, `_share_slot_mapping`.
    - `_use_dense_allreduce = FLAGS.enable_sync_training`.
  - `__getattr__`:
    - For attributes in `self.p`, returns param value.
    - Special case `batch_size`: returns `eval.per_replica_batch_size` if `p.mode == EVAL`, else `train.per_replica_batch_size`.
    - Falls back to property getters or base `__getattr__`.
  - `__setattr__`:
    - If `self.p` has attr, sets there.
    - Special case `batch_size`: sets both `train.per_replica_batch_size` and `eval.per_replica_batch_size`.
  - `__deepcopy__`:
    - Deep-copies all attributes except `dump_utils` (shared reference).
  - `_get_file_ops(features, pred)`:
    - Requires `p.output_fields` set.
    - Builds `output_path = p.output_path/part-{worker_index:05d}`; opens `file_ops.WritableFile`.
    - `op_fields` is `features[field]` for each `output_fields` (comma-separated).
    - Appends predictions:
      - list/tuple → extend
      - dict → sorted by key before extending
      - scalar → append
    - Attempts to `tf.squeeze` tensors where rank > 1 and last dim == 1.
    - Formats each row using `tf.strings.format` with delimiter-joined `{}` and `summarize=-1`.
    - Uses `tf.map_fn` with `fn_output_signature=tf.string` and `tf.stop_gradient`.
    - Returns `(op_file, write_op)` where `write_op = op_file.append(tf.strings.reduce_join(result))`.
  - `_dump_item_embedding_ops(features)`:
    - Assumes instance is `DeepRoughSortBaseModel` and features contain `item_id`, `item_bias`, `item_vec`.
    - Writes `MonolithHashTable_cached_item_embeddings-00000-of-00001` under `_cal_item_cache_table_path()`.
    - Uses `WritableFile.append_entry_dump` to write.
  - `_get_real_mode(mode)`:
    - If `mode == PREDICT`, returns `PREDICT`.
    - If `mode == TRAIN`, returns `self.mode` (not necessarily `TRAIN`).
    - Otherwise raises `ValueError('model error!')`.
  - `is_fused_layout()` returns `ctx.layout_factory is not None`.
  - `instantiate()` returns `self` (no cloning).
  - `add_loss(losses)` appends one or many losses to graph-level `__losses`.
  - `losses` property:
    - Stored on `tf.compat.v1.get_default_graph()` as `__losses` list.
  - `_global_step` property:
    - Inside `maybe_device_if_allowed('/device:GPU:0')`, returns `tf.compat.v1.train.get_or_create_global_step()`.
  - `_training_hooks` property:
    - Stored on graph as `__training_hooks` list.
  - `clean()` clears feature-slot caches: `fs_dict`, `fc_dict`, `slice_dict`, `_occurrence_threshold`.
  - `create_input_fn()`:
    - Returns closure that wraps `self.input_fn(mode)` in `MonolithDeviceCtx(INPUT_FN)`.
  - `create_model_fn()`:
    - Resets caches via `clean()`.
    - Defines `model_fn_internal(features, mode, config)`:
      - `global_step = _global_step`, `real_mode = _get_real_mode(mode)`.
      - Runs `self.model_fn(features, real_mode)` inside `MonolithDeviceCtx(MODEL_FN)`.
      - Accepts either `EstimatorSpec` or `(label, loss, pred)` tuple/list:
        - `EstimatorSpec`: extract `label`, `loss`, `pred`, optional `head_name`, `classification`.
          If `pred` is dict, `head_name` becomes keys (in insertion order).
        - Tuple/list: `label, loss, pred`; if `pred` dict, `head_name` from keys; else `head_name` from
          `self.metrics.deep_insight_target`; sets `is_classification=True` and emits a warning.
        - Otherwise raises `Exception("EstimatorSpec Error!")`.
      - Validates `head_name`, `label`, `pred` shapes and alignment.
      - Normalizes `label` to `tf.identity` (name `label_{_node_name(...)}` or dict keys).
      - Calls `dump_utils.add_model_fn(self, mode, features, label, loss, pred, head_name, is_classification)`.
      - Adds auxiliary losses: `loss += tf.add_n(self.losses)` when present.
      - **Resource-constrained roughsort predict path**:
        - When not exporting, `real_mode == PREDICT`, and `FLAGS.enable_resource_constrained_roughsort`,
          and `self` is `DeepRoughSortBaseModel`, it writes item cache table and returns
          `EstimatorSpec(PREDICT, loss=1, train_op=no_op, training_hooks=[FileCloseHook]+_training_hooks, predictions=identity(...))`.
      - **Predict path**:
        - `predictions = dict(zip(head_name, pred))` for list/tuple, else `pred`.
        - If exporting or `p.output_path` is `None`: returns `EstimatorSpec(PREDICT, predictions=..., training_hooks=_training_hooks)`.
        - Else writes per-worker file using `_get_file_ops` and wraps `predictions` with `tf.identity` under control deps.
        - If exporting and `_export_outputs` populated, merges via `spec._replace(export_outputs=...)`.
      - **Metrics accumulation for train/eval**:
        - Builds `targets`, `labels_list`, `preds_list` aligned with heads.
        - If `FLAGS.disable_native_metrics` is false:
          - Classification → `tf.compat.v1.metrics.auc`; regression → `tf.compat.v1.metrics.mean_squared_error`.
          - Adds `tf.print` of metric value to stderr and `tf.compat.v1.summary.scalar`.
          - Adds update op to `train_ops`.
      - **Deep insight metrics**:
        - If any of `metrics.enable_kafka_metrics`, `enable_file_metrics`, `enable_deep_insight` and
          `metrics.deep_insight_sample_ratio > 0`:
          - Calls `metric_utils.write_deep_insight` with features, labels, preds, model_name, target, etc.
          - Optionally uses `dump_filename = f\"{dump_filename}.part-{worker_index:05d}\"`.
          - Adds op to collection `"deep_insight_op"`.
          - Adds `KafkaMetricHook` or `FileMetricHook` (only one of each type allowed; see `add_training_hook`).
      - **Eval path**:
        - If exporting or no `output_path`: returns `EstimatorSpec(mode, loss=loss, train_op=tf.group(train_ops), training_hooks=_training_hooks)`,
          with `pred`/`preds` added into `train_ops`.
        - Else writes outputs to file (same as predict) and returns EstimatorSpec with close hook.
      - **Train path**:
        - Determines `dense_optimizer` from `local_spec.optimizer` or `self._default_dense_optimizer`, else raises `Exception("dense_optimizer not found!")`.
        - Calls `dump_utils.add_optimizer(dense_optimizer)`.
        - Adds `feature_utils.apply_gradients_with_var_optimizer` to `train_ops` with:
          - `clip_type=ClipByGlobalNorm`, `clip_norm=self.clip_norm`, `dense_weight_decay=self.dense_weight_decay`,
            `global_step=_global_step`, `grads_and_vars_summary`, `sparse_norm_warmup_steps`,
            `is_fused_layout`, `use_allreduce=_use_dense_allreduce`.
        - Calls `add_batch_norm_into_update_ops()` then groups `UPDATE_OPS` and returns EstimatorSpec with `train_op=tf.group(train_ops)`.
  - `create_serving_input_receiver_fn()`:
    - Wraps `self.serving_input_receiver_fn()` in `MonolithDeviceCtx(INPUT_RECEIVER_FN)` and
      passes through `dump_utils.record_receiver`.
  - Abstract methods:
    - `input_fn(mode) -> DatasetV2`, `model_fn(features, mode)`, `serving_input_receiver_fn() -> ServingInputReceiver`.
  - `_export_outputs` property:
    - Graph-attached dict `__export_outputs` (created lazily).
  - `add_extra_output(name, outputs, head_name=None, head_type=None)`:
    - Adds `name` to collection `'signature_name'`.
    - If exporting: inserts `PredictOutput(outputs)` into `_export_outputs`, else ignores.
    - Raises `KeyError` if `name` already exists.
  - `add_training_hook(hook)`:
    - Prevents multiple `KafkaMetricHook` or `FileMetricHook`.
  - `add_layout(name, slice_list, out_type, shape_list)`:
    - Builds `OutConfig` with `OutType` mapping (`concat/stack/addn/none`).
    - For each slice, adds `slice_configs` with `feature_name`, `start`, `end`.
    - For each shape, writes dims; first dim forced to `-1`, subsequent dims from int or `.value`.
    - Stores in `_layout_dict[name]`.
  - `layout_dict` property getter/setter.
- **`MonolithModel(MonolithBaseModel)`**
  - `params()` adds `feature_list` string path.
  - `__init__`:
    - Uses provided params or class params.
    - Sets `dump_utils.enable = FLAGS.enable_model_dump`.
  - `_get_fs_conf(shared_name, slot, occurrence_threshold, expire_time)`:
    - Returns `FeatureSlotConfig` with `has_bias=False`, `slot_id=slot`, `occurrence_threshold`, `expire_time`,
      and hash table config using `GpucucoHashTableConfig` if `self.p.train.use_gpu_emb_table` else `CuckooHashTableConfig`.
  - `_embedding_slice_lookup(fc, slice_name, slice_dim, initializer, optimizer, compressor, learning_rate_fn, slice_list)`:
    - Asserts non-fused layout.
    - Accepts `fc` as feature name or `FeatureColumn`.
    - Applies `_share_slot_mapping` for shared embedding names.
    - Creates or reuses `FeatureSlice` in `slice_dict[feature_name][slice_name]`.
    - Appends `(fc.feature_name, fc_slice)` to `slice_list`.
    - Returns `fc.embedding_lookup(fc_slice)`.
  - `create_embedding_feature_column(feature_name, occurrence_threshold=None, expire_time=36500, max_seq_length=0, shared_name=None, combiner=None)`:
    - Converts `combiner` string to `FeatureColumn.reduce_sum/reduce_mean/first_n`.
    - Resolves `feature_name` and `slot` via `get_feature_name_and_slot`.
    - If `feature_name` exists in `fc_dict`, returns it.
    - If `shared_name` provided:
      - Stores `_share_slot_mapping[feature_name] = shared_name`.
      - Reuses existing `fs_dict` or `fc_dict` for shared slot if present.
      - Else creates new `FeatureSlot` for `shared_name` and stores in `fs_dict`.
      - If shared slot not created first and `get_feature_name_and_slot` fails, raises exception with explicit message.
    - If not shared: creates new `FeatureSlot` via `ctx.create_feature_slot`.
    - Default `combiner`: `first_n(max_seq_length)` for sequence features, else `reduce_sum`.
    - Creates `FeatureColumn`, stores in `fc_dict`, returns it.
  - `lookup_embedding_slice(features, slice_name, slice_dim=None, initializer=None, optimizer=None, compressor=None, learning_rate_fn=None, group_out_type='add_n', out_type=None)`:
    - Computes `layout_name = f'{slice_name}_{md5(sorted(features)).hexdigest()}'`.
    - If fused layout:
      - If `features` is list/tuple and `slice_dim` int and contains group tuples/lists: raises `ValueError("group pool is not support when fused_layout")`.
      - Returns `ctx.layout_factory.get_layout(layout_name)`.
    - Otherwise builds `feature_embeddings` and `slice_list` from `features` in these cases:
      - `dict`: feature name → slice dim.
      - `list/tuple` + `slice_dim` int:
        - if all elements are (str|int|FeatureColumn): fixed-dim list.
        - if all elements are list/tuple groups: `group_out_type` must be `concat` or `add_n`;
          each group is a list of feature names; group embeddings are summed or concatenated.
        - else raises `ValueError("ValueError for features")`.
      - `list/tuple` of `(feature, dim)` pairs: variable dims.
      - Otherwise raises `ValueError("ValueError for features")`.
    - If `out_type is None`: records layout with `shape_list` from embeddings and returns list of embeddings.
    - Else `out_type` in `{concat, stack, add_n, addn}`:
      - `concat`: `tf.concat(axis=1)`
      - `stack`: `tf.stack(axis=1)`
      - `add_n/addn`: `tf.add_n`
      - Records layout and returns tensor.
  - `share_slot(features=None, share_meta=None, variant_type='example', suffix='share')`:
    - For each `name -> (inplace, slot)` in `share_meta`:
      - Registers slot mapping via `register_slots`, using `shared_name = f'{name}_{suffix}'` when not inplace.
    - If `features` is dict:
      - `inplace=True`: `features[name] = switch_slot(features[name], slot)`
      - Else: `features[shared_name] = switch_slot(features[name], slot)`
      - Returns modified dict.
    - Else returns `map_fn = lambda tensor: switch_slot_batch(tensor, share_meta, variant_type=variant_type, suffix=suffix)`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` for model base + training loop glue; `monolith-rs/crates/monolith-layers` for logit correction; `monolith-rs/crates/monolith-data` for feature slots; `monolith-rs/crates/monolith-hash-table` for embedding tables; `monolith-rs/crates/monolith-serving` for export signatures.
- Rust public API surface:
  - `get_sigmoid_loss_and_pred` / `get_softmax_loss_and_pred` equivalents in a `losses` or `metrics` module.
  - `DeviceCtxType` + `MonolithDeviceCtx` analog for device placement (no-op in pure Candle; required for TF runtime backend).
  - `MonolithBaseModel` trait + `MonolithModel` struct that mirror param plumbing, embedding-slice helpers,
    training/eval/predict flow, file output, and metrics hooks.
  - Export signature registry mirroring `add_extra_output`.
- Data model mapping:
  - Feature slots/slices map to Rust `FeatureSlot`, `FeatureColumn`, `FeatureSlice` types.
  - Layout dictionary maps to Rust `OutConfig` protobufs (via `monolith-proto`) with consistent shape semantics (`-1` batch dim).
- Feature gating:
  - **Default**: Candle-native backend; `MonolithDeviceCtx` becomes a no-op.
  - **Optional**: TF runtime backend only when `saved_model.pb` + `libtensorflow` + custom ops present.
  - Metrics hooks (Kafka/file/deep_insight) should be feature-gated with optional dependencies.
- Integration points:
  - Training entrypoints (Estimator analog) must call into `create_model_fn` flow equivalents.
  - Export flow must honor `export_context`-like state to control output signatures and device placement.

**Implementation Steps (Detailed)**
1. Define Rust equivalents for loss helpers with identical shape handling, sample-rate correction, clipping, and mask/weight semantics.
2. Implement `MonolithDeviceCtx` abstraction; in Candle, no-op; in TF runtime, map to device placement APIs.
3. Build Rust `MonolithBaseModel` trait:
   - Store per-graph/per-run `losses`, `training_hooks`, and `export_outputs`.
   - Implement `create_input_fn`, `create_model_fn`, `create_serving_input_receiver_fn` analogs.
4. Implement file output writer with exact formatting and ordering (output_fields order, dict pred sorted by key, delimiter handling).
5. Port metrics collection:
   - AUC/MSE metrics with logging + summary behavior (or compatible replacements).
   - Deep insight pipeline with Kafka/file hooks and per-worker filename suffixing.
6. Implement embedding feature slot + slice machinery with shared slots and layout tracking.
7. Add export signature registry and merge semantics for extra outputs.
8. Add feature flags and error handling parity for unsupported paths (e.g., fused layout group pooling).
9. Add cross-language tests: compare output file contents, metric logging events, and embedding slice layouts.

**Tests (Detailed)**
- Python tests: `monolith/native_training/model_comp_test.py` (uses `MonolithModel`).
- Rust tests: none yet (needs new parity tests for loss helpers, file output formatting, and embedding slice layouts).
- Cross-language parity test:
  - Golden test that runs a minimal TF model with two heads and compares loss/pred outputs + output file formatting.
  - Embedding slice layout test comparing `OutConfig` shape/slice configs between Python and Rust.

**Gaps / Notes**
- `_get_real_mode` rejects `EVAL`; this is likely intentional in their training flow (only TRAIN/PREDICT). If Rust adds eval mode, define parity behavior explicitly.
- Prediction file output uses **sorted dict keys** only when `pred` is dict; list/tuple order is preserved.
- `create_model_fn` treats tuple/list return as classification and emits warning; exact warning text should be preserved if exposed.
- Uses private TF API `graph._apply_device_functions` for `global_step` ops.
- Metrics include `tf.print(..., output_stream=sys.stderr)` side effects.
- Deep insight hooks only attach when sample ratio > 0.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/native_task.py`
<a id="monolith-native-training-native-task-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 213
- Purpose/role: Defines the `NativeTask` base class (TF-native training/eval/serving) and the `NativeContext` helper for feature slot creation, embedding gradient application, and async functions.
- Key symbols/classes/functions: `NativeContext`, `NativeTask`.
- External dependencies: TensorFlow, `monolith.core.base_task.BaseTask`, `monolith.core.hyperparams`,
  `monolith.native_training.feature`, `monolith.native_training.prefetch_queue`,
  `monolith.native_training.model_export.export_context.ExportMode`.
- Side effects:
  - Raises `ValueError` if both `feature_factory` and `layout_factory` are provided in `NativeContext`.
  - Delegates to async function manager (may enqueue TF ops).

**Required Behavior (Detailed)**
- **`NativeContext(feature_factory=None, async_function_mgr=None, layout_factory=None)`**
  - Stores `feature_factory`, `async_function_mgr`, `layout_factory`.
  - If both `feature_factory` and `layout_factory` are set, raises:
    - `ValueError("Cannot set feature_factory and layout_factory in the same time")`.
- **`NativeContext.create_feature_slot(config)`**
  - If `layout_factory` present, delegates to `layout_factory.create_feature_slot(config)`.
  - Else uses `feature_factory.create_feature_slot(config)`.
  - No TF ops are created by this call (per docstring).
- **`NativeContext.apply_embedding_gradients(grads_and_vars, scale=1)`**
  - If `layout_factory` present, delegates to `layout_factory.apply_gradients(grads_and_vars)`.
  - Else delegates to `feature_factory.apply_gradients(grads_and_vars, scale=scale)`.
  - Expects `grads_and_vars` from `FeatureColumn.get_all_embeddings_concatenated`.
- **`NativeContext.add_async_function(target, args=None, kwargs=None, is_async=None, queue_name="async_queue")`**
  - Delegates to `async_function_mgr.add_async_function(...)`.
  - Returns enqueue op if async, else result of `target`.
  - Semantic contract (documented): tensors used by async function should be passed via args/kwargs only.
- **`NativeTask(BaseTask, abc.ABC)`**
  - `params()` extends BaseTask params with:
    - `metrics.*`:
      - `enable_deep_insight` (default False), `deep_insight_target` ("ctr_head"),
        `deep_insight_name` (None), `deep_insight_sample_ratio` (0.01),
        `extra_fields_keys` (list),
        `enable_throughput_hook` (True),
        `enable_kafka_metrics` (False),
        `enable_tf2_profiler_hook` (True),
        `enable_file_metrics` (False),
        `file_base_name` ("/vepfs/jaguar_deepinsight_results"),
        `file_ext` ("txt"),
        `parse_fn`/`key_fn`/`layout_fn` (None),
        `dump_filename` (""),
        `use_data_service` (False).
    - `mode`: `tf.estimator.ModeKeys.TRAIN` (temporary; doc says will be removed).
    - `train.*`:
      - `max_pending_seconds_for_barrier` (30),
      - `slow_start_steps` (0),
      - `sample_bias` (0.0),
      - `use_gpu_emb_table` (False),
      - `use_fountain` (False),
      - `fountain_zk_host` (""), `fountain_model_name` (""), `fountain_parse_on_server` (False),
        `fountain_precompute_value_rowids` (False).
    - `serving.*`:
      - `export_with_gpu_allowed` (False),
      - `export_with_cleared_entry_devices` (False),
      - `export_when_saving` (False),
      - `export_dir_base` ("exported_models"),
      - `export_mode` (`ExportMode.DISTRIBUTED`),
      - `shared_embedding` (True),
      - `with_remote_gpu` (False).
  - `__init__(params)`:
    - Calls `BaseTask.__init__` and sets `self._ctx = NativeContext()` and `self.p = params`.
  - `ctx` property returns `self._ctx`.
  - Abstract methods:
    - `create_input_fn(self, mode)`
    - `create_model_fn(self)`
  - `create_serving_input_receiver_fn()`:
    - Returns `None` by default; callers must override when `serving.export_when_saving` is enabled.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` (task base + context), `monolith-rs/crates/monolith-data` (feature factories/layouts).
- Rust public API surface:
  - `NativeContext` struct with `create_feature_slot`, `apply_embedding_gradients`, `add_async_function`.
  - `NativeTask` trait with `params()`, `create_input_fn`, `create_model_fn`, `create_serving_input_receiver_fn`.
- Data model mapping:
  - `feature_factory` ↔ `FeatureFactory` trait.
  - `layout_factory` ↔ `EmbeddingLayoutFactory` trait (optional, mutually exclusive).
- Feature gating:
  - Async function manager (prefetch queue) behind feature flag if not supported by backend.
  - Export-related params gated by serving feature.
- Integration points:
  - `NativeTask` becomes base for `MonolithBaseModel` in Rust; param defaults must match Python.

**Implementation Steps (Detailed)**
1. Port `NativeContext` with mutual exclusion validation and delegation to feature/layout factories.
2. Implement an async function manager interface in Rust (or stubs if backend lacks it).
3. Port `NativeTask.params()` with identical defaults and nested param groups.
4. Implement `NativeTask` base that stores `params` and `ctx`.
5. Add validation hooks so `create_serving_input_receiver_fn` must be overridden when export is enabled.
6. Add unit tests for parameter defaults and mutual exclusion errors.

**Tests (Detailed)**
- Python tests: none dedicated; covered indirectly by model/task usage.
- Rust tests: add unit tests for `NativeContext` validation and param defaults.
- Cross-language parity test: compare serialized defaults from Python vs Rust `params()` output.

**Gaps / Notes**
- `OutConfig`, `OutType`, `TensorShape` are imported but unused in this file (no runtime effect).
- `create_serving_input_receiver_fn` returning `None` is explicitly invalid when export is enabled; Rust should enforce or document this.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/native_task_context.py`
<a id="monolith-native-training-native-task-context-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 58
- Purpose/role: Defines a global per-process task context (`NativeTaskContext`) and helper functions to set/get it.
- Key symbols/classes/functions: `NativeTaskContext`, `with_ctx`, `get`.
- External dependencies: `contextlib`, `typing.NamedTuple`, `monolith.agent_service.backends.SyncBackend`.
- Side effects: Mutates module-level `_CTX` global.

**Required Behavior (Detailed)**
- **`NativeTaskContext(NamedTuple)`** with fields:
  - `num_ps: int`
  - `ps_index: int`
  - `num_workers: int`
  - `worker_index: int`
  - `model_name: str`
  - `sync_backend: SyncBackend`
  - `server_type: str`
- **`with_ctx(ctx)`** (context manager):
  - Stores previous `_CTX` as `old_ctx`, sets `_CTX = ctx`, yields.
  - On exit:
    - If `old_ctx` is not `None`, restores `_CTX = old_ctx`.
    - If `old_ctx` is `None`, leaves `_CTX` set to `ctx` (no reset to `None`).
- **`get()`**:
  - If `_CTX is None`, returns a new `NativeTaskContext` with defaults:
    - `num_ps=0`, `ps_index=0`, `num_workers=1`, `worker_index=0`,
      `server_type=""`, `model_name=""`, `sync_backend=None`.
  - Else returns `_CTX` object as-is (no copy).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` (task context).
- Rust public API surface:
  - `struct NativeTaskContext { ... }`
  - `with_ctx(ctx, |...| { ... })` or RAII guard to set/restore.
  - `get_ctx()` returns current context or default.
- Data model mapping:
  - `sync_backend` should be an enum or trait object mirroring `SyncBackend`.
- Feature gating: none.
- Integration points:
  - Training/serving flows should call `get_ctx()` for worker index and model name.

**Implementation Steps (Detailed)**
1. Implement a thread-local or global context storage in Rust (match Python semantics).
2. Implement context guard that **only** restores prior context if it existed.
3. Provide `get_ctx()` returning default values when unset.
4. Add unit tests for default context and nesting behavior.

**Tests (Detailed)**
- Python tests: none dedicated.
- Rust tests: add unit tests for `with_ctx` nesting and default values.
- Cross-language parity test: compare defaults and nesting semantics.

**Gaps / Notes**
- The context manager does **not** clear `_CTX` when exiting outermost scope; Rust should match this behavior exactly.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/nested_tensors.py`
<a id="monolith-native-training-nested-tensors-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 110
- Purpose/role: Utility for flattening arbitrarily nested structures containing Tensors/RaggedTensors/primitive constants into a flat tensor list and reconstructing the nested structure after computation.
- Key symbols/classes/functions: `_iterate`, `NestedTensors`.
- External dependencies: `tensorflow`, `itertools`, `copy`.
- Side effects:
  - Mutates dict/list/tuple structures passed into `_iterate` (and thus into `NestedTensors.__init__`) by replacing leaves with object IDs.

**Required Behavior (Detailed)**
- **`_iterate(nested, action)`** (internal helper):
  - Recurses through nested structures; for each leaf, applies `action(leaf)` and replaces leaf with the return value.
  - Handles:
    - `None`: no action, returns `None`.
    - `list`/`tuple`: maps recursively; preserves tuple vs list types.
    - `dict`: iterates `items()` and assigns `nested[k] = _iterate(v, action)` (mutates dict).
    - other: replaces with `action(nested)`.
- **`NestedTensors(nested)`**
  - Stores original `nested` reference as `self._nested`.
  - Initializes:
    - `_id_mapping: dict[obj_id -> (kind_idx, list_index)]`
    - `_ragged_tensors: List[tf.RaggedTensor]`
    - `_tensors: List[tf.Tensor]`
    - `_other_objs: List[Any]` for allowed constants/objects.
  - Calls `_iterate(self._nested, self._add_tensor)` so **all leaves become object IDs**.
- **`_add_tensor(tensor)`**:
  - For each unique object ID:
    - `tf.Tensor` → kind 0, appended to `_tensors`.
    - `tf.RaggedTensor` → requires `ragged_rank == 1`; else raises
      `ValueError("Nested tensor doesn't support nested RaggedTensor.")`; kind 1, appended to `_ragged_tensors`.
    - `(bool, int, str, tf.Variable, None)` → kind 2, appended to `_other_objs` (preserved).
    - Otherwise raises `ValueError("Tensor is not supported. {}".format(tensor))`.
  - Returns the object ID (an int), which replaces the leaf.
- **`get_tensors()`**
  - Flattens ragged tensors into `(values, row_splits)` pairs.
  - Returns `self._tensors + flatten_ragged_tensors` as a single list of `tf.Tensor`.
- **`get_nested_result(tensors)`**
  - Splits `tensors` into:
    - `tensors[:len(self._tensors)]` for dense tensors,
    - `flatten_ragged_tensors = tensors[len(self._tensors):]`.
  - Asserts `len(flatten_ragged_tensors) == len(self._ragged_tensors) * 2`.
  - Reconstructs ragged tensors via `_flatten_to_ragged`.
  - Uses `_id_mapping` to replace object IDs in a deep-copied nested structure with the
    corresponding tensor/ ragged tensor / other object.
  - Returns reconstructed nested structure with original container shapes.
- **`_convert_ragged_to_tensors(ragged)`**:
  - Returns `(ragged.values, ragged.row_splits)`.
- **`_convert_tensors_to_ragged(values, row_splits)`**:
  - Returns `tf.RaggedTensor.from_row_splits(values, row_splits, validate=False)`.
- **`_ragged_to_flatten(ragged_tensors)`**:
  - Returns flattened list `[values0, row_splits0, values1, row_splits1, ...]`.
- **`_flatten_to_ragged(tensors)`**:
  - Takes even/odd positions as `values` and `row_splits`; reconstructs list of ragged tensors.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tensor/src` (nested tensor utilities).
- Rust public API surface:
  - `NestedTensors` struct with `get_tensors()` and `get_nested_result()`.
  - Internal recursion helper for nested structures.
- Data model mapping:
  - Support for dense tensors + ragged tensors with `values`/`row_splits`.
  - Preserve primitive constants and variables when rebuilding.
- Feature gating: none (unless ragged tensors are optional in backend).
- Integration points:
  - Use in async/queue or feature pipelines where nested structures must be flattened for execution.

**Implementation Steps (Detailed)**
1. Implement a `NestedValue` enum for nested structures (list/tuple/map/leaf).
2. Track object IDs and build mapping identical to Python (kind indices 0/1/2).
3. Support ragged tensors only with rank 1; error on higher rank.
4. Implement flatten/unflatten with identical ordering (values then row_splits).
5. Add round-trip tests mirroring Python coverage.

**Tests (Detailed)**
- Python tests: `monolith/native_training/nested_tensors_test.py`.
- Rust tests: add unit tests for:
  - basic dict/tuple nesting,
  - constants-only nesting (no tensors),
  - ragged tensor round-trip,
  - ragged rank error path.
- Cross-language parity test: round-trip nested structures and compare to Python evaluation.

**Gaps / Notes**
- `_iterate` mutates dicts in place; `NestedTensors` does **not** copy input `nested` before replacing leaves.
- Order of dict iteration affects reconstruction order; Python preserves insertion order.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/nested_tensors_test.py`
<a id="monolith-native-training-nested-tensors-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 57
- Purpose/role: Unit tests for `NestedTensors` flatten/unflatten behavior.
- Key symbols/classes/functions: `NestedTensorTest`.
- External dependencies: TensorFlow, `monolith.native_training.nested_tensors`.
- Side effects: Uses TF test harness; disables eager execution in `__main__`.

**Required Behavior (Detailed)**
- `testBasic`:
  - Creates nested structure with tensors `{ "a": ones([]), "b": (ones([]), ones([])) }`.
  - Replaces flattened tensors with zeros; reconstructs and asserts dict equals `{"a": 0, "b": (0, 0)}`.
- `testConstant`:
  - Uses nested constants only (`{"a": {"b": 2}}`).
  - Expects `get_tensors()` returns empty list and reconstruction returns the same constants.
- `testRaggedTensor`:
  - Round-trip a ragged tensor `tf.ragged.constant([[], [1], [2, 3]])`.
  - Asserts reconstructed value equals original ragged list.
- `testRaggedTensorWithPlaceHolder`:
  - Creates ragged tensor, gets flattened tensors, builds placeholders for each flattened tensor.
  - Calls `get_nested_result(tensors)` (no explicit assert).
  - Implicitly validates that placeholder creation doesn't break `get_nested_result` call path.
- `__main__`:
  - Calls `tf.compat.v1.disable_eager_execution()` and runs `tf.test.main()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tensor/src` (tests alongside `NestedTensors`).
- Rust public API surface: test helpers to validate nested round-trip and ragged support.
- Data model mapping: ragged tensors must support `values` + `row_splits`.
- Feature gating: ragged tests conditional on ragged support.
- Integration points: ensure tests reflect the same ordering and reconstruction semantics as Python.

**Implementation Steps (Detailed)**
1. Port test cases to Rust equivalents (basic nesting, constants-only, ragged round-trip).
2. Add a placeholder/shape-only test if the backend supports placeholders or uninitialized tensors.
3. Ensure test runner matches eager/graph expectations (if applicable).

**Tests (Detailed)**
- Python tests: `NestedTensorTest` in this file.
- Rust tests: implement `nested_tensors_basic`, `nested_tensors_constants`, `nested_tensors_ragged`.
- Cross-language parity test: compare round-trip output for identical inputs.

**Gaps / Notes**
- `testRaggedTensorWithPlaceHolder` has no assertions; it only exercises the code path.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/net_utils.py`
<a id="monolith-native-training-net-utils-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 133
- Purpose/role: Network utility helpers for address formatting, local IP discovery, and threaded liveness checks for host:port addresses.
- Key symbols/classes/functions: `NodeAliveChecker`, `is_ipv6_address`, `concat_ip_and_port`, `get_local_ip`, `is_ipv4_supported`, `get_local_server_addr`, `AddressFamily`.
- External dependencies: `socket`, `threading`, `queue.Queue`, `ipaddress`, `absl.logging`.
- Side effects:
  - Opens TCP sockets to check liveness.
  - Spawns threads in `NodeAliveChecker.__init__`.
  - Prints connection errors to stdout.

**Required Behavior (Detailed)**
- **`NodeAliveChecker(addrs, timeout=1, num_thread=10)`**
  - Initializes with:
    - `_addrs` list, `_timeout`, `_num_thread`.
    - `_lock`, `_alive` set, `_dead` set.
    - `_q = Queue()` populated with all `addrs`.
  - Immediately starts `_start()` (blocks until all threads finish).
- **`_ping(addr)`**
  - Expects `addr` string in `host:port` form; uses `addr.rsplit(':', 1)` and `ip.strip('[]')`.
  - Determines IPv6 via `is_ipv6_address`.
  - Creates `socket.socket(AF_INET6 if IPv6 else AF_INET, SOCK_STREAM)` and `settimeout(timeout)`.
  - On successful `connect`, adds `addr` to `_alive`.
  - On exception:
    - Prints `cannot connect to {addr}, because {err}`.
    - Adds `addr` to `_dead`.
  - Always closes socket if created.
- **`_check_open()`**
  - Drains `_q` using `get_nowait()` in a loop; for each addr calls `_ping`.
  - Exits on `queue.Empty`.
- **`_start()`**
  - Spawns `_num_thread` threads running `_check_open()`.
  - Joins all threads (synchronous completion).
- **`all_nodes_alive()`** returns `len(_dead) == 0`.
- **`get_dead_nodes()`** returns `list(_dead)` (order arbitrary).
- **`get_alive_nodes()`** returns `list(_alive)` (order arbitrary).
- **`get_addrs()`** returns `_addrs` (original list).
- **`is_ipv6_address(ip)`**
  - Uses `ipaddress.ip_address(ip)`; returns False on `ValueError`, else `ip_obj.version == 6`.
- **`concat_ip_and_port(ip, port)`**
  - If not IPv6, returns `"ip:port"`.
  - If IPv6, returns `"[ip]:port"`.
- **`get_local_ip()`**
  - Tries `socket.getaddrinfo(gethostname(), None)[0][4][0]`.
  - On `socket.gaierror`, retries with `family=AF_INET6`.
- **`is_ipv4_supported()`**
  - Returns `not is_ipv6_address(get_local_ip())`.
- **`get_local_server_addr(port)`**
  - Returns `concat_ip_and_port(get_local_ip(), port)`.
  - Docstring notes IPv4 hosts return `gethostbyname(gethostname())` equivalent.
- **`AddressFamily`**
  - Constants: `IPV4 = 'ipv4'`, `IPV6 = 'ipv6'`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src` (network utils).
- Rust public API surface:
  - `NodeAliveChecker` equivalent (synchronous check on construction or explicit `run()`).
  - Helpers for IPv4/IPv6 detection and addr formatting.
- Data model mapping:
  - Socket API via `std::net` (`TcpStream::connect_timeout`).
  - Address parsing should accept bracketed IPv6 addresses.
- Feature gating: none.
- Integration points: used by distributed training/serving orchestration.

**Implementation Steps (Detailed)**
1. Implement `is_ipv6_address` using `std::net::IpAddr` parse.
2. Implement `concat_ip_and_port` with IPv6 bracket handling.
3. Implement `get_local_ip` with fallback IPv6 resolution.
4. Implement `NodeAliveChecker` with thread pool + shared sets protected by mutex.
5. Preserve behavior of printing on connection failure (stdout).
6. Add tests for IPv6 formatting and local addr retrieval.

**Tests (Detailed)**
- Python tests: `monolith/native_training/net_utils_test.py`.
- Rust tests: add unit tests for concat and local addr; add mocked checker tests.
- Cross-language parity test: compare formatting and IPv6 detection results.

**Gaps / Notes**
- `NodeAliveChecker` performs all checks in `__init__` and blocks until complete.
- Address parsing assumes `host:port` or `[ipv6]:port` format; raw IPv6 with port and no brackets may not parse correctly.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/net_utils_test.py`
<a id="monolith-native-training-net-utils-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 94
- Purpose/role: Tests for `net_utils` helpers and `NodeAliveChecker`.
- Key symbols/classes/functions: `NetUtilsTest`.
- External dependencies: `unittest`, `unittest.mock`, `random`, `time`, `absl.logging`, `monolith.native_training.net_utils`.
- Side effects: Uses randomized sleep; modifies module-level `_FAILED_TIME` and `_DEAD_SET`.

**Required Behavior (Detailed)**
- Custom `socket` class emulates `socket.socket` with:
  - `connect` sleeping random duration in `[0, 2 * timeout]`.
  - Marks a failure when sleep > timeout, increments `_FAILED_TIME`, and adds addr to `_DEAD_SET`.
- `test_basic`:
  - Mocks `net_utils.socket.socket` to return custom socket.
  - Creates `NodeAliveChecker` with 5 localhost addrs.
  - Asserts:
    - `get_addrs()` matches input set.
    - `len(alive) == 5 - _FAILED_TIME`, `len(dead) == _FAILED_TIME`.
    - Alive set equals input set minus `_DEAD_SET`; dead set equals `_DEAD_SET`.
    - `all_nodes_alive()` true only if `_FAILED_TIME == 0`.
- `test_concat_ip_and_port`:
  - Asserts IPv4 and hostname formatting (no brackets) and IPv6 formatting (`[::1]:10`).
- `test_get_local_server_addr`:
  - Asserts result is non-`None`.
- `__main__`: runs `unittest.main()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src` (network utils tests).
- Rust public API surface: tests for `concat_ip_and_port`, `get_local_server_addr`, and `NodeAliveChecker`.
- Data model mapping: mock TCP connection attempts and timeouts.
- Feature gating: none.
- Integration points: verify `NodeAliveChecker` behavior under simulated timeouts.

**Implementation Steps (Detailed)**
1. Add deterministic tests for `concat_ip_and_port` with IPv4 and IPv6.
2. Add a test for local server addr non-empty.
3. Implement a mockable connector for `NodeAliveChecker` to emulate timeouts without real sockets.

**Tests (Detailed)**
- Python tests: `NetUtilsTest` in this file.
- Rust tests: create `net_utils_concat`, `net_utils_local_addr`, `node_alive_checker_mocked`.
- Cross-language parity test: compare IPv6 formatting and default addr shapes.

**Gaps / Notes**
- Randomized sleeps can make the Python test non-deterministic; Rust tests should be deterministic.
- `_FAILED_TIME` and `_DEAD_SET` are global and not reset between tests.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/optimizers/adamom.py`
<a id="monolith-native-training-optimizers-adamom-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 68
- Purpose/role: Defines `AdamomOptimizer`, a TF v1 optimizer backed by a custom Monolith op (`resource_apply_adamom`).
- Key symbols/classes/functions: `AdamomOptimizer`.
- External dependencies: TensorFlow v1 optimizer APIs, `monolith.native_training.runtime.ops.gen_monolith_ops`.
- Side effects: Creates optimizer slot variables (`m`, `v`, `c`) on first use.

**Required Behavior (Detailed)**
- **`AdamomOptimizer(tf.compat.v1.train.Optimizer)`**
  - `__init__(learning_rate=5e-6, ada_decay=0.9999, mom_decay=0.99, epsilon=1e-6, weight_decay=0.0, use_locking=False, name="Adamom")`:
    - Stores parameters on instance.
    - `_learning_rate_tensor` initialized to `None`.
  - `_create_slots(var_list)`:
    - For each variable `v`, creates zero slots:
      - `"m"` with name scope `self._name + "/m"`.
      - `"v"` with name scope `self._name + "/v"`.
      - `"c"` with name scope `self._name + "/c"`.
  - `_prepare()`:
    - Resolves learning rate via `_call_if_callable`.
    - Converts to tensor named `"learning_rate"`.
  - `_resource_apply_dense(grad, var)`:
    - Retrieves slots `m`, `v`, `c`.
    - Calls `training_ops.resource_apply_adamom` with:
      - `var.handle`, `m.handle`, `v.handle`, `c.handle`,
      - `learning_rate` cast to `grad.dtype.base_dtype`,
      - `ada_decay`, `mom_decay`, `epsilon`, `weight_decay`,
      - `grad`,
      - `use_locking=self._use_locking`.
  - Sparse gradients: no `_resource_apply_sparse` override; relies on base-class behavior (likely unsupported).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-optimizer/src`.
- Rust public API surface:
  - `AdamomOptimizer` with identical hyperparameters.
  - Slot state for `m`, `v`, `c`.
- Data model mapping:
  - Slot tensors stored alongside parameters; update rule must match custom op semantics.
- Feature gating:
  - If TF runtime backend enabled, use TF custom op; otherwise implement in Rust.
- Integration points:
  - Used by training loop in `MonolithBaseModel.create_model_fn`.

**Implementation Steps (Detailed)**
1. Confirm exact math used by `resource_apply_adamom` (check TF custom op source).
2. Implement slot creation and zero-initialization matching TF names (`/m`, `/v`, `/c`).
3. Implement dense update rule and weight decay handling.
4. Decide behavior for sparse gradients (error or unsupported).
5. Add tests reproducing Python values in `adamom_test.py`.

**Tests (Detailed)**
- Python tests: `monolith/native_training/optimizers/adamom_test.py`.
- Rust tests: add deterministic numeric test for a single variable update.
- Cross-language parity test: compare slot values and updated var after one step.

**Gaps / Notes**
- Custom op semantics are not visible in this file; parity depends on `resource_apply_adamom` implementation.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/optimizers/adamom_test.py`
<a id="monolith-native-training-optimizers-adamom-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 57
- Purpose/role: Validates `AdamomOptimizer` slot creation and update values.
- Key symbols/classes/functions: `AdamomTest`.
- External dependencies: TensorFlow v1 test APIs, `monolith.native_training.optimizers.adamom`.
- Side effects: Disables eager execution in `__main__`.

**Required Behavior (Detailed)**
- `testBasic`:
  - Creates variable `v=[0.1]` and loss `loss = 0.12 * v`.
  - Optimizer: `AdamomOptimizer(learning_rate=0.1, weight_decay=0.01, ada_decay=0.99, mom_decay=0.9)`.
  - Runs one `minimize` step.
  - Reads all variables and asserts:
    - slot `m` ≈ `0.0121`
    - slot `c` ≈ `1.0`
    - slot `v` ≈ `0.014641`
    - variable `v` ≈ `0.090000336`
  - Expects exactly 4 variables (`m`, `v`, `c`, and the original variable).
- `__main__`:
  - `tf.compat.v1.disable_eager_execution()`, then `tf.test.main()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-optimizer/src` (tests).
- Rust public API surface: `AdamomOptimizer` update step and slot access.
- Data model mapping: slot tensors must be accessible and comparable.
- Feature gating: custom op parity when TF runtime backend used.
- Integration points: training loop tests or direct optimizer tests.

**Implementation Steps (Detailed)**
1. Reproduce the one-step update with a single scalar variable.
2. Assert slot values and updated var match Python within tolerance.
3. Ensure slot naming/ordering does not affect test outcomes.

**Tests (Detailed)**
- Python tests: `AdamomTest` in this file.
- Rust tests: add `adamom_basic_update` golden test.
- Cross-language parity test: compare slot/var values for the same input and hyperparameters.

**Gaps / Notes**
- The numeric expectations depend on custom op semantics; confirm via TF op source.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/optimizers/rmsprop.py`
<a id="monolith-native-training-optimizers-rmsprop-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 102
- Purpose/role: Defines `RmspropOptimizer`, a TF v1 optimizer backed by the custom `resource_apply_rmsprop` op with optional v2 behavior.
- Key symbols/classes/functions: `RmspropOptimizer`.
- External dependencies: TensorFlow v1 optimizer APIs, `monolith.native_training.runtime.ops.gen_monolith_ops`.
- Side effects: Creates optimizer slot variables (`m`, `v`) on first use.

**Required Behavior (Detailed)**
- **`RmspropOptimizer(tf.compat.v1.train.Optimizer)`**
  - `__init__(learning_rate=5e-6, beta1=0.99, beta2=0.999, epsilon=1e-8, weight_decay=0.0, use_locking=False, use_v2=False, name="Rmsprop")`:
    - Stores parameters on instance, including `use_v2`.
  - `_create_slots(var_list)`:
    - For each variable `v`, creates zero slots:
      - `"m"` with name scope `self._name + "/m"`.
      - `"v"` with name scope `self._name + "/v"`.
  - `_prepare()`:
    - Resolves learning rate via `_call_if_callable`.
    - Converts to tensor named `"learning_rate"`.
  - `_apply_dense(grad, var)`:
    - Always raises `NotImplementedError("Please use tf.compat.v1.disable_eager_execution() instead of tf.compat.v1.disable_v2_behavior()")`.
  - `_resource_apply_dense(grad, var)`:
    - Retrieves slots `m`, `v`.
    - Calls `training_ops.resource_apply_rmsprop` with:
      - `var.handle`, `m.handle`, `v.handle`,
      - `learning_rate` cast to `grad.dtype.base_dtype`,
      - `beta1`, `beta2`, `epsilon`, `weight_decay`,
      - `grad`,
      - `use_locking=self._use_locking`, `use_v2=self._use_v2`.
  - Sparse gradients: no `_resource_apply_sparse` override.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-optimizer/src`.
- Rust public API surface:
  - `RmspropOptimizer` with `use_v2` toggle.
  - Slot state for `m` and `v`.
- Data model mapping:
  - Slot tensors stored alongside parameters; update rule must match custom op semantics.
- Feature gating:
  - If TF runtime backend enabled, use TF custom op; otherwise implement in Rust.
- Integration points:
  - Used by training loop in `MonolithBaseModel.create_model_fn` or optimizer registry.

**Implementation Steps (Detailed)**
1. Confirm exact math used by `resource_apply_rmsprop` and its `use_v2` branch.
2. Implement slot creation and zero-initialization matching TF names (`/m`, `/v`).
3. Implement dense update rule and weight decay handling.
4. Decide sparse gradient behavior (error or unsupported).
5. Add tests reproducing Python values from `rmsprop_test.py` and `rmspropv2_test.py`.

**Tests (Detailed)**
- Python tests: `monolith/native_training/optimizers/rmsprop_test.py`, `monolith/native_training/optimizers/rmspropv2_test.py`.
- Rust tests: add deterministic numeric tests for both v1 and v2 behavior.
- Cross-language parity test: compare slot/var values for one update step.

**Gaps / Notes**
- Custom op semantics are not visible in this file; parity depends on `resource_apply_rmsprop` implementation.
- `_apply_dense` raises a hard error; Rust should surface an equivalent error if similar path exists.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/optimizers/rmsprop_test.py`
<a id="monolith-native-training-optimizers-rmsprop-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 77
- Purpose/role: Tests for `RmspropOptimizer` (v1 behavior) including GPU/CPU consistency.
- Key symbols/classes/functions: `RmspropTest`, `build_graph`.
- External dependencies: TensorFlow, `tensorflow.python.framework.test_util`, `monolith.native_training.optimizers.rmsprop`.
- Side effects: Uses GPU if available and compares CPU/GPU results.

**Required Behavior (Detailed)**
- `build_graph()`:
  - Creates variable `v=[0.1]`, loss `0.12 * v`.
  - Optimizer: `RmspropOptimizer(learning_rate=0.1, weight_decay=1, beta1=0.9, beta2=0.9, epsilon=0.1)`.
  - Returns `opt.minimize(loss)`.
- `testBasic`:
  - Runs training once on GPU (if available) in a fresh graph with `test_util.use_gpu()`.
  - Checks that variables are placed on `/device:GPU:0` when GPU is available.
  - After one step, asserts:
    - `m` ≈ `0.06794526153774846`
    - `v` ≈ `0.00484`
    - variable `v` ≈ `0.03205473846225154`
    - exactly 3 variables (m, v, and the variable).
  - Runs the same graph on CPU (`test_util.force_cpu()`), checks `/device:CPU:0`.
  - Asserts CPU results equal GPU results.
- `__main__`: disables eager execution and runs `tf.test.main()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-optimizer/src` (tests).
- Rust public API surface: `RmspropOptimizer` update step and slot access.
- Data model mapping: slot tensors accessible and comparable.
- Feature gating: GPU/CPU parity checks only if backend supports both.
- Integration points: optimizer correctness tests.

**Implementation Steps (Detailed)**
1. Reproduce one-step update with the same hyperparameters.
2. Assert slot and variable values match Python within tolerance.
3. If GPU backend exists, ensure CPU/GPU parity.

**Tests (Detailed)**
- Python tests: `RmspropTest` in this file.
- Rust tests: add `rmsprop_basic_update` and optional CPU/GPU parity test.
- Cross-language parity test: compare numeric results for the same input.

**Gaps / Notes**
- GPU/CPU parity is asserted; Rust must match this if both backends available.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/optimizers/rmspropv2_test.py`
<a id="monolith-native-training-optimizers-rmspropv2-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 112
- Purpose/role: Tests for `RmspropOptimizer` with `use_v2=True` (alternate update rule).
- Key symbols/classes/functions: `RmspropTest`, `build_graph`.
- External dependencies: TensorFlow, `tensorflow.python.framework.test_util`, `monolith.native_training.optimizers.rmsprop`.
- Side effects: Uses GPU if available and compares CPU/GPU results.

**Required Behavior (Detailed)**
- `build_graph()`:
  - Creates variable `v=[0.1]`, loss `0.12 * v`.
  - Optimizer: `RmspropOptimizer(learning_rate=0.1, weight_decay=1, beta1=0.9, beta2=0.9, epsilon=0.1, use_v2=True)`.
  - Returns `opt.minimize(loss)`.
- `testBasic`:
  - Runs training on GPU (if available), checks `/device:GPU:0` placement.
  - After one step, asserts:
    - `m` ≈ `0.068750`
    - `v` ≈ `0.0484`
    - variable `v` ≈ `0.031250`
    - exactly 3 variables (m, v, variable).
  - Runs on CPU and asserts results equal GPU.
- `testWeightDecay`:
  - Duplicates `testBasic` (same graph and expectations).
- `__main__`: disables eager execution and runs `tf.test.main()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-optimizer/src` (tests).
- Rust public API surface: `RmspropOptimizer` with `use_v2` flag.
- Data model mapping: slot tensors accessible and comparable.
- Feature gating: GPU/CPU parity checks conditional on backend support.
- Integration points: optimizer correctness tests.

**Implementation Steps (Detailed)**
1. Reproduce one-step update with `use_v2=True`.
2. Assert slot and variable values match Python within tolerance.
3. If GPU backend exists, ensure CPU/GPU parity.
4. Decide whether to keep duplicated `testWeightDecay` semantics or consolidate.

**Tests (Detailed)**
- Python tests: `RmspropTest` in this file.
- Rust tests: add `rmsprop_v2_basic_update` and optional CPU/GPU parity test.
- Cross-language parity test: compare numeric results for the same input.

**Gaps / Notes**
- `testWeightDecay` duplicates `testBasic` (same expectations).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/optimizers/shampoo.py`
<a id="monolith-native-training-optimizers-shampoo-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 207
- Purpose/role: Implements a dense-only Shampoo optimizer variant with eigen-based preconditioning, warmup blending, and AdaGrad-style normalization.
- Key symbols/classes/functions: `eigen_inverse_root`, `apply_sparse_precond`, `ShampooOptimizer`.
- External dependencies: TensorFlow v1 optimizer APIs, `tf.linalg.eigh`, `tensorflow.python.ops.state_ops`, `io_ops`.
- Side effects: Creates multiple slot variables per tensor dimension and updates them on each step.

**Required Behavior (Detailed)**
- **`eigen_inverse_root(mat, p, head, tail, damping=1e-3)`** (`@tf.function`):
  - Computes eigen-decomposition `eval, evec = tf.linalg.eigh(mat)`.
  - `alpha = -1.0 / p`, `dim = mat.shape[0]`.
  - `non_zero = tf.where(eval > damping)`.
  - `zeros` is:
    - `min(non_zero)` if non_zero not empty,
    - else `0`.
  - `eval_p = pow(max(eval, damping), alpha)`.
  - Head/tail selection adjustments:
    - If `head + tail > dim`: set `zeros = 0`, `head = dim`, `tail = 0`.
    - Else if `zeros + head + tail > dim`: set `zeros = dim - head - tail`.
  - Selects eigenvalues/vectors:
    - `eval_ht = concat(eval_p[zeros:zeros+head], eval_p[dim-tail:])`
    - `evec_ht = concat(evec[:, zeros:zeros+head], evec[:, dim-tail:], axis=1)`
  - `offset`:
    - `0.0` if `zeros + head + tail == dim`,
    - else `mean(eval[zeros+head:dim-tail])`.
  - Returns `(evec_ht, eval_ht - offset, offset)`.
- **`apply_sparse_precond(tensor, pvec, pval, offset)`**
  - Applies preconditioner using `tensordot`:
    - `tensor_tmp_1 = tensordot(tensor, pvec, axes=[[0],[0]])`
    - `tensor_tmp_2 = tensor_tmp_1 * pval`
    - `tensor_tmp_3 = tensordot(tensor_tmp_2, pvec, axes=[[-1],[-1]])`
  - Computes `tensor_transpose = transpose(tensor, perm=[1..rank-1,0])`.
  - Returns `tensor_tmp_3 + tensor_transpose * offset`.
- **`ShampooOptimizer(tf.compat.v1.train.Optimizer)`**
  - `__init__(learning_rate=0.03, beta_1=0.9, beta_2=1.0, warmup=5000, tau_1=200, tau_2=20, eigen_head=100, eigen_tail=100, damping_epsilon=1e-3, use_locking=False, name="Shampoo", **kwargs)`:
    - Stores parameters; passes `**kwargs` to base optimizer.
  - `_create_slots(var_list)`:
    - For each variable and each dimension `i`:
      - Creates `s{i}` and `g{i}` slots of shape `[dim, dim]`.
      - Computes `eigens = min(dim, eigen_head + eigen_tail)` and creates:
        - `pvec{i}` `[dim, eigens]`
        - `pval{i}` `[eigens]`
        - `o{i}` scalar.
    - Creates zero slots `d`, `m`, `pm` for each variable.
  - `_resource_apply_dense(grad, var)`:
    - Computes:
      - `global_step = tf.cast(tf.train.get_global_step(), int32)`.
      - `if_update_stat = (global_step % tau_2 == 0)`.
      - `if_warmed_up = global_step > warmup`.
      - `if_update_precond = if_warmed_up AND (global_step % tau_1 == 0)`.
      - `warmup_rate = clamp(global_step / warmup - 1, 0, 1)`.
      - `if_stat_momentum = beta_2 < 1.0 - 1e-10`.
    - For each dimension `i`:
      - `g_t`: if_update_stat → assign `tensordot(grad, grad, axes=[axes, axes])`, else identity.
      - `s_t`: if_stat_momentum → `s = beta_2*s + (1-beta_2)*g_t`, else `s += g_t`.
      - `pvec/pval/offset`: if_update_precond → compute from `eigen_inverse_root(s_t, 2*rank, ...)` and assign; else identity.
      - Updates `grad_precond = apply_sparse_precond(grad_precond, pvec_t, pval_t, offset_t)`.
    - Accumulates `d_t = d + grad*grad`.
    - `m_t = beta_1*m + (1-beta_1)*grad*rsqrt(d_t + 1e-30)`.
    - `pm_t = beta_1*pm + (1-beta_1)*grad_precond`.
    - `update_diag = lr * m_t`.
    - `update_second = lr * norm(m_t) / (norm(pm_t) + 1e-10) * pm_t`.
    - `var_t`:
      - If warmed up: `var -= (1-warmup_rate)*update_diag + warmup_rate*update_second`.
      - Else: `var -= update_diag`.
    - Returns `tf.group(*ops)` of all state updates.
  - `_resource_apply_sparse(grad, var)`:
    - `raise tf.no_op()` (note: this raises an op, likely unintended but must match).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-optimizer/src`.
- Rust public API surface:
  - `ShampooOptimizer` with identical hyperparameters.
  - Helper functions for eigen inverse root and preconditioning.
- Data model mapping:
  - Slot tensors for each dimension (`s`, `g`, `pvec`, `pval`, `o`, `d`, `m`, `pm`).
  - Requires eigen decomposition and matrix operations.
- Feature gating:
  - This optimizer is heavy; consider optional feature or backend requirement (eigen decomposition support).
- Integration points:
  - Training loop should only enable for dense tensors.

**Implementation Steps (Detailed)**
1. Implement `eigen_inverse_root` and `apply_sparse_precond` with identical axis semantics.
2. Implement slot creation per dimension and scalar slots `d/m/pm`.
3. Mirror update scheduling via `tau_1`, `tau_2`, `warmup`, and global step.
4. Implement warmup blending and normalization exactly.
5. Decide handling of sparse gradients (match Python error).
6. Add tests that validate slot shapes and a small numeric step.

**Tests (Detailed)**
- Python tests: none in this repo for Shampoo.
- Rust tests: add shape/slot tests and a small-step numeric sanity check.
- Cross-language parity test: optional, requires TF reference run.

**Gaps / Notes**
- `_resource_apply_sparse` raises an op (`tf.no_op()`), which likely throws; Rust should match behavior or document deviation.
- Eigen decomposition and slicing logic is subtle; deterministic ordering must be preserved.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/prefetch_queue.py`
<a id="monolith-native-training-prefetch-queue-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 379
- Purpose/role: Implements queue-based prefetching and async execution helpers that support mixed CPU/GPU tensors and nested structures.
- Key symbols/classes/functions: `_GPUCompatiblePaddingFIFOQueue`, `_FIFOQueue`, `_MultiFIFOQueue`, `MultiQueueRunner`, `EnqueueHook`, `enqueue_dicts_with_queue_return`, `AsyncPushHook`, `AsyncFunctionMgr`.
- External dependencies: TensorFlow queue APIs (`data_flow_ops`, `gen_data_flow_ops`), `nested_tensors`, `utils.check_ops_dependence`, `absl.logging`.
- Side effects:
  - Creates TF queue resources (CPU and/or GPU).
  - Starts QueueRunner threads via hooks.
  - Adds control dependencies to enforce async op order.

**Required Behavior (Detailed)**
- **`_GPUCompatiblePaddingFIFOQueue`** (QueueBase):
  - Wraps `padding_fifo_queue_v2` but allows placement on CPU or GPU.
  - Validates `dtypes`/`shapes` length; raises `ValueError` if mismatch.
  - `enqueue_many` / `dequeue_many` are **not supported** and raise `NotImplementedError`.
- **`_FIFOQueue(dense_list, capacity=2, queue_name="prefetch_queue")`**:
  - `dense_list` must be a list; raises:
    - `ValueError` if `dense_list` is `None`,
    - `TypeError` if not a list.
  - Creates `_GPUCompatiblePaddingFIFOQueue` inside `tf.init_scope()`.
  - `enqueue_op = queue.enqueue(flatten_tensor_list)`.
  - `dequeue()` returns list of tensors; if single element, returns `[tensor]`.
- **`_MultiFIFOQueue(dense_list, capacity=2, queue_name="prefetch_queue")`**:
  - Splits tensors by device (`"GPU"` substring in `tensor.device`).
  - Always creates CPU queue; creates GPU queue only if GPU tensors exist.
  - `enqueue_op` is `tf.group` of all queue `enqueue_op`s.
  - `queue` property:
    - If only one queue, returns it.
    - If multiple queues, raises `NotImplementedError`.
  - `dequeue()`:
    - If one queue, returns its dequeue.
    - Else dequeues all queues and merges tensors back to original order using saved indices.
  - `size()`:
    - If multiple queues, returns CPU queue size (assumes synchronized enqueue/dequeue).
  - `queues` property returns list of queue resources.
- **`MultiQueueRunner`**:
  - `_init_from_args` accepts `queue` as list:
    - Builds grouped `close_op` and `cancel_op`.
  - `queue` property raises `NotImplementedError` when multi-queue.
  - `name` property returns first queue name in multi-queue case.
- **`EnqueueHook(q)`**:
  - Wraps `MultiQueueRunner(q.queues, [q.enqueue_op])`.
  - Starts threads in `after_create_session`.
- **`enqueue_dicts_with_queue_return(tensors, capacity=1, queue_name="prefetch_queue")`**
  - If `capacity == 0`, returns `(tensors, None)` with no queue.
  - Flattens nested tensors via `NestedTensors`.
  - Creates `_MultiFIFOQueue` with flattened tensors.
  - Dequeues in `tf.init_scope()` and rebuilds original nested structure.
  - Returns `(nested_result, queue)`.
- **`AsyncPushHook(queue, ops)`**:
  - `begin`: stores `queue.size()`.
  - `before_run`: returns `SessionRunArgs(run_ops)` only after queue initialized.
  - `after_run`: sets `_queue_init` once queue size > 0.
  - `end`: drains queue by running `run_ops` while `queue_size > 0`.
- **`AsyncFunctionMgr(is_async=True)`**:
  - `add_async_function(target, args=None, kwargs=None, is_async=None, queue_name="async_queue")`:
    - If `is_async` is False, runs `target(*args, **kwargs)` synchronously.
    - If async:
      - Appends dummy constant tensor to `args` to avoid empty input lists.
      - Enqueues `(args, kwargs)` via `enqueue_dicts_with_queue_return`.
      - Builds `run_ops = target(*args[:-1], **kwargs)` under control deps on dummy tensor and dummy op.
      - Adds `AsyncPushHook(queue, run_ops)`.
      - Calls `utils.check_ops_dependence(queue.enqueue_op.name, dummy_op.name)` to validate dependencies.
      - Returns `queue.enqueue_op`.
  - `hooks` property returns list of hooks.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` (queue + async helpers).
- Rust public API surface:
  - Queue wrapper that supports nested tensors and optional CPU/GPU split.
  - Async function manager that enqueues inputs and runs ops via hooks.
- Data model mapping:
  - Preserve nested tensor structure using Rust equivalent of `NestedTensors`.
  - Separate queues per device; merge outputs in original order.
- Feature gating:
  - GPU queue support depends on backend; may be optional.
- Integration points:
  - Used by `NativeContext.add_async_function` and estimator hooks.

**Implementation Steps (Detailed)**
1. Implement a queue abstraction with padding and fixed shapes.
2. Implement split/merge logic for CPU/GPU tensors.
3. Add EnqueueHook equivalents to manage background threads.
4. Port `enqueue_dicts_with_queue_return` to flatten nested structures and rebuild.
5. Implement AsyncFunctionMgr with dummy tensor injection and dependency checks.
6. Add tests for nested structures, capacity 0, and async behavior.

**Tests (Detailed)**
- Python tests: `monolith/native_training/prefetch_queue_test.py`.
- Rust tests: add unit tests for queue behavior, nesting, async manager, and control-dependency handling.
- Cross-language parity test: compare nested structures and async side effects.

**Gaps / Notes**
- `_MultiFIFOQueue` assumes CPU/GPU queues are dequeued together; this is noted as a limitation in comments.
- Device detection uses `"GPU" in tensor.device` string matching.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/prefetch_queue_test.py`
<a id="monolith-native-training-prefetch-queue-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 305
- Purpose/role: Tests for queue helpers, nested tensor enqueue/dequeue, and async function manager.
- Key symbols/classes/functions: `GPUCompatiblePaddingFIFOQueueTests`, `FIFOQueueTest`, `PrefetchTest`, `AsyncManagerTest`.
- External dependencies: TensorFlow, `tensorflow.python.framework.test_util`, `nested_tensors`, `prefetch_queue`.
- Side effects: Uses GPU if available; runs MonitoredSessions.

**Required Behavior (Detailed)**
- **GPUCompatiblePaddingFIFOQueueTests**
  - `testEnqueueAndDequeue`:
    - Enqueues three float scalars on GPU; dequeues and validates device placement and arithmetic results.
  - `testGPUQueueCPUTensor`:
    - Creates CPU tensors, enqueues to GPU queue, dequeues on CPU; validates results.
  - `testMultiEnqueueAndDequeue`:
    - Enqueues tuple `(int32, float32)` and checks values in order.
  - `testIdentityHelper`:
    - Ensures `tf.identity` on GPU and queue enqueue/dequeue work; checks value `2`.
- **FIFOQueueTest**
  - `test_fifo_queue_data`:
    - Enqueues dense + ragged tensors; verifies round-trip values.
  - `test_fifo_queue_capacity`:
    - Enqueues/dequeues 4 items with capacity 4; validates values.
- **PrefetchTest**
  - **NOTE**: There are two methods named `test_enqueue_dicts_with_queue_return`; the second overrides the first, so only the second runs.
  - First (shadowed) version:
    - Enqueues dense/ragged dicts with capacity 3, validates output across multiple enqueue/dequeue cycles.
  - Second (effective) version:
    - Enqueues nested dicts containing tensors, strings, and `None`.
    - Asserts string and None entries preserved before session run, then runs queue and validates tensor values.
  - `test_enqueue_dicts_with_control_flow`:
    - Uses control dependency (`v.assign_add(1)`) and ensures it executes when enqueuing.
  - `test_enqueue_with_zero_capacity`:
    - `capacity=0` returns original tensors; validates values.
  - `test_estimator_prefetch`:
    - Uses Estimator `predict` with enqueue hook; verifies predicted values 0..19.
- **AsyncManagerTest**
  - `testBasic`: async adds once even after two enqueue runs (value = 1).
  - `testSync`: synchronous add yields value = 1 after one run.
  - `testEmptyInput`: async function with no args still works (value = 1 after two runs).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` (tests).
- Rust public API surface: queue helpers, nested enqueue/dequeue, async manager hooks.
- Data model mapping: ragged tensor support and CPU/GPU queueing if backend supports it.
- Feature gating: GPU-specific tests conditional on backend support.
- Integration points: estimator-style predict loops or equivalent.

**Implementation Steps (Detailed)**
1. Port queue tests for enqueue/dequeue ordering and device placement.
2. Add nested tensor round-trip tests (dense + ragged).
3. Add tests for `capacity=0` passthrough.
4. Add async function manager tests for async vs sync behavior.
5. Document/handle duplicate test name if porting to Rust.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: mirror test classes where possible; use deterministic inputs.
- Cross-language parity test: compare nested results and async side effects.

**Gaps / Notes**
- Duplicate method name in `PrefetchTest` hides the first test; only the second runs in Python.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/ps_benchmark.py`
<a id="monolith-native-training-ps-benchmark-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 273
- Purpose/role: Implements a parameter-server throughput benchmark task that measures PS performance and selects top PS nodes.
- Key symbols/classes/functions: `BenchmarkConfig`, `_BenchmarkWorkerHook`, `_DummyCheckpointSaverHook`, `PsBenchMarkTask`.
- External dependencies: TensorFlow, `native_task.NativeTask`, `utils.ps_device`, `cpu_training` (tests), `logging_ops`, `service_discovery` (imported, unused).
- Side effects:
  - Mutates `BenchmarkConfig.ps_list` after benchmark completion.
  - Writes TF variables and metrics; uses session hooks.

**Required Behavior (Detailed)**
- **`BenchmarkConfig` dataclass**
  - Fields:
    - `ps_list: List`
    - `num_ps_required: int`
    - `num_workers: int`
    - `index: int`
    - `benchmark_secs: float = 60.0`
    - `ps_str_overridden: str = ""` (comma-separated override list; skips benchmark).
- **`_BenchmarkWorkerHook(SessionRunHook)`**
  - Creates variables in scope `_SCOPE_NAME`:
    - `_result` (string), `_ready` (bool list), `_done` (bool list),
      `_make_ready`/`_make_done` assigns, `_result_placeholder` + `_result_assign`.
  - `after_create_session`:
    - Marks self ready; waits until `sum(ready) >= int(num_workers * 0.9)`.
    - Sleeps 1 second, records `_start_time`.
  - `before_run`:
    - If `ps_str_overridden` set, assigns `_result` to override value.
    - Reads `_result`; if non-empty, raises `tf.errors.OutOfRangeError` to stop.
  - `after_run`:
    - Logs duration and requests stop.
  - `end`:
    - Marks done; waits until all workers done (timeout 10s).
    - If `index == 0` and no override:
      - Reads `throughput_tensor` (mean tensor metric).
      - Sorts PS nodes by throughput.
      - Logs per-PS throughput and then adjusts throughput by IP (sums for same IP).
      - Selects top `num_ps_required` PS entries and writes comma-separated string to `_result`.
    - Waits until `_result` is non-empty, then replaces `bm_config.ps_list` with selected list.
  - `_wait(cond, timeout=3600)` polls condition every 0.5s.
- **`_DummyCheckpointSaverHook`**
  - Extends `CheckpointSaverHook`, but overrides lifecycle methods to no-op.
  - Default `checkpoint_dir` is `$HOME/tmp` if not provided.
  - `_save` returns False (prevents saving).
- **`PsBenchMarkTask(NativeTask)`**
  - `params()` adds `bm_config`.
  - `create_input_fn` returns dataset of constant vector `[0.12, 0.23, 0.34, 0.45]` repeating and prefetching.
  - `create_model_fn`:
    - For each PS in `bm_config.ps_list`, creates a 256×256 variable on that PS.
    - Builds a while loop that performs heavy math (splits/concat/sqrt) until `benchmark_secs` elapsed.
    - Computes throughput `j / (ts_now - ts_before)` per PS.
    - Uses `tf.compat.v1.metrics.mean_tensor(tf.stack(throughputs))`.
    - Adds `_BenchmarkWorkerHook` and `_DummyCheckpointSaverHook`.
    - `PREDICT` returns predictions `0.0`.
    - `TRAIN/EVAL` returns EstimatorSpec with loss 0 and train_op group of metric update + global step increment.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` (benchmark task).
- Rust public API surface:
  - `BenchmarkConfig` struct.
  - Benchmark task or command that measures PS throughput.
- Data model mapping:
  - Use equivalent session hooks / callbacks for multi-worker coordination.
- Feature gating:
  - PS benchmark likely only for TF runtime backend; may be optional.
- Integration points:
  - PS device placement logic (`ps_device`) and distributed training orchestration.

**Implementation Steps (Detailed)**
1. Port `BenchmarkConfig` and its semantics (override skipping).
2. Implement a benchmark task that measures throughput per PS (or stub with TF backend only).
3. Implement worker coordination and result selection logic (including IP aggregation).
4. Ensure `ps_list` is mutated to selected subset after completion.
5. Add tests mirroring Python behavior for override and selection count.

**Tests (Detailed)**
- Python tests: `monolith/native_training/ps_benchmark_test.py`.
- Rust tests: add tests for override behavior and list truncation.
- Cross-language parity test: compare selected PS list with identical throughput inputs (if TF backend available).

**Gaps / Notes**
- Imports `logging_ops` and `service_discovery` but does not use them in this file.
- Benchmark uses heavy TF ops; not suitable for pure Candle backend without substantial rework.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/ps_benchmark_test.py`
<a id="monolith-native-training-ps-benchmark-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 57
- Purpose/role: Tests `PsBenchMarkTask` behavior in local CPU training.
- Key symbols/classes/functions: `PsBenchmarkTest`.
- External dependencies: TensorFlow, `cpu_training.local_train`, `utils.get_test_tmp_dir`.
- Side effects: Runs local training which mutates `bm_config.ps_list`.

**Required Behavior (Detailed)**
- `testBasic`:
  - Creates `BenchmarkConfig` with `ps_list=["ps0","ps1"]`, `num_ps_required=1`, `num_workers=1`, `index=0`, `benchmark_secs=1.0`.
  - Calls `cpu_training.local_train` with `num_ps=2`.
  - Asserts `len(p.bm_config.ps_list) == 1` after run.
- `testSkipBenchmark`:
  - Same config but sets `ps_str_overridden="overridden"`.
  - After training, expects `p.bm_config.ps_list[0] == "overridden"`.
- `__main__`: disables eager execution and runs test via `absl.app`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` (tests).
- Rust public API surface: PS benchmark task and local training harness.
- Data model mapping: `BenchmarkConfig` with override string.
- Feature gating: likely TF runtime only.
- Integration points: local training driver for PS benchmark.

**Implementation Steps (Detailed)**
1. Provide a local training harness for benchmark task (or mock).
2. Ensure `ps_list` is reduced to `num_ps_required` entries.
3. Ensure override string bypasses benchmark.

**Tests (Detailed)**
- Python tests: `PsBenchmarkTest` in this file.
- Rust tests: add unit tests for selection and override behavior.
- Cross-language parity test: compare resulting `ps_list` for identical config.

**Gaps / Notes**
- Test relies on `cpu_training.local_train` which is not yet mapped in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/ragged_utils.py`
<a id="monolith-native-training-ragged-utils-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 29
- Purpose/role: Provides a faster cached alternative to `RaggedTensor.value_rowids()` using a custom op.
- Key symbols/classes/functions: `fused_value_rowids`.
- External dependencies: TensorFlow, `gen_monolith_ops.monolith_fused_value_rowids`.
- Side effects: Caches result on the `RaggedTensor` instance via `monolith_fused_value_rowids` attribute.

**Required Behavior (Detailed)**
- **`fused_value_rowids(rt)`**
  - Validates `rt` is a `tf.RaggedTensor`; otherwise raises `ValueError("rt must be RaggedTensor")`.
  - If `rt` lacks attribute `monolith_fused_value_rowids`, computes it once via:
    - `ops.monolith_fused_value_rowids(rt.row_splits)`.
  - Returns the cached tensor (same object on subsequent calls).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tensor/src`.
- Rust public API surface: `fused_value_rowids(ragged)` helper.
- Data model mapping:
  - Ragged tensor must expose `row_splits`.
  - Cache results on ragged tensor wrapper if possible.
- Feature gating: requires custom op or Rust implementation of row-id computation.
- Integration points: used in ragged pipelines and feature preprocessing.

**Implementation Steps (Detailed)**
1. Implement row-id computation or bind to TF custom op when TF backend enabled.
2. Add caching on ragged tensor wrapper to avoid recomputation.
3. Preserve error message for non-ragged input.

**Tests (Detailed)**
- Python tests: `monolith/native_training/ragged_utils_test.py`.
- Rust tests: add caching test and expected row-ids for a sample ragged tensor.
- Cross-language parity test: compare value_rowids output and object identity if applicable.

**Gaps / Notes**
- Attribute-based caching mutates the ragged tensor object; Rust should use a wrapper or external cache.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/ragged_utils_test.py`
<a id="monolith-native-training-ragged-utils-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 32
- Purpose/role: Tests `fused_value_rowids` caching and output correctness.
- Key symbols/classes/functions: `RaggedUtilsTestCase`.
- External dependencies: TensorFlow, `ragged_utils`.
- Side effects: Disables eager execution in `__main__`.

**Required Behavior (Detailed)**
- `test_basic`:
  - Creates `rt = tf.ragged.constant([[], [1], [2, 3]])`.
  - Calls `fused_value_rowids` twice; asserts returned objects are identical (`is`).
  - Asserts values equal `[1, 2, 2]`.
- `__main__`: disables eager execution and runs `tf.test.main()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tensor/src` (tests).
- Rust public API surface: `fused_value_rowids` and caching behavior.
- Data model mapping: ragged tensor representation with `row_splits`.
- Feature gating: custom op or Rust implementation required.
- Integration points: ragged pipelines using row-id mapping.

**Implementation Steps (Detailed)**
1. Add a test that calls `fused_value_rowids` twice and checks cache hit semantics (if applicable).
2. Verify output row-ids for a sample ragged tensor.

**Tests (Detailed)**
- Python tests: `RaggedUtilsTestCase` in this file.
- Rust tests: add `ragged_utils_basic` test.
- Cross-language parity test: compare row-id outputs for identical ragged input.

**Gaps / Notes**
- Python relies on object identity for caching; Rust may need explicit cache handles.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/remote_predict_ops.py`
<a id="monolith-native-training-remote-predict-ops-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 0
- Purpose/role: Empty placeholder module.
- Key symbols/classes/functions: none.
- External dependencies: none.
- Side effects: none.

**Required Behavior (Detailed)**
- This module is empty; importing it should have no side effects.

**Rust Mapping (Detailed)**
- Target crate/module: none required.
- Rust public API surface: none.
- Data model mapping: none.
- Feature gating: none.
- Integration points: none.

**Implementation Steps (Detailed)**
1. No implementation needed unless future Python adds content.

**Tests (Detailed)**
- Python tests: none.
- Rust tests: none.
- Cross-language parity test: not applicable.

**Gaps / Notes**
- Keep stub to preserve import paths if referenced elsewhere.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/restore_test.py`
<a id="monolith-native-training-restore-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 240
- Purpose/role: Integration tests for partial and full checkpoint restore across parameter servers (with and without PS monitor).
- Key symbols/classes/functions: `_generate_config`, `_get_id_tensor`, `PartialRestoreTest`.
- External dependencies: TensorFlow distributed server APIs, `basic_restore_hook`, `hash_table_ops`, `save_utils`, `utils`.
- Side effects: Creates local TF servers, writes checkpoints to `TEST_TMPDIR`.

**Required Behavior (Detailed)**
- **`_generate_config(servers, job_name=utils.PS_JOB_NAME)`**
  - Builds a `ClusterDef` with one job and tasks derived from `server.target` (strip `grpc://`).
  - Returns `ConfigProto` with `experimental.share_session_state_in_clusterspec_propagation = True`.
- **`_get_id_tensor(x)`** returns `tf.constant(x, dtype=tf.int64)`.
- **`PartialRestoreTest.build_graph()`**
  - On `ps_device(0)`:
    - Creates `global_step`, increments, `v0`, `op0`, hash table 0, assign_add, lookup.
  - On `ps_device(1)`:
    - Creates `v1`, `op1`, hash table 1, assign_add, lookup.
  - Returns `(train_op, v0, v1, lookup0, lookup1)` where `train_op` groups ops.
- **`test_restore_with_ps_monitor`**
  - Uses `PartialRecoverySaver` with `PsMonitor(2)` and `NoFirstSaveCheckpointSaverHook`.
  - Uses `HashTableCheckpointSaverListener` and `HashTableCheckpointRestorerListener`.
  - Creates two local servers, saves checkpoint after one step.
  - Runs a second session without restore hooks to mutate variables.
  - Creates new servers and restores full checkpoint; asserts values back to 1.
  - Creates a cluster with only one of the original PS servers and restores partially; asserts v0/lookup0 restored to 2 while v1/lookup1 restored to 1.
- **`test_restore_without_ps_monitor`**
  - Same as above but without `PsMonitor`:
    - Save checkpoint after one step.
    - Mutate values in a second session.
    - Restore all variables in a third session; assert values back to 1.
- `__main__`: disables eager execution and runs `tf.test.main()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` (checkpoint restore tests).
- Rust public API surface: restore hooks, saver utilities, hash table checkpoint listeners.
- Data model mapping: distributed cluster config and PS device placement.
- Feature gating: TF runtime backend required.
- Integration points: `save_utils` and `hash_table_ops` parity.

**Implementation Steps (Detailed)**
1. Provide Rust equivalents for `PartialRecoverySaver`, `PsMonitor`, and restore hooks.
2. Implement hash table checkpoint save/restore listeners for embedding tables.
3. Build a local distributed test harness that supports multiple PS servers.
4. Port both tests: full restore and partial restore with PS monitor.

**Tests (Detailed)**
- Python tests: `PartialRestoreTest` in this file.
- Rust tests: add integration tests for checkpoint restore paths.
- Cross-language parity test: compare restored values across identical checkpoints.

**Gaps / Notes**
- Heavy reliance on TF distributed runtime and custom hash table ops.
- Partial restore logic depends on PS monitor and server set differences.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/runner_utils.py`
<a id="monolith-native-training-runner-utils-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 396
- Purpose/role: Runner configuration, checkpoint override logic, and service discovery helpers for distributed training.
- Key symbols/classes/functions: `isabs`, `gen_get_checkpoint_state`, `ContainerType`, `RunnerConfig`, `get_discovery`, `monolith_discovery`.
- External dependencies: TensorFlow checkpoint APIs, `gflags_utils`, service discovery classes, `save_utils`, `mlp_utils`, protobuf `text_format`.
- Side effects:
  - Monkey-patches `os.path.isabs`, `checkpoint_management.get_checkpoint_state`, and `tf.train.get_checkpoint_state`.
  - Writes checkpoint and monolith checkpoint metadata files to `model_dir`.
  - Updates global `FLAGS` for kafka settings and restore checkpoints.

**Required Behavior (Detailed)**
- **`isabs(path)`**:
  - Returns True for `hdfs:/` prefix; otherwise uses original `os.path.isabs`.
  - Assigned to `os.path.isabs` at import time.
- **`gen_get_checkpoint_state()`**:
  - Wraps original `checkpoint_management.get_checkpoint_state` with retry and restore override logic.
  - Retries up to 5 times when checkpoint file exists but state is `None`; raises `Exception("read ckpt error!")` if exceeded.
  - If `FLAGS.restore_ckpt` set and `latest_filename == "checkpoint"`:
    - Builds `restore_ckpt` path using checkpoint state directory + basename of `FLAGS.restore_ckpt`.
    - If `restore_ckpt` exists in `all_model_checkpoint_paths`:
      - TRAIN: uses `restore_ckpt` only if `restore_ckpt` marker file does not exist.
      - Non-TRAIN: always uses `restore_ckpt`.
    - Writes `restore_ckpt` marker and updates checkpoint file when applying restore.
  - Logs warnings for missing/identical restore checkpoints.
  - Catches `UnparsedFlagAccessError` and logs other exceptions with traceback.
  - Assigned to `checkpoint_management.get_checkpoint_state` and `tf.train.get_checkpoint_state`.
- **`ContainerType`** enum: `DOCKER=1`, `NATIVE=2`.
- **`RunnerConfig(DistributedCpuTrainingConfig)`**:
  - Large dataclass with training/runtime flags (see source for full list).
  - `__post_init__`:
    - Calls `mlp_pass()` and `add_mpi_exception_hook()`.
    - Updates via `gflags_utils.update(self)` (logs on failure).
    - For GPU partial sync training: sets `index` from `OMPI_COMM_WORLD_RANK` when worker index unset.
    - Propagates kafka settings into global `FLAGS`.
    - Asserts `zk_watch_address_family` in {IPV4, IPV6}.
    - Sets `FLAGS.restore_ckpt` if unset.
    - If `restore_dir` set:
      - Chief (local or worker 0) runs `_copy_ckpt_file`.
      - Others wait for `monolith_checkpoint` file to appear (poll every 30s).
  - **`_copy_ckpt_file()`**:
    - Reads `checkpoint` from `restore_dir`.
    - Writes `checkpoint` file into `model_dir` if not present.
    - Writes `restore_ckpt` marker file.
    - Updates/creates `monolith_checkpoint` with restore paths added to `exempt_model_checkpoint_paths`.
    - Logs warnings when restore checkpoint missing or invalid.
- **`get_discovery(runner_conf, psm=None)`**:
  - Local → `None`.
  - PRIMUS → `TfConfigServiceDiscovery` from `tf_config` JSON; sets `server_type` and `index`.
  - CONSUL → `ConsulServiceDiscovery(psm)`.
  - MLP → `MLPServiceDiscovery()`.
  - Else → `ZKServiceDiscovery(deep_insight_name, zk_server)`.
- **`monolith_discovery(runner_conf)`**:
  - Context manager; for non-local creates `psm` via `env_utils.generate_psm_from_uuid(runner_conf.uuid)` and yields discovery.
  - Ensures `discovery.close()` on exit; logs enter/exit.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src`.
- Rust public API surface:
  - `RunnerConfig` struct with post-init behavior.
  - Checkpoint state wrapper for restore override logic.
  - Service discovery factory and context guard.
- Data model mapping:
  - CheckpointState + MonolithCheckpointState handling.
  - HDFS path handling in `isabs`.
- Feature gating:
  - TF checkpoint patching only under TF runtime backend.
  - Service discovery requires Consul/ZK/MLP client support.
- Integration points:
  - Training entrypoints, service discovery, and checkpoint restore flows.

**Implementation Steps (Detailed)**
1. Implement path `isabs` override for `hdfs:/`.
2. Implement checkpoint state override logic and restore marker handling.
3. Port `RunnerConfig` fields and `__post_init__` behavior.
4. Implement `_copy_ckpt_file` logic for checkpoint + monolith checkpoint.
5. Add service discovery factory and context manager.
6. Add unit tests for discovery selection and checkpoint copy behavior.

**Tests (Detailed)**
- Python tests: `monolith/native_training/runner_utils_test.py`.
- Rust tests: add unit tests for `get_discovery` and `_copy_ckpt_file` logic.
- Cross-language parity test: compare checkpoint files and discovery types.

**Gaps / Notes**
- Monkey-patching `tf.train.get_checkpoint_state` is process-global; Rust must replicate or document differences.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/runner_utils_test.py`
<a id="monolith-native-training-runner-utils-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 108
- Purpose/role: Tests service discovery selection and checkpoint copy logic in `RunnerConfig`.
- Key symbols/classes/functions: `RunnerUtilsTest`.
- External dependencies: TensorFlow, `CheckpointState`, `KazooTimeoutError`, `RunnerConfig`, `get_discovery`.
- Side effects: Writes checkpoint files under `TEST_TMPDIR`.

**Required Behavior (Detailed)**
- `test_get_discovery_local`:
  - `RunnerConfig(is_local=True)` → `get_discovery` returns `None`.
  - After toggling `is_local=False`, still uses previous return (no assertion change).
- `test_get_discovery_primus`:
  - Builds `tf_config` JSON with ps/worker/chief.
  - Expects `get_discovery` returns `TfConfigServiceDiscovery`.
- `test_get_discovery_consul`:
  - Expects `ConsulServiceDiscovery` when `discovery_type=CONSUL` and `psm` provided.
- `test_get_discovery_zk`:
  - Attempts ZK discovery; catches `KazooTimeoutError`.
- `test_copy_ckpt`:
  - Creates `restore_dir` with a `checkpoint` file containing three checkpoints.
  - Instantiates `RunnerConfig` with `restore_dir`, `model_dir`, `restore_ckpt='model.ckpt-30'`.
  - Asserts:
    - `monolith_checkpoint` file exists in `model_dir`.
    - `restore_ckpt` marker file exists.
    - `tf.train.get_checkpoint_state(model_dir)` returns `model.ckpt-30`.
  - Instantiates a worker `RunnerConfig` (index=2) to verify non-chief path.
- `__main__`: disables eager execution and runs tests.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` (tests).
- Rust public API surface: `RunnerConfig`, `get_discovery`.
- Data model mapping: checkpoint file format and monolith checkpoint metadata.
- Feature gating: ZK/Consul discovery tests conditional on clients.
- Integration points: runner initialization and restore logic.

**Implementation Steps (Detailed)**
1. Add tests for discovery selection by `discovery_type`.
2. Add checkpoint copy test: ensure checkpoint and monolith_checkpoint files are written.
3. Add test for override `restore_ckpt` selection.

**Tests (Detailed)**
- Python tests: `RunnerUtilsTest` in this file.
- Rust tests: add discovery tests and checkpoint copy test.
- Cross-language parity test: compare checkpoint file contents and selected discovery type.

**Gaps / Notes**
- `test_get_discovery_local` mutates `is_local` after calling `get_discovery` but does not re-run discovery.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/runtime/ops/gen_monolith_ops.py`
<a id="monolith-native-training-runtime-ops-gen-monolith-ops-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 23
- Purpose/role: Loads the Monolith custom op shared library and exposes generated op wrappers.
- Key symbols/classes/functions: `gen_monolith_ops_base` imports, `tf.load_library(...)`.
- External dependencies: TensorFlow, `monolith.utils.get_libops_path`.
- Side effects: Loads shared library `libtfkernel_monolith_ops_for_load.so` at import time.

**Required Behavior (Detailed)**
- Imports all symbols from `gen_monolith_ops_base`.
- Calls `tf.load_library(utils.get_libops_path("monolith/native_training/runtime/ops/libtfkernel_monolith_ops_for_load.so"))` on import.
- This is required for custom ops used throughout native training (hash tables, optimizers, ragged utils, etc.).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/src` (TF runtime bindings).
- Rust public API surface: dynamic library loader or FFI bindings for custom ops.
- Data model mapping: expose wrappers or direct FFI calls matching generated ops.
- Feature gating: only available when TF runtime backend is enabled and custom ops library present.
- Integration points: required by optimizers, hash table ops, ragged utils, etc.

**Implementation Steps (Detailed)**
1. Provide a Rust wrapper that loads the custom op shared library at startup.
2. Ensure load is idempotent and errors are surfaced clearly.
3. Map or bind the generated op APIs used in Python.

**Tests (Detailed)**
- Python tests: none in this file; exercised by downstream tests.
- Rust tests: add a smoke test that loads the shared library when TF backend enabled.
- Cross-language parity test: verify ops are available and callable.

**Gaps / Notes**
- Import-time side effect means load failures are fatal early; Rust should handle similarly or fail fast.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/save_utils.py`
<a id="monolith-native-training-save-utils-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 1309
- Purpose/role: Checkpoint save/restore utilities with partial recovery, PS monitoring, monolith checkpoint metadata, and tide-aware saving.
- Key symbols/classes/functions: `get_latest_checkpoint_state`, `get_monolith_checkpoint_state`, `SaveHelper`, `SecondOrStepTimerWithTideSetting`, `NoFirstSaveCheckpointSaverHook`, `PsMonitor`, `SaverBuilder`, `PartialRecoverySaver`.
- External dependencies: TensorFlow checkpoint APIs, `MonolithCheckpointState`, `cli` metrics, `tide_available_now`, `CUSTOM_RESTORE_OP`, `calc_feed_dict`.
- Side effects:
  - Writes `monolith_checkpoint` file.
  - Emits metrics via `cli`.
  - Uses custom restore hooks and may write `clear_nn` flag file.

**Required Behavior (Detailed)**
- **`get_latest_checkpoint_state(checkpoint_dir, global_step_value)`**
  - Caches checkpoint state per directory in `_ckpt_state_cache_map`.
  - Refreshes cache when `global_step_value` increases or cached state is None.
- **`get_monolith_checkpoint_state(checkpoint_dir, filename=None, remove_invalid_path=False)`**
  - Reads `monolith_checkpoint` (or `filename`) and parses `MonolithCheckpointState`.
  - If `remove_invalid_path`:
    - Converts relative paths to absolute by prepending `checkpoint_dir`.
    - Removes paths that do not exist on filesystem.
  - Returns None on `OpError` or `ParseError` with warnings.
- **`SaveHelper`**
  - `get_ckpt_prefix(basename, step)` → `"basename-step"`.
  - `get_ckpt_asset_dir(ckpt_prefix)` → `"ckpt_prefix.assets/"`.
  - `get_global_step_value(ckpt_prefix)` → parse suffix after `-`.
  - `get_existing_checkpoint_steps()` uses `tf.train.get_checkpoint_state` to list steps.
- **`SecondOrStepTimerWithTideSetting`**
  - Extends `SecondOrStepTimer` with tide availability checks.
  - If tide unavailable (via `tide_available_now`), uses `_tide_every_secs` instead of `every_secs`.
  - `enable()/disable()` toggles trigger behavior.
- **`NoFirstSaveCheckpointSaverHook`**
  - Wraps `CheckpointSaverHook` with:
    - Optional tide settings (`tide_*`).
    - `no_first_save` (skip initial save).
    - `_is_dense_only`, `_use_native_multi_hash_table`, `_guard_saver_listeners`.
  - `after_create_session`:
    - Exports meta graph if requested.
    - For `PartialRecoverySaver`, calls `setup_ps_initialized_state`.
    - Creates monolith checkpoint state file if needed.
  - `trigger_save(session, ignore_save_errors=False)`:
    - Resets timer and invokes save logic under lock.
  - `_save(session, step)`:
    - Skips first save when `no_first_save`.
    - Skips dense-only save if same step as last.
    - Calls guard listeners even on save failures.
    - Retries save on `OpError` up to 2 times; optionally ignores errors.
    - Emits `save_checkpoint` metrics and timing.
  - `_create_or_update_monolith_ckpt_state(do_update)`:
    - Writes `monolith_checkpoint` with hash table type and timestamp.
  - `end(session)`:
    - Forces save if dense-only or model dump mode flags set.
- **`PsMonitor`**
  - Creates per-PS FIFOQueue to detect initialization.
  - `is_ps_uninitialized(sess, device)` checks queue size.
  - `setup_ps_initialized_state(sess)` enqueues to mark initialized.
- **`SaverBuilder(BulkSaverBuilder)`**
  - Overrides `_AddShardedRestoreOps` to store grouped restore ops per device.
  - `restore_ops_per_device` property returns grouped ops.
- **`PartialRecoverySaver`**
  - Based on TF `Saver` with modifications:
    - Supports `ps_monitor`, `exempt_checkpoint_paths`, `skip_save`, `model_dir`.
    - `exempt_checkpoint_paths` property reads `monolith_checkpoint` to avoid deleting exempt ckpts.
    - `_RecordLastCheckpoint` excludes exempt paths from deletion accounting.
    - `_MaybeDeleteOldCheckpoints` respects `keep_checkpoint_every_n_hours`.
    - `save(...)` supports `save_relative_paths`, handles parent dir errors, writes meta graph optionally.
    - `_origin_restore` restores by device; if `ps_monitor` set, skips devices already initialized.
    - `restore(...)`:
      - Falls back to `_origin_restore` during export or when no graph.
      - Supports `CUSTOM_RESTORE_OP` collection:
        - If alias map present, uses `calc_feed_dict` and `NewCheckpointReader`.
        - If init ops present, handles `clear_nn` flag and optionally updates global_step.
        - Removes `CUSTOM_RESTORE_OP` from graph collections.
    - `setup_ps_initialized_state(sess)` delegates to `ps_monitor`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-checkpoint/src`.
- Rust public API surface:
  - `PartialRecoverySaver`, `NoFirstSaveCheckpointSaverHook`, `SecondOrStepTimerWithTideSetting`, `SaveHelper`.
  - Monolith checkpoint state parser/writer.
- Data model mapping:
  - `MonolithCheckpointState` protobuf and checkpoint metadata.
  - Device-specific restore ops and partial recovery logic.
- Feature gating:
  - TF runtime backend required for Saver-based logic.
  - Tide-based timing requires time availability checks.
- Integration points:
  - Used by restore hooks, PS monitor, and runner utilities.

**Implementation Steps (Detailed)**
1. Implement checkpoint state cache and monolith checkpoint state parsing.
2. Port SaveHelper and tide-aware timer.
3. Implement NoFirstSaveCheckpointSaverHook with retry and metrics.
4. Implement PsMonitor queue-based initialization state.
5. Implement PartialRecoverySaver with exempt checkpoint handling and custom restore logic.
6. Add tests to mirror `save_utils_test.py` coverage.

**Tests (Detailed)**
- Python tests: `monolith/native_training/save_utils_test.py`.
- Rust tests: extensive saver/restore tests, max_to_keep, tide timer, and partial restore.
- Cross-language parity test: save in TF, restore in Rust (TF backend), compare values.

**Gaps / Notes**
- Custom restore logic depends on `CUSTOM_RESTORE_OP` collection and `clear_nn` flag file semantics.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/save_utils_test.py`
<a id="monolith-native-training-save-utils-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 1740
- Purpose/role: Comprehensive tests for `PartialRecoverySaver`, NoFirstSaveCheckpointSaverHook, checkpoint retention, and tide-aware timer.
- Key symbols/classes/functions: `SaveUtilsTest`, `SaverHookTest`, `SaverTest`, `SaveRestoreShardedTest`, `MaxToKeepTest`, `RecoverLastCheckpointsTest`, `KeepCheckpointEveryNHoursTest`, `SaveRestoreWithVariableNameMap`, `SecondOrStepTimerWithTideSettingTest`.
- External dependencies: TensorFlow, `freezegun`, `mock`, `saver_test_utils`, `resource_variable_ops`, `checkpoint_management`.
- Side effects: Writes checkpoint files under `TEST_TMPDIR`, uses eager/graph modes.

**Required Behavior (Detailed)**
- **SaveUtilsTest**
  - `test_get_ckpt_steps`: `SaveHelper.get_existing_checkpoint_steps` returns steps {10,20,300}.
  - `test_exempt_checkpoints`: validates `max_to_keep` with exempt checkpoints.
- **SaverHookTest**
  - `test_basic`: NoFirstSaveCheckpointSaverHook skips initial save, saves on session close.
  - `test_op_error`: guard listener `after_save` is called even when save fails.
  - `test_trigger_save`: `trigger_save` forces save within session.
- **SaverTest**
  - `basicSaveRestore`: save/restore of variables and `CheckpointedOp`.
  - `testSaveMaxToKeep`: max_to_keep with exempt path.
  - `testResourceColocation`: SaveV2 inputs colocated on CPU of same device.
  - `testResourceVariableReadOpsAddedDeterministically`: graph defs deterministic.
  - `testEagerBasic` and `testEagerGraphCompatibility`: eager/graph save/restore compatibility.
  - `testResourceSaveRestoreCachingDevice`: caching_device variable save/restore.
  - `testNoAdditionalOpsAddedBySaverForResourceVariablesOutsideSaveScope`: no extraneous ops.
  - `testSaveCopyRestoreWithSaveRelativePaths`: relative paths survive directory move.
  - `testFilenameTensor`, `testInvalidPath`, `testInt64`, `testSomeErrors`, `testSameName`.
  - `testBasicsWithListOfVariables`, `_SaveAndLoad` helper, `testCacheRereadsFile`.
  - `testAllowEmpty`, `testGPU`, `testSharedServerOnGPU`, `testVariables`.
  - `testVarListShouldBeEmptyInDeferredBuild`, `testBuildShouldBeCalledBeforeSaveInCaseOfDeferBuild`, `testDeferredBuild`.
  - `testReshape`: reshape restore behavior.
  - `testSaveWithGlobalStep`, `testSaveWithGlobalStepWithPadding`.
  - `testSaveToNonexistingPath`, `testSaveToURI`.
  - `testSaveRestoreAndValidateVariableDtype`, `testRestoreLargeTensors`.
- **SaveRestoreShardedTest**
  - `testIterators`: sharded save/restore of dataset iterators across devices.
  - `testIteratorsUnshardedRestore`: restore from sharded checkpoint when saver is unsharded.
- **MaxToKeepTest**
  - `testMaxToKeepEager`: checkpoint rotation in eager mode.
  - `testNonSharded`, `testSharded`, `testNoMaxToKeep`, `testNoMetaGraph`.
- **RecoverLastCheckpointsTest**
  - `test_recover_last_checkpoints`: recover last checkpoints, handle missing files.
- **KeepCheckpointEveryNHoursTest**
  - `testNonSharded`: uses mocked time to validate keep-every-N-hours behavior.
- **SaveRestoreWithVariableNameMap**
  - `testNonReshapeResourceVariable` / `testNonReshapeVariable`: name mapping restore.
- **SecondOrStepTimerWithTideSettingTest**
  - `testNoTideSetting`, `testTideAvailable`, `testTideNotAvailable` using `freezegun`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-checkpoint/src` (tests).
- Rust public API surface: saver, hook, timer, checkpoint retention.
- Data model mapping: checkpoint files + metadata; iterator saveables if supported.
- Feature gating: eager/graph tests depend on backend; GPU tests conditional.
- Integration points: checkpoint manager and saver semantics.

**Implementation Steps (Detailed)**
1. Port core saver tests: basic save/restore, max_to_keep, exempt checkpoints.
2. Add hook tests for NoFirstSaveCheckpointSaverHook and trigger_save.
3. Add timer tests for tide behavior (mock time).
4. Add sharded save/restore tests if supported.
5. Add deferred build and reshape tests.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: map major behaviors; use deterministic fixtures.
- Cross-language parity test: save in TF, restore in Rust/TF backend, compare values.

**Gaps / Notes**
- Some tests rely on TF-specific Saveable objects (`CheckpointedOp`, iterators).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/service_discovery.py`
<a id="monolith-native-training-service-discovery-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 481
- Purpose/role: Service discovery abstraction with Consul, TF_CONFIG, Zookeeper, and MLP implementations.
- Key symbols/classes/functions: `ServiceDiscoveryType`, `ServiceDiscovery`, `ConsulServiceDiscovery`, `TfConfigServiceDiscovery`, `ZKServiceDiscovery`, `MLPServiceDiscovery`, `deregister_all`.
- External dependencies: Consul client, Kazoo (ZK), `MonolithKazooClient`, `MLPEnv`.
- Side effects: Registers/deregisters nodes in external systems; spawns periodic registration threads for ZK.

**Required Behavior (Detailed)**
- **`ServiceDiscoveryType`**: `PRIMUS`, `CONSUL`, `ZK`, `MLP`.
- **`ServiceDiscovery`** abstract methods: `register`, `deregister`, `query`; optional `close`.
- **`retry_with_socket_error`**:
  - Retries callable up to 5 times on `socket.error` with randomized backoff (≤ `_RETRY_MAX_BACKOFF_SECS`).
- **`ConsulServiceDiscovery(consul_id, retry_time_secs=3.0)`**
  - `register`: best-effort de-register existing index, register via tags `{index,name,ip}`, then poll until visible (or raise `OSError` after ~180s).
  - `deregister`: removes entry by port.
  - `query_all`: returns `{name: {index: "ip:port"}}` from Consul lookup.
  - `query(name)`: returns mapping for that name.
- **`TfConfigServiceDiscovery(tf_config)`**
  - `register`/`deregister`: no-ops.
  - `query('ps')` uses `cluster['ps']`.
  - `query('worker')` prepends `cluster['chief']` if present.
  - `server_type`: `'worker'` if task type is `'chief'`, else task type.
  - `addr`: address for current task.
  - `index`: if chief exists and task is worker, index+1.
- **`ZKServiceDiscovery(job_name, zk_server=None, max_tries=3, delay=5)`**
  - Uses `MonolithKazooClient`, `ZKListener`, and Children/Data watches to maintain `_cluster`.
  - Registers nodes under `/monolith/{job_name}/{server_type}.{index}` as ephemeral.
  - Periodically re-registers every `_ZK_REGISTRATION_PERIOD` using background threads.
  - `query(name)` returns mapping for that name.
  - `close()` stops client and threads.
- **`MLPServiceDiscovery`**
  - Validates registration against `MLPEnv` expected roles/ports.
  - Maintains `_filters` to hide deregistered entries.
  - `query(name, skip_port_check=False)` returns address map; for PS checks port connectivity unless skipped.
  - `deregister_all` filters all roles.
  - `query_all` returns maps for `ps/worker/chief`.
  - `server_type` and `index` derived from `MLPEnv`.
- **`deregister_all(consul_id)`**: deregisters all entries in Consul for id.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src`.
- Rust public API surface: discovery trait + Consul/ZK/TF_CONFIG/MLP implementations.
- Data model mapping: `name -> index -> addr`.
- Feature gating: Consul/ZK/MLP clients behind optional features.
- Integration points: runner utilities and distributed training orchestration.

**Implementation Steps (Detailed)**
1. Define discovery trait and type enum.
2. Implement TF_CONFIG discovery (pure JSON).
3. Implement Consul discovery with retry + blacklist behavior.
4. Implement ZK discovery with watchers and periodic registration threads.
5. Implement MLP discovery with filter logic and port checks.
6. Add `deregister_all`.

**Tests (Detailed)**
- Python tests: `monolith/native_training/service_discovery_test.py`.
- Rust tests: mock clients and validate mappings, duplicate registration, retry behavior.
- Cross-language parity test: compare mapping outputs.

**Gaps / Notes**
- ZK implementation is stateful and thread-based; Rust must emulate or adapt.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/service_discovery_test.py`
<a id="monolith-native-training-service-discovery-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 407
- Purpose/role: Unit tests for Consul, TF_CONFIG, ZK service discovery and deregistration helpers.
- Key symbols/classes/functions: `FakeConsul`, `FakeKazooClient`, `ConsultServiceDiscovery`, `TfConfigServiceDiscoveryTest`, `ZKServiceDiscoveryTest`, `UtilsTest`.
- External dependencies: `unittest`, `mock`, `kazoo` exceptions, `service_discovery`.
- Side effects: Uses mocked clients; no real network.

**Required Behavior (Detailed)**
- **Consul tests**
  - `test_basic`: register two entries, query returns mapping, deregister removes all.
  - `test_duplicate_registration`: re-register same index replaces addr.
  - `test_multi_names`: independent maps for `ps` and `worker`.
  - `test_retry`: mocked socket timeout triggers retry and eventually raises.
  - `test_registration_failed`: blacklist causes `OSError`.
- **TF_CONFIG tests**
  - `test_tf_conf_sd`: verifies ps and worker lists (chief prepended), addr, server_type, and index behavior.
- **ZK tests**
  - `test_basic`: register, query, deregister works.
  - `test_duplicate_registration`: re-register same index updates addr.
  - `test_multi_names`: multi service names.
  - `test_periodic_registration`: with shortened period, corrupted data is repaired.
  - `test_listener`: LOST then CONNECTED triggers re-registration.
- **Utils test**
  - `test_deregister_all`: deregister_all removes all entries.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` (tests).
- Rust public API surface: discovery implementations and deregister_all.
- Data model mapping: in-memory mocks for Consul/ZK.
- Feature gating: optional client dependencies.
- Integration points: service discovery unit tests.

**Implementation Steps (Detailed)**
1. Add mock Consul client with register/lookup/deregister.
2. Add mock ZK client with watches and ephemeral nodes.
3. Port tests for duplicate registration, retry, periodic registration, and listener behavior.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: replicate with mocks.
- Cross-language parity test: compare mapping outputs and failure behaviors.

**Gaps / Notes**
- No direct tests for `MLPServiceDiscovery` in Python.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/serving_ps_test.py`
<a id="monolith-native-training-serving-ps-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 231
- Purpose/role: Generates example batches and feature configs for serving PS tests.
- Key symbols/classes/functions: `FeatMeta`, `TableMeta`, `ServingPSTest`.
- External dependencies: `distribution_ops` (imported), `idl.matrix.proto.example_pb2` protos.
- Side effects: Prints generated `ExampleBatch` and `FeatureConfigs` to stdout.

**Required Behavior (Detailed)**
- Defines feature/table metadata (`FeatMeta`, `TableMeta`) and a `features` map.
- `test_example_gen`:
  - Builds `ExampleBatch` with `batch_size=10`.
  - For each feature, fills `fid_v1_list` or `fid_v2_list` based on `fid_version`.
  - For SHARED feature lists, only adds first feature (breaks after one).
  - Prints the batch.
- `test_conf_gen`:
  - Builds `FeatureConfigs` with per-feature slice dims and pooling types.
  - Constructs `OutConfig` objects:
    - `bias` and `vec` as `CONCAT`.
    - `uffm`, `iffm`, `seq` as `NONE`.
    - `user_only` as `STACK`.
  - Configures slice ranges and shapes based on metadata.
  - Prints configs.
- `__main__`: disables eager execution and runs `tf.test.main()`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` (tests or utilities).
- Rust public API surface: helper to generate example batches and feature configs.
- Data model mapping: ExampleBatch, FeatureConfigs, OutConfig protos.
- Feature gating: requires proto definitions and encoding.
- Integration points: serving PS pipelines or export utilities.

**Implementation Steps (Detailed)**
1. Port metadata structs and feature definitions.
2. Implement example batch generator with fid_v1/v2 bit packing.
3. Implement feature config generation and OutConfig creation.
4. Add tests to validate shapes and slice configs.

**Tests (Detailed)**
- Python tests: `ServingPSTest` in this file (no assertions besides prints).
- Rust tests: add assertions for generated proto contents.
- Cross-language parity test: compare serialized protos.

**Gaps / Notes**
- Tests mainly print outputs; no asserts in Python.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/session_run_hooks.py`
<a id="monolith-native-training-session-run-hooks-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 171
- Purpose/role: SessionRunHooks for tide-aware stopping and delayed worker start based on global step.
- Key symbols/classes/functions: `before`, `tide_available_now`, `CustomGlobalStepWaiterHook`, `TideStoppingHook`.
- External dependencies: TensorFlow session hooks, `training_util`, `datetime`, `random`.
- Side effects: Sleeps and may request session stop.

**Required Behavior (Detailed)**
- **`before(hour1, minute1, hour2, minute2)`**:
  - Returns True if time1 < time2 (lexicographic hour/minute).
- **`tide_available_now(start_h, start_m, end_h, end_m)`**:
  - Determines if current UTC time is within tide window; handles wrap-around (start > end).
- **`CustomGlobalStepWaiterHook(wait_until_step, tide_*, max_non_tide_wait_minute=10)`**
  - `begin`: creates global step tensor; raises if missing.
  - `before_run`:
    - If already started or wait_until_step <= 0, returns immediately.
    - If tide window configured and tide not available, logs and requests stop.
    - Polls global step until >= wait_until_step; sets `_worker_is_started`.
    - Also starts a timer once global_step > 1; if wait exceeds `max_non_tide_wait_minute` (+ random 0–600s), starts anyway.
    - Sleeps 0.5s between checks and logs periodically.
- **`TideStoppingHook(tide_*)`**
  - `before_run`: if tide not available, logs and requests stop.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src`.
- Rust public API surface: session hooks for wait-until-step and tide stopping.
- Data model mapping: global step access and session stop request.
- Feature gating: none.
- Integration points: training runner hooks.

**Implementation Steps (Detailed)**
1. Implement time window logic (`tide_available_now`) with UTC time.
2. Implement wait hook with global step polling and timeout behavior.
3. Implement tide stopping hook for graceful shutdown.
4. Add tests using time freezing/mocking.

**Tests (Detailed)**
- Python tests: `monolith/native_training/session_run_hooks_test.py`.
- Rust tests: add tests for tide availability and wait logic.
- Cross-language parity test: compare tide window evaluations.

**Gaps / Notes**
- `CustomGlobalStepWaiterHook` uses random extra wait time (0–600s) in timeout calculation.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/session_run_hooks_test.py`
<a id="monolith-native-training-session-run-hooks-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 144
- Purpose/role: Tests for tide availability logic and global step waiter hook.
- Key symbols/classes/functions: `GlobalStepWaiterHookTest`, `TideStoppingHookTest`.
- External dependencies: TensorFlow, `freezegun`, `time`, `session_run_hooks`.
- Side effects: Uses frozen time and mocked `time.sleep`.

**Required Behavior (Detailed)**
- `GlobalStepWaiterHookTest`:
  - `test_not_wait_for_step_zero`: `wait_until_step=0` returns immediately.
  - `test_not_wait_if_tide_not_available`: with tide window outside current time, hook returns without waiting.
  - `test_wait_for_step`: mocks `time.sleep` to advance global_step; expects hook to loop twice.
- `TideStoppingHookTest`:
  - `test_stop_if_tide_not_available`: when tide not available, `request_stop` is called.
  - `test_do_not_stop_if_tide_available`: when tide available, no stop requested.
- `__main__`: disables eager execution and runs tests.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` (tests).
- Rust public API surface: wait hook and tide stopping hook.
- Data model mapping: global step mock and session context.
- Feature gating: none.
- Integration points: training runner.

**Implementation Steps (Detailed)**
1. Add tests for immediate return when wait_until_step=0.
2. Mock time and global step updates to test waiting loop.
3. Add tests for tide stopping behavior.

**Tests (Detailed)**
- Python tests: this file.
- Rust tests: mirror with mocked time and global step.
- Cross-language parity test: compare tide-window logic outputs.

**Gaps / Notes**
- Uses freezegun; Rust tests should use deterministic time control.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/signal_utils.py`
<a id="monolith-native-training-signal-utils-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 37
- Purpose/role: Installs a SIGUSR1 handler that prints stack traces.
- Key symbols/classes/functions: `print_stack_trace`, `add_siguser1_handler`.
- External dependencies: `signal`, `traceback`.
- Side effects: Registers a SIGUSR1 handler at import time.

**Required Behavior (Detailed)**
- `print_stack_trace(sig, frame)`:
  - Prints each line from `traceback.format_stack(frame)`.
- `add_siguser1_handler()`:
  - Captures current SIGUSR1 handler (`signal.getsignal`).
  - Registers new handler that calls previous handler (if callable) then prints stack trace.
- Module import calls `add_siguser1_handler()` immediately.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src`.
- Rust public API surface: signal handler registration and stack trace printing.
- Data model mapping: signal handling and backtrace capture.
- Feature gating: may be platform-specific (POSIX only).
- Integration points: runtime diagnostics.

**Implementation Steps (Detailed)**
1. Register SIGUSR1 handler on module init.
2. Chain previous handler if present.
3. Print backtrace to stdout/stderr.

**Tests (Detailed)**
- Python tests: `monolith/native_training/signal_utils_test.py`.
- Rust tests: add signal handler test (if supported).
- Cross-language parity test: verify handler chaining and output.

**Gaps / Notes**
- Signal handling is OS-specific; Rust may need conditional compilation.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/signal_utils_test.py`
<a id="monolith-native-training-signal-utils-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 30
- Purpose/role: Verifies SIGUSR1 handler registration and chaining.
- Key symbols/classes/functions: `SignalUtilsTest`.
- External dependencies: `signal`, `signal_utils`.
- Side effects: Raises SIGUSR1 signal.

**Required Behavior (Detailed)**
- `testBasic`:
  - Calls `add_siguser1_handler()` twice to ensure chaining works.
  - Raises SIGUSR1 signal via `signal.raise_signal`.
  - Test passes if no exception is raised.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src` (tests).
- Rust public API surface: signal handler registration.
- Data model mapping: SIGUSR1 handling.
- Feature gating: OS-specific support.
- Integration points: runtime diagnostics.

**Implementation Steps (Detailed)**
1. Add test that registers handler twice and raises SIGUSR1.
2. Ensure no panic/crash.

**Tests (Detailed)**
- Python tests: `SignalUtilsTest` in this file.
- Rust tests: add signal handler test if supported.
- Cross-language parity test: ensure handler chaining does not error.

**Gaps / Notes**
- Signal handling tests may be flaky on non-POSIX systems.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/static_reshape_op.py`
<a id="monolith-native-training-static-reshape-op-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 58
- Purpose/role: Wrapper for custom static reshape op and a small builder utility.
- Key symbols/classes/functions: `static_reshape`, `StaticReshapeNBuilder`.
- External dependencies: `gen_monolith_ops.monolith_static_reshape_n`.
- Side effects: None beyond calling custom op.

**Required Behavior (Detailed)**
- **`static_reshape(inputs, shapes, enable_parallelism=True)`**
  - Calls `monolith_static_reshape_n` custom op.
  - Returns `(outputs, sizes)` where `sizes` are flattened output sizes.
- **`StaticReshapeNBuilder`**
  - Collects `inputs` and `shapes`.
  - `add(input, shape)` appends and returns index.
  - `build()` calls `static_reshape` with collected lists.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/src`.
- Rust public API surface: `static_reshape` wrapper and builder.
- Data model mapping: list of tensors + shape tuples with optional `None`.
- Feature gating: requires custom op library.
- Integration points: preprocessing pipelines needing fast reshape.

**Implementation Steps (Detailed)**
1. Bind to custom op `monolith_static_reshape_n` or implement equivalent.
2. Implement builder helper with index mapping.
3. Add tests for sizes and shape constraints.

**Tests (Detailed)**
- Python tests: `monolith/native_training/static_reshape_op_test.py`.
- Rust tests: add tests for sizes and nested structure indexing.
- Cross-language parity test: compare output shapes and sizes.

**Gaps / Notes**
- Custom op semantics must match exactly; shapes include `None` placeholders.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/static_reshape_op_test.py`
<a id="monolith-native-training-static-reshape-op-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 79
- Purpose/role: Tests static reshape custom op and builder.
- Key symbols/classes/functions: `StaticReshapeOpTest`.
- External dependencies: TensorFlow, NumPy, `static_reshape_op`.
- Side effects: Runs sessions to execute custom op.

**Required Behavior (Detailed)**
- `test_static_reshape_n`:
  - Reshapes three inputs to target shapes with `None` dimensions.
  - Asserts `sizes == [5, 40, 12]`.
  - Verifies each output shape matches specified non-`None` dims.
- `test_nested_reshape_n`:
  - Uses `StaticReshapeNBuilder` with nested structures.
  - Flattens tensors via `tf.nest.map_structure` and builds op.
  - Validates outputs match expected flattened arrays.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/src` (tests).
- Rust public API surface: `static_reshape` and builder.
- Data model mapping: nested structure flattening.
- Feature gating: requires custom op.
- Integration points: custom op tests.

**Implementation Steps (Detailed)**
1. Add tests for sizes output and shape constraints.
2. Add nested structure test using builder indices.

**Tests (Detailed)**
- Python tests: `StaticReshapeOpTest` in this file.
- Rust tests: mirror both tests.
- Cross-language parity test: compare outputs for same inputs.

**Gaps / Notes**
- Relies on custom op `monolith_static_reshape_n`.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/summary/summary_ops.py`
<a id="monolith-native-training-summary-summary-ops-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 78
- Purpose/role: Summary ops for NAS and feature insight data with custom metadata.
- Key symbols/classes/functions: `nas_data`, `feature_insight_data`.
- External dependencies: TensorFlow summary APIs, `summary.utils`, `feature_insight` op.
- Side effects: Adds tensor summaries to graph collections.

**Required Behavior (Detailed)**
- **`nas_data(weight, segment_names=None, segment_sizes=None, group_info=None, raw_tag=None, collections=None, description=None, name=None)`**
  - Uses `utils.prepare_head(..., out_type='bytes')` to build metadata.
  - Creates `tf.summary.tensor_summary` with tag `MONOLITH_NAS_DATA`.
  - Name defaults to `{summaty_type}` or `{name}_{summaty_type}`.
  - Description defaults to summary type.
- **`feature_insight_data(input_tensor, segment_names, segment_sizes, weight=None, group_info=None, label=None, collections=None, description=None, name=None)`**
  - If `weight` is provided, calls `feature_insight` op to aggregate, and adjusts `segment_sizes` to uniform dimension.
  - Determines `raw_tag`: direct vs train based on `label`.
  - If label provided:
    - Casts to float32, ensures rank 2, sets `label_size` in metadata.
    - Concatenates label to summary_data.
  - Writes `tf.summary.tensor_summary` with tag `MONOLITH_FI_DATA` and JSON metadata.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/src`.
- Rust public API surface: summary helpers for NAS and feature insight.
- Data model mapping: summary metadata proto + tensor data.
- Feature gating: requires TensorBoard summary APIs; feature_insight op if used.
- Integration points: training summaries and analysis tooling.

**Implementation Steps (Detailed)**
1. Port `prepare_head` and metadata creation.
2. Implement summary writer helpers with matching tags and metadata.
3. Bind or reimplement `feature_insight` op if needed.
4. Add tests to validate metadata and tensor shapes.

**Tests (Detailed)**
- Python tests: `monolith/native_training/summary/summary_ops_test.py`.
- Rust tests: add metadata and shape validation tests.
- Cross-language parity test: compare serialized summary metadata.

**Gaps / Notes**
- Summary metadata uses JSON content; ensure exact formatting and ordering.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/summary/summary_ops_test.py`
<a id="monolith-native-training-summary-summary-ops-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 122
- Purpose/role: Tests summary metadata and tensor outputs for NAS and feature insight summaries.
- Key symbols/classes/functions: `SummaryTest`.
- External dependencies: TensorBoard data provider APIs, `summary_ops`.
- Side effects: Writes summary logs to `demo_logs_v1` and reads them back.

**Required Behavior (Detailed)**
- `setUpClass`:
  - Builds summary ops (`nas_data`, `feature_insight_data`), writes summaries for 10 steps.
  - Initializes `EventMultiplexer` and `MultiplexerDataProvider`.
- `test_nas_data`:
  - Reads summary metadata and tensor values for tag `gating/monolith_nas_weight`.
  - Asserts plugin content JSON matches expected string and values equal weights.
- `test_feature_insight_data`:
  - Reads tag `fi_train/monolith_feature_insight`.
  - Asserts plugin content JSON (including `label_size`) matches expected.
  - Validates tensor shape `(3, 7)` when label_size=1.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/src` (tests).
- Rust public API surface: summary ops and metadata.
- Data model mapping: TensorBoard summary metadata content.
- Feature gating: TensorBoard event parsing required.
- Integration points: summary writers and readers.

**Implementation Steps (Detailed)**
1. Add tests that emit summary data and read via event parsing.
2. Validate plugin metadata JSON and tensor shapes.

**Tests (Detailed)**
- Python tests: `SummaryTest` in this file.
- Rust tests: mirror event log parsing if supported.
- Cross-language parity test: compare plugin metadata content.

**Gaps / Notes**
- Uses TensorBoard data provider APIs; Rust may need alternative test approach.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/summary/utils.py`
<a id="monolith-native-training-summary-utils-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 114
- Purpose/role: Summary metadata helpers and NAS weight extraction.
- Key symbols/classes/functions: `SummaryType`, `create_summary_metadata`, `prepare_head`, `get_nas_weight_json`.
- External dependencies: TensorFlow, TensorBoard summary protos.
- Side effects: None (pure helpers).

**Required Behavior (Detailed)**
- Constants:
  - `PLUGIN_NAME = 'monolith'`
  - `MONOLITH_NAS_DATA`, `MONOLITH_FI_DATA`, `KTYPE`, `KMETA`, `KDATA`.
- `SummaryType` values: `gating`, `selecting`, `mixed`, `simple`, `fi_direct`, `fi_train`.
- `create_summary_metadata(description=None, meta_content=b'')`:
  - Returns `SummaryMetadata` with plugin data and `DATA_CLASS_TENSOR`.
  - Accepts `meta_content` as bytes or str; encodes to UTF-8 if str.
- `_name_to_group_id(segment_names, group_info)`:
  - Builds mapping of segment names to group indices based on group_info.
  - Reorders group IDs by sorted group id.
- `prepare_head(segment_names, segment_sizes, group_info=None, raw_tag=None, out_type='tensor')`:
  - If no segments: returns empty tensor/bytes with raw_tag.
  - Determines `raw_tag`:
    - `gating` when all sizes are ints, else `selecting`.
  - Builds data dict with tag type, names, sizes, and optional `group_index`.
  - Returns tensor/JSON/bytes depending on out_type.
- `get_nas_weight_json(ckpt_dir_or_file, prefix=None)`:
  - Loads checkpoint, finds variable containing `prefix` (defaults to `ARCH_TENSOR_PREFIX`), returns list of values as strings.
  - Raises Exception if not found.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/src`.
- Rust public API surface: metadata builder + NAS weight extraction.
- Data model mapping: Summary metadata proto and JSON content.
- Feature gating: checkpoint reader required for `get_nas_weight_json`.
- Integration points: summary ops.

**Implementation Steps (Detailed)**
1. Port constants and summary metadata creation.
2. Implement `prepare_head` with identical JSON ordering.
3. Implement checkpoint reader helper (or stub) for NAS weights.

**Tests (Detailed)**
- Python tests: `monolith/native_training/summary/utils_test.py`.
- Rust tests: add tests for `prepare_head` outputs.
- Cross-language parity test: compare JSON metadata strings.

**Gaps / Notes**
- `ARCH_TENSOR_PREFIX` is referenced but not defined in this file; locate source before porting.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/summary/utils_test.py`
<a id="monolith-native-training-summary-utils-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 43
- Purpose/role: Tests `prepare_head` output for gating and selecting cases.
- Key symbols/classes/functions: `UtilsTest`.
- External dependencies: `summary.utils`.
- Side effects: None.

**Required Behavior (Detailed)**
- `test_read_head_gating`:
  - `segment_sizes` are ints → SummaryType.GATING.
  - Asserts returned tensor equals expected JSON bytes with `group_index`.
- `test_read_head_selecting`:
  - `segment_sizes` are lists → SummaryType.SELECTING.
  - Asserts returned tensor equals expected JSON bytes without `group_index`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/src` (tests).
- Rust public API surface: `prepare_head` output.
- Data model mapping: JSON bytes in tensor.
- Feature gating: none.
- Integration points: summary metadata.

**Implementation Steps (Detailed)**
1. Add tests for gating/selecting outputs and expected JSON ordering.

**Tests (Detailed)**
- Python tests: `UtilsTest` in this file.
- Rust tests: mirror gating/selecting cases.
- Cross-language parity test: compare JSON bytes.

**Gaps / Notes**
- JSON ordering must match Python `json.dumps` output.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/sync_hooks.py`
<a id="monolith-native-training-sync-hooks-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 176
- Purpose/role: Implements synchronization hooks between chief and workers.
- Key symbols/classes/functions: `SyncHelper`, `ChiefSyncHook`, `WorkerSyncHook`, `TrainingHooksHelper`.
- External dependencies: TensorFlow, `absl.logging`.
- Side effects: Uses TF variables to track worker status.

**Required Behavior (Detailed)**
- **`SyncHelper(num_workers, is_chief, var_device="/job:chief/task:0")`**
  - Creates `control_var` boolean vector length `num_workers`.
  - Index 0 = restore status; indices >0 = worker alive flags.
  - `mark_restore_done` sets index 0 True.
  - `start_worker`/`finish_worker` toggles worker index.
  - `get_alive_workers` returns indices with True.
  - `get_num_alive_workers` returns count of alive workers.
- **`ChiefSyncHook(sync_helper, timeout_seconds=1800)`**
  - `after_create_session`: marks restore done.
  - `end`: waits until no alive workers or timeout; logs remaining workers.
- **`WorkerSyncHook(worker_index, sync_helper)`**
  - `after_create_session`: marks worker alive and waits for restore status.
  - `end`: marks worker finished.
- **`TrainingHooksHelper(enable_sync, num_workers, worker_idx, chief_timeout_seconds)`**
  - If enabled, creates SyncHelper and attaches Chief/Worker hooks.
  - `training_hooks` and `training_chief_hooks` return tuples.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src`.
- Rust public API surface: sync helper and hooks.
- Data model mapping: shared status vector (e.g., in a distributed store).
- Feature gating: distributed training only.
- Integration points: training runner hooks.

**Implementation Steps (Detailed)**
1. Implement SyncHelper state tracking.
2. Implement Chief and Worker hooks with timeout.
3. Add helper to assemble hooks.
4. Add tests for synchronization flow.

**Tests (Detailed)**
- Python tests: `monolith/native_training/sync_hooks_test.py`.
- Rust tests: add multi-threaded hook tests.
- Cross-language parity test: compare wait/finish behavior.

**Gaps / Notes**
- `var_device` defaults to chief device; may differ in Rust backends.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/sync_hooks_test.py`
<a id="monolith-native-training-sync-hooks-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 119
- Purpose/role: Tests sync helper and hook coordination.
- Key symbols/classes/functions: `SyncHooksTest`, `CountHook`.
- External dependencies: TensorFlow, threading.
- Side effects: Spawns threads.

**Required Behavior (Detailed)**
- `test_sync_process`:
  - Creates `SyncHelper`, Chief/Worker hooks, and count hooks.
  - Worker waits at after_create_session until chief marks restore done.
  - Chief waits at end until worker finishes; verifies counts.
- `test_hook_helper`:
  - Ensures TrainingHooksHelper returns empty tuples when disabled.
  - Creates enabled helper for grammar check.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` (tests).
- Rust public API surface: sync helper and hooks.
- Data model mapping: thread synchronization.
- Feature gating: distributed training.
- Integration points: training runner.

**Implementation Steps (Detailed)**
1. Add multithreaded test for sync flow.
2. Add test for TrainingHooksHelper output.

**Tests (Detailed)**
- Python tests: `SyncHooksTest` in this file.
- Rust tests: mirror with threads.
- Cross-language parity test: compare hook sequencing.

**Gaps / Notes**
- Relies on timing; Rust tests should avoid flakiness.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/sync_training_hooks.py`
<a id="monolith-native-training-sync-training-hooks-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 355
- Purpose/role: Hooks for synchronized training, parameter sync, forced dumps, and EOF-aware input wrapping.
- Key symbols/classes/functions: `SyncTrainingBarrierSaverListener`, `ParameterSyncHook`, `SyncTrainingForceDumpHook`, `SyncTrainingSaverControlHook`, `SyncTrainingInfoHook`, `ReqTimeControlDumpHook`, `EofAwareTask`.
- External dependencies: TensorFlow, Horovod (`hvd_lib`), distributed serving ops, hash table ops.
- Side effects: Broadcasts control flags, reads marker files, requests stop, modifies input pipeline.

**Required Behavior (Detailed)**
- **`SyncTrainingBarrierSaverListener`**
  - Uses `hvd_lib.broadcast` to sync after save.
- **`ParameterSyncHook(sync_backend, ps_index, refresh_interval=100)`**
  - Refreshes sync config periodically.
  - Calls `ParameterSyncClient.create_sync_op` and runs with config feed.
- **`SyncTrainingForceDumpHook(model_dir, target_timer, step_interval=100)`**
  - Every `step_interval`, rank 0 checks `dump_{step}` and `stop_{step}` files.
  - Broadcasts flags to all ranks; enables/disables target_timer based on UTC hour (18–20) and flags.
  - Requests stop if `should_stop`.
- **`SyncTrainingSaverControlHook(model_dir, target_timer, step_interval=100)`**
  - Toggles `target_timer` based on existence of `ONLINE` file.
- **`SyncTrainingInfoHook`**
  - Every 600s, logs hash table sizes collected from graph.
- **`ReqTimeControlDumpHook(model_dir, target_timer, step_interval=1000)`**
  - Rank 0 reads `req_time` collection and `limit_req_time` file.
  - Broadcasts values; requests stop if `req_time >= limit`.
- **`EofAwareTask(task, use_dataservice=False)`**
  - Wraps `NativeTask` to inject EOF flag into features and stop training across ranks when EOF is reached.
  - Adds `EofHook` to training hooks using `hvd_lib.allgather`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src`.
- Rust public API surface: training hooks for sync and EOF handling.
- Data model mapping: Horovod/allreduce equivalents if used.
- Feature gating: Horovod/distributed backend required for some hooks.
- Integration points: training runner and serving parameter sync.

**Implementation Steps (Detailed)**
1. Implement sync barrier and parameter sync hooks.
2. Implement dump/stop control via filesystem flags.
3. Implement EOF-aware task wrapper with distributed stop.
4. Add tests for EOF-aware task behavior.

**Tests (Detailed)**
- Python tests: `monolith/native_training/sync_training_hooks_test.py`.
- Rust tests: add EOF-aware task tests and hook smoke tests.
- Cross-language parity test: compare EOF stop behavior.

**Gaps / Notes**
- Several hooks depend on Horovod; Rust parity depends on distributed backend support.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/sync_training_hooks_test.py`
<a id="monolith-native-training-sync-training-hooks-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 92
- Purpose/role: Tests EOF-aware task wrapper with simple datasets.
- Key symbols/classes/functions: `EofAwareTaskTest`.
- External dependencies: TensorFlow, `hvd_lib`, `sync_training_hooks`.
- Side effects: Initializes Horovod.

**Required Behavior (Detailed)**
- `test_basic`:
  - Defines simple NativeTask with dataset [1,2,3].
  - Wraps in `EofAwareTask` and trains estimator.
  - Expects `global_step == 6` (sum of 1+2+3).
- `test_dict`:
  - Similar but dataset yields dict `{"1": x}`.
  - Expects `global_step == 6`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src` (tests).
- Rust public API surface: EOF-aware task wrapper.
- Data model mapping: dataset + global_step.
- Feature gating: Horovod/distributed backend if used.
- Integration points: estimator or training loop.

**Implementation Steps (Detailed)**
1. Implement EOF-aware task wrapper.
2. Add tests for scalar and dict datasets.

**Tests (Detailed)**
- Python tests: `EofAwareTaskTest` in this file.
- Rust tests: mirror both cases.
- Cross-language parity test: compare global_step final value.

**Gaps / Notes**
- Tests call `hvd_lib.init()`; Rust may stub if Horovod not supported.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/tensor_utils.py`
<a id="monolith-native-training-tensor-utils-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 162
- Purpose/role: Utilities for packing/unpacking tensors (including typed groups) and ragged squeeze helper.
- Key symbols/classes/functions: `maybe_squeeze_3d_tensor`, `pack_tensors`, `unpack_tensors`, `pack_typed_keyed_tensors`, `unpack_packed_tensors`, `split_tensors_with_type`, `merge_dicts`.
- External dependencies: TensorFlow, `static_reshape_op`.
- Side effects: None (pure tensor ops).

**Required Behavior (Detailed)**
- `maybe_squeeze_3d_tensor(x)`:
  - Accepts RaggedTensor with rank 2 or 3; squeezes axis=1 when rank 3.
  - Raises ValueError on non-ragged or unexpected rank.
- `pack_tensors(keyed_tensors)`:
  - Uses StaticReshapeNBuilder to flatten tensors to 1-D, concatenates in sorted key order.
  - Returns `(packed_tensor, sizes)` where sizes are per-tensor sizes.
- `unpack_tensors(keyed_shape, packed)`:
  - Splits packed tensor by sizes, reshapes to original shapes, returns dict by sorted key.
- `split_tensors_with_type` / `merge_dicts`:
  - Group tensors by dtype string; returns list of dicts.
  - `merge_dicts` flattens back into one dict.
- `pack_typed_keyed_tensors` / `unpack_packed_tensors`:
  - Packs list of dicts (already grouped by type) into list of packed tensors plus a final concat of sizes.
  - `unpack_packed_tensors` reconstructs list of dicts by shape metadata.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tensor/src`.
- Rust public API surface: packing/unpacking helpers and ragged squeeze.
- Data model mapping: tensor shapes and sizes encoded as concatenated sizes.
- Feature gating: requires static reshape op or equivalent.
- Integration points: data transfer and async pipelines.

**Implementation Steps (Detailed)**
1. Port ragged squeeze helper with rank checks.
2. Implement pack/unpack with deterministic key ordering.
3. Implement typed packing and size concatenation.
4. Add tests for placeholder support and dtype grouping.

**Tests (Detailed)**
- Python tests: `monolith/native_training/tensor_utils_test.py`.
- Rust tests: add pack/unpack and typed pack/unpack tests.
- Cross-language parity test: compare packed tensors and reconstructed outputs.

**Gaps / Notes**
- `StaticReshapeNBuilder` relies on custom op; ensure Rust backend supports it.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/tensor_utils_test.py`
<a id="monolith-native-training-tensor-utils-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 175
- Purpose/role: Tests for tensor packing/unpacking utilities.
- Key symbols/classes/functions: `TensorUtilsTest`.
- External dependencies: TensorFlow, `tensor_utils`.
- Side effects: None.

**Required Behavior (Detailed)**
- `test_maybe_squeeze_3d_tensor`: verifies ragged squeeze for rank 2/3.
- `test_pack_tensors`: pack/unpack dict of tensors; verifies sizes and values.
- `test_pack_typed_keyed_tensors`: pack/unpack multiple dtype dicts; verifies packed sizes and outputs.
- `test_pack_typed_keyed_tensors_with_placeholder`: supports placeholders with feed_dict.
- `test_split_tensors_with_type_and_merge_dicts`: dtype grouping and merge round-trip.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tensor/src` (tests).
- Rust public API surface: pack/unpack helpers.
- Data model mapping: dtype grouping and shape handling.
- Feature gating: requires static reshape op support.
- Integration points: packing utilities.

**Implementation Steps (Detailed)**
1. Port tests for pack/unpack and typed packing.
2. Add placeholder-like tests if backend supports deferred shapes.

**Tests (Detailed)**
- Python tests: `TensorUtilsTest` in this file.
- Rust tests: mirror test cases.
- Cross-language parity test: compare packed sizes and reconstructed dicts.

**Gaps / Notes**
- Placeholder test relies on feed_dict semantics.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/test_utils.py`
<a id="monolith-native-training-test-utils-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 65
- Purpose/role: Test helpers for hash table config and PS cluster setup.
- Key symbols/classes/functions: `generate_test_hash_table_config`, `create_test_ps_cluster`, `profile_it`.
- External dependencies: TensorFlow, `entry`, `utils`, `embedding_hash_table_pb2`.
- Side effects: Starts local TF servers; profiler writes to `/tmp/tests_profile`.

**Required Behavior (Detailed)**
- `generate_test_hash_table_config(dim=2, use_float16=False, learning_rate=1.0)`:
  - Builds `EmbeddingHashTableConfig` with cuckoo, one segment, SGD, zero init, fp32 compression.
  - Returns `entry.HashTableConfigInstance(config, [learning_rate])`.
- `create_test_ps_cluster(num_ps)`:
  - Creates `num_ps` local servers.
  - Builds `ClusterDef` with PS job and returns `(servers, ConfigProto)`.
- `profile_it(fn)` decorator:
  - Starts TensorFlow profiler with fixed options.
  - Stops profiler and sleeps 1s after run (note: `time` is not imported in file).

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src`.
- Rust public API surface: test helpers for hash table config and local cluster setup.
- Data model mapping: hash table config proto.
- Feature gating: TF runtime required for local server creation.
- Integration points: test infrastructure.

**Implementation Steps (Detailed)**
1. Port hash table config helper.
2. Implement local PS cluster helper if TF backend available.
3. Add profiling helper or stub.

**Tests (Detailed)**
- Python tests: none dedicated.
- Rust tests: use helpers in other test modules.
- Cross-language parity test: not applicable.

**Gaps / Notes**
- `profile_it` uses `time.sleep` but `time` is not imported; Python would error if called.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/touched_key_set_ops.py`
<a id="monolith-native-training-touched-key-set-ops-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 61
- Purpose/role: Thin Python wrapper for TF custom ops that manage a thread-safe “touched key set” resource (insert IDs, steal+clear IDs).
- Key symbols/classes/functions: `TOUCHED_KEY_SET_CAPACITY`, `TOUCHED_KEY_SET_CONCURRENCY_LEVEL`, `create_touched_key_set`, `TouchedKeySet`.
- External dependencies: TensorFlow, `monolith.native_training.runtime.ops.gen_monolith_ops` (custom TF ops).
- Side effects:
  - Creates a stateful TF resource (`MonolithTouchedKeySet`) with `shared_name="MonolithTouchedKeySet" + name_suffix`.
  - `insert`/`steal` are stateful ops that mutate the underlying set.
  - `TouchedKeySet.__init__` ignores the `name_suffix` argument (potential bug / resource collision risk).

**Required Behavior (Detailed)**
- Constants:
  - `TOUCHED_KEY_SET_CAPACITY = 64 * 1024 * 1024 // (8 * 4)` → 2,097,152 (matches TF op default capacity).
  - `TOUCHED_KEY_SET_CONCURRENCY_LEVEL = 1024` (matches TF op default).
- `create_touched_key_set(capacity, concurrency_level, name_suffix="")`:
  - Calls TF custom op `MonolithTouchedKeySet` with `capacity`, `concurrency_level`, `shared_name="MonolithTouchedKeySet" + name_suffix`.
  - Returns a TF resource handle (scalar resource tensor).
- `TouchedKeySet.__init__(capacity=..., concurrency_level=..., name_suffix="")`:
  - Creates resource via `create_touched_key_set(capacity, concurrency_level)` **without** passing `name_suffix`.
  - Stores `_capacity`, `_concurrency_level`, and `_set` (resource handle).
- `TouchedKeySet.insert(ids)`:
  - Calls `monolith_touched_key_set_insert(handle, ids)`.
  - `ids` is `int64` tensor of any shape; op flattens via `NumElements()` and iterates in row-major order.
  - Returns `total_dropped_num` (int64) = sum of dropped keys across this call’s inserts.
  - Dropped keys semantics (from C++ hopscotch set):
    - On insert: if current size **> capacity**, the set is **cleared**, and `dropped_keys = size_before_clear`.
    - For each inserted ID, the per-key `Insert` returns `dropped_keys` (0 if no clear, `size_before_clear` if cleared during that insert).
    - Duplicate inserts return `dropped_keys` without increasing size.
  - The TF op allocates a 1-element tensor output; tests treat it as scalar.
- `TouchedKeySet.steal()`:
  - Calls `monolith_touched_key_set_steal(handle)`.
  - Returns all currently stored keys as a 1-D int64 tensor; **clears** the set.
  - Output order is **non-deterministic** (internal hopscotch table + `absl::flat_hash_set` iteration).
- Threading/concurrency:
  - Insert is thread-safe with per-bucket locks; `concurrency_level` controls lock count (rounded to power of two).
  - `steal` obtains a global clear lock and locks all buckets, blocking concurrent inserts while it drains.
- Determinism:
  - Output order of `steal` is unspecified; callers sort when comparing.
  - Overflow behavior is “clear-all” once size exceeds capacity (no partial eviction).
- Performance characteristics:
  - Average O(1) inserts; table size = next power of two of `capacity * 1.2`.
  - Uses extra overflow set (`absl::flat_hash_set`) when hopscotch insertion fails.
  - Lazy initialization to reduce memory until first insert.
- Metrics/logging: none.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-hash-table/src` (new `touched_key_set.rs` or equivalent).
- Rust public API surface:
  - `struct TouchedKeySet { capacity: u32, concurrency_level: u32, ... }`
  - `fn new(capacity: u32, concurrency_level: u32, name_suffix: Option<&str>) -> Self`
  - `fn insert(&self, ids: &[i64]) -> i64`
  - `fn steal(&self) -> Vec<i64>`
  - Accessors: `capacity()`, `concurrency_level()`.
- Data model mapping:
  - TF resource handle ↔ Rust in-process set instance.
  - If TF runtime backend is enabled, wrap the custom op handles instead of in-process implementation.
- Feature gating:
  - Default Rust-native implementation (no TF).
  - Optional `tf-runtime` feature: if a real `saved_model.pb` and custom ops are present, use TF-backed ops for parity.
- Integration points:
  - Parameter sync / hash table touched key tracking (see `runtime/ops/parameter_sync_tf_bridge.*` and `hash_table_op.cc` for Python-side wiring).
  - Rust hash-table or parameter-sync module should consume `TouchedKeySet` to report touched keys.

**Implementation Steps (Detailed)**
1. Define Rust `TouchedKeySet` API and decide backend selection (native vs TF runtime).
2. Port hopscotch hash set semantics (clear-all on `size > capacity`, duplicate-insert behavior, `steal` order nondeterminism).
3. Preserve lazy initialization (defer allocation until first insert) or document the deviation if eager.
4. Implement concurrency: shard locks by `concurrency_level`, match power-of-two rounding.
5. Ensure `insert` returns **total dropped count** per call (sum of per-key clears).
6. Implement `steal` to return all keys and clear; order undefined.
7. Add feature flag wiring in `monolith-rs` to switch to TF runtime custom ops when available.
8. Document the `name_suffix` bug in Python (ignored in `TouchedKeySet.__init__`) and decide whether Rust mirrors it or fixes it.

**Tests (Detailed)**
- Python tests: `monolith/native_training/touched_key_set_ops_test.py`.
- Rust tests:
  - `test_basic`: insert 0..999 into capacity 1000, `insert` returns 0, `steal` returns all keys (sorted match).
  - `test_overflow_clear`: insert 0..1004 into capacity 1000; `insert` returns 1001; `steal` returns {1001..1004}.
  - `test_duplicate_inserts`: repeated inserts do not increase size or return drops.
  - `test_thread_safety`: concurrent inserts + steal does not panic and preserves clear semantics.
- Cross-language parity test:
  - Run Python op + Rust implementation with same inserts; compare `total_dropped_num` and sorted `steal` output.

**Gaps / Notes**
- `TouchedKeySet.__init__` accepts `name_suffix` but never passes it to `create_touched_key_set` (likely unintended).
- TF op returns a 1-element tensor for `total_dropped_num` even though shape inference says scalar.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/touched_key_set_ops_test.py`
<a id="monolith-native-training-touched-key-set-ops-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 51
- Purpose/role: Validates basic `TouchedKeySet` behavior (insert/steal) and overflow clear semantics.
- Key symbols/classes/functions: `TouchedKeySetOpsTest`, `test_touched_key_set_basic`, `test_touched_key_set_overflow`.
- External dependencies: TensorFlow, `TouchedKeySet` wrapper.
- Side effects: Creates TF resource-backed touched key set.

**Required Behavior (Detailed)**
- `test_touched_key_set_basic`:
  - Create `TouchedKeySet(1000, 1)`.
  - Insert `ids = [0..999]`, expect `total_dropped_num == 0`.
  - `steal()` returns exactly those IDs (order ignored; test sorts).
- `test_touched_key_set_overflow`:
  - Create `TouchedKeySet(1000, 1)`.
  - Insert `ids = [0..1004]`, expect `total_dropped_num == 1001` (clear-all on overflow).
  - `steal()` returns `[1001, 1002, 1003, 1004]` (sorted).
- Uses TF v1 session execution (`tf.test.TestCase` + `self.session()`).
- `tf.compat.v1.disable_eager_execution()` in `__main__`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-hash-table/src` (new touched key set tests).
- Rust public API surface: `TouchedKeySet::new`, `insert`, `steal`.
- Data model mapping: `Vec<i64>` ↔ TF int64 tensor equivalents.
- Feature gating: if TF backend is optional, tests should run against native implementation; add TF-backed parity tests under feature flag.
- Integration points: unit tests in hash table crate; optional integration test comparing to Python output.

**Implementation Steps (Detailed)**
1. Port `test_touched_key_set_basic` as Rust unit test (sort results before compare).
2. Port `test_touched_key_set_overflow` to validate clear-all behavior.
3. Add a Rust test for duplicate insert (not in Python, but covers insert semantics).
4. Add optional cross-language test (Python vs Rust) if CI can run TF custom ops.

**Tests (Detailed)**
- Python tests: `TouchedKeySetOpsTest.test_touched_key_set_basic`, `.test_touched_key_set_overflow`.
- Rust tests:
  - `touched_key_set_basic`
  - `touched_key_set_overflow`
  - `touched_key_set_duplicate_insert` (optional but recommended)
- Cross-language parity test:
  - Compare Python session outputs vs Rust outputs for the two scenarios above.

**Gaps / Notes**
- Output ordering from `steal` is non-deterministic; tests must sort before compare.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/utils.py`
<a id="monolith-native-training-utils-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 320
- Purpose/role: Utility helpers for TF-native training (device strings, gradient backprop helpers, shape helpers, params introspection, graph dependency checks, TF collections, env-driven metric prefix).
- Key symbols/classes/functions: `PS_JOB_NAME`, `ps_device`, `propagate_back_gradients`, `propagate_back_dict_gradients`, `get_ndim`, `int_shape`, `extend_as_list`, `check_list`, `to_snake_case`, `to_list`, `_get_parameters`, `_get_all_parameters`, `_inverted_index`, `params`, `check_ops_dependence`, `with_params`, `get_local_host`, `get_test_tmp_dir`, `get_debugging_info_file_name`, `get_meta_graph_file_name`, `add_to_collections`, `get_collection`, `set_metric_prefix`, `get_metric_prefix`.
- External dependencies: TensorFlow (core + internal graph utils), `absl.logging`, `monolith.core.base_layer.get_uname`, `monolith.core.hyperparams` (`allowed_kwargs`, `InstantiableParams`, `Params`), stdlib (`os`, `platform`, `socket`, `re`, `inspect`, `copy`, `types`).
- Side effects:
  - Raises `RuntimeError`, `TypeError`, `ValueError`, generic `Exception` in validation helpers.
  - Mutates TF collections via `tf.compat.v1.add_to_collections`.
  - Sets environment variable `MONOLITH_METRIC_PREFIX`.

**Required Behavior (Detailed)**
- Constants:
  - `PS_JOB_NAME = "ps"`.
- `ps_device(index)`:
  - Returns `"/job:ps/task:{index}/device:CPU:0"` (string formatting only; no validation).
- `propagate_back_gradients(grads_and_vars, xs, valid_var_set=None)`:
  - Iterates `(grad, var)` pairs; if `valid_var_set` and `var` not in it → `RuntimeError("Invalid variables in the input", var, valid_var_set)`.
  - Accumulates `combined_vars` and `combined_grads` in input order.
  - Returns `tf.gradients(combined_vars, list(xs), combined_grads)` (list aligned with `xs` order).
- `propagate_back_dict_gradients(grads_and_vars, x_to_key, valid_var_set=None)`:
  - Calls `propagate_back_gradients` using `x_to_key.keys()`.
  - Zips returned `dxs` with `x_to_key.items()` (dict iteration order), grouping into `defaultdict(list)` keyed by `key` → list of `(dx, x)`.
- `get_ndim(x)`:
  - Returns `len(x.get_shape()._dims)` if `_dims` is not `None`; else `None`.
- `int_shape(x)`:
  - Uses `x.get_shape().as_list()`; maps `None` → `-1`, `int` as-is, `tf.compat.v1.Dimension` → `.value`.
  - Any other dim type raises `ValueError`; function catches `ValueError` and returns `None`.
- `extend_as_list(x, n)`:
  - If `x` is `list`/`tuple`:
    - If `len(x) < n`: return `x + [None] * (n - len(x))`.
    - Else: return `x` as-is (no copy).
  - Else: attempts `[x if i == 0 else deepcopy(x) for i in range(n)]`; if deepcopy fails, returns `[x] * n`.
- `check_list(candidate, length_checker, could_be_none=False)`:
  - If `candidate is None` and `not could_be_none`: `TypeError`.
  - Only accepts `None` or `list`; other types (including `tuple`) → `TypeError`.
  - If `candidate` is list and `length_checker(len(candidate))` is false → `ValueError`.
  - Returns `candidate` unchanged.
- `to_snake_case(name)`:
  - Inserts underscores between camel-case boundaries; lowercases.
  - If result starts with `_`, prefix with `"private"`.
- `to_list(x)`:
  - If `x` is a `list`, returns `x` (no copy).
  - Else returns `[x]` (tuples are not expanded).
- Parameter helpers:
  - `_get_parameters(cls, parameters)` collects `__init__` parameters except `self/cls` and varargs/kwargs.
  - `_get_all_parameters` walks base classes (excluding `object`) then calls `_get_parameters`.
  - `_inverted_index(ips, idx_dict)` recursively maps param names to their `InstantiableParams` container.
  - `params(cls)`:
    - Finds nearest base with `.params()`, otherwise `InstantiableParams(cls)`.
    - Sets `ips.cls = cls`.
    - Builds param list from `__init__` signature across the MRO.
    - Attempts to define `name` using `get_uname(cls.__name__)` (ignores exceptions).
    - For each parameter:
      - If name exists in inverted index: update default value if non-empty.
      - Else: `ips.define(name, default_or_None, name)`; if define fails and default is not empty/None, set via `ips[name] = default`.
    - Defines `allowed_kwargs` with default `None` (ignores exceptions).
    - Returns `ips`.
- `check_ops_dependence(op_names_1, op_names_2)`:
  - Extracts subgraph of `op_names_1` from default graph.
  - If any op in `op_names_2` appears in that subgraph, raises `Exception` with message:
    `"Checking ops dependence, the ops [%s] depend on ops [%s], which may cause ops [%s] to be run twice."`
- `with_params(cls)`:
  - Binds `params` as a method of `cls` and returns `cls`.
- Host/paths/env:
  - `get_local_host()`:
    - Windows/Linux: `socket.gethostbyname(socket.gethostname())`.
    - Other OS: `socket.gethostbyname(socket.gethostname() + ".local")`.
  - `get_test_tmp_dir()` reads `TEST_TMPDIR` env var, default `/tmp`.
  - `get_debugging_info_file_name(model_dir)` → `model_dir/debugging_info.pb`.
  - `get_meta_graph_file_name(model_dir)` → `model_dir/meta_graph_for_debugging.pb`.
  - `set_metric_prefix(prefix)` sets `MONOLITH_METRIC_PREFIX`.
  - `get_metric_prefix()` returns `MONOLITH_METRIC_PREFIX` or default `"monolith.training"`.
- Collections:
  - `add_to_collections(names, value)`:
    - If value is `bool|int|float|str`, always adds (even falsy).
    - Else if `value` is truthy, adds.
    - Else logs `value is {value}, skip`.
  - `get_collection(name)`:
    - If collection is `bool|int|float|str`, returns it directly.
    - Else if collection is truthy (non-empty list), returns it.
    - Else returns `None`.
- Threading/concurrency: no explicit locks here (TF graph-level ops only).
- Determinism: `propagate_back_dict_gradients` relies on dict iteration order (preserves insertion order in Python 3.7+).
- Performance: `check_ops_dependence` materializes graph def + subgraph; can be expensive on large graphs.
- Metrics/logging: only `add_to_collections` logs when skipping falsy values.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/utils.rs` (new).
- Rust public API surface:
  - `ps_device(index: usize) -> String`
  - `propagate_back_gradients(...)` / `propagate_back_dict_gradients(...)` (TF backend only)
  - `get_ndim`, `int_shape`, `extend_as_list`, `check_list`, `to_snake_case`, `to_list`
  - `params` analog for Rust config structs (likely a builder/derive macro)
  - `check_ops_dependence` (TF backend only)
  - `get_local_host`, `get_test_tmp_dir`, `get_debugging_info_file_name`, `get_meta_graph_file_name`
  - `add_to_collections` / `get_collection` (TF backend only)
  - `set_metric_prefix`, `get_metric_prefix`
- Data model mapping:
  - TF tensors/graph ops map only under `tf-runtime` feature.
  - Rust-native training uses native tensors and may skip TF-specific utilities.
- Feature gating:
  - `tf-runtime` for gradient propagation + collections + graph dependence checks.
  - Native-only implementations for string/path/env helpers.
- Integration points:
  - `monolith-training` modules referencing `utils.ps_device`, `utils.get_metric_prefix`, `utils.get_test_tmp_dir`.

**Implementation Steps (Detailed)**
1. Create `monolith-training/src/utils.rs` and port pure helpers (string/path/env/shape helpers).
2. Implement `ps_device` exactly (string formatting).
3. For `propagate_back_*`, gate behind TF runtime and map to TF gradient APIs (or document unsupported for Candle).
4. Recreate `params` behavior in Rust (likely a builder or derive macro). Preserve defaults + `allowed_kwargs` handling.
5. Implement `check_ops_dependence` for TF backend; otherwise return Ok/NotSupported.
6. Provide collection helpers under TF backend (or maintain a Rust-side registry if needed).
7. Mirror exceptions and error messages where externally visible (especially `RuntimeError` in `propagate_back_gradients`).

**Tests (Detailed)**
- Python tests: `monolith/native_training/utils_test.py`.
- Rust tests:
  - `test_propagate_back_dict_gradients` (TF backend only).
  - `test_check_ops_dependence` (TF backend only).
  - `test_collections` (TF backend only).
  - Unit tests for `ps_device`, `to_snake_case`, `extend_as_list`, `check_list`, `get_metric_prefix`.
- Cross-language parity test:
  - Compare `propagate_back_dict_gradients` outputs (dx, x) for the same graph.

**Gaps / Notes**
- `extend_as_list` will error for tuples when `len(x) < n` due to `tuple + list` (no guard).
- `to_list` treats tuples as a single element (not expanded).
- Uses private TF shape attribute `_dims`.
- Several imports are unused (`numpy.lib.arraysetops.isin`, `tensorflow.python.framework.ops`, `variables`, `six`).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/utils_test.py`
<a id="monolith-native-training-utils-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 70
- Purpose/role: Tests gradient backprop grouping, graph dependence checks, and TF collection helpers.
- Key symbols/classes/functions: `UtilsTest`, `test_propagate_back_dict_gradients`, `test_check_ops_dependence`, `test_collections`.
- External dependencies: TensorFlow, `monolith.native_training.utils`.
- Side effects: Adds TF collections within the test graph.

**Required Behavior (Detailed)**
- `test_propagate_back_dict_gradients`:
  - Create `x = tf.Variable(8.0)`, `y = 2 * x`, `grad_y = 3 * y`.
  - `valid_vars = {y}`; call `propagate_back_dict_gradients(zip([grad_y], [y]), {x: "group1"}, valid_vars)`.
  - After session init, `grouped["group1"]` yields list `[(dx, x)]` with `dx == 96` and `x == 8`.
- `test_check_ops_dependence`:
  - `v.assign_add(1)` stored as `add`; create `t1`, `t2` under control dependency on `add`.
  - `check_ops_dependence(t1.op.name, add.name)` raises `Exception`.
  - `check_ops_dependence(t1.op.name, t2.op.name)` does not raise.
- `test_collections`:
  - Adds scalars, lists, and None to collections.
  - `get_collection('int')[-1] == 2`, `'str'[-1] == 'str'`, `'bool'[-1] == True`.
  - Lists: last entries are `[4,5,6]`, `['hello','world']`, `[False]` (empty list and None are skipped).
- Uses TF v1 session mode (`tf.test.TestCase` + `self.session()`).
- `tf.compat.v1.disable_eager_execution()` in `__main__`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/utils.rs` (new).
- Rust public API surface: `propagate_back_dict_gradients`, `check_ops_dependence`, `add_to_collections`, `get_collection`.
- Data model mapping: Rust-side tensor or TF runtime tensor under `tf-runtime` feature.
- Feature gating: tests run only when TF backend is enabled.
- Integration points: `monolith-training` utils module.

**Implementation Steps (Detailed)**
1. Port `test_propagate_back_dict_gradients` using TF backend (or skip with explicit feature guard).
2. Port `test_check_ops_dependence` by constructing a simple TF graph and verifying exception behavior.
3. Port `test_collections` by adding values to TF collections and asserting last entries.
4. Add native-only tests for non-TF helpers (string/path/env) separately.

**Tests (Detailed)**
- Python tests: `UtilsTest.test_propagate_back_dict_gradients`, `.test_check_ops_dependence`, `.test_collections`.
- Rust tests:
  - `propagate_back_dict_gradients_matches_python` (TF backend)
  - `check_ops_dependence_raises_on_dependency` (TF backend)
  - `collections_add_get` (TF backend)
- Cross-language parity test:
  - Compare gradients and collection ordering between Python and Rust TF backends.

**Gaps / Notes**
- Collections ordering relies on TF collection append order; Rust must preserve the same semantics.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/variables.py`
<a id="monolith-native-training-variables-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 147
- Purpose/role: Provides a cached-variable mechanism for distributed TF training: creates local cached copies of remote variables, updates them via fetch/assign ops, and supplies a session hook.
- Key symbols/classes/functions: `_CACHED_VARIABLES`, `CachedVariableAssociates`, `CachedVariableMeta`, `cached_value`, `cached_variable_creator`, `fetch_all_cached_variables`, `assign_all_cached_variables`, `FetchAllCachedVariablesHook`.
- External dependencies: TensorFlow (resource variables, custom gradients, estimator hooks), `graph_meta.get_meta`.
- Side effects:
  - Mutates variables’ private `_cached_value` attribute.
  - Adds variables to TF collections.
  - Creates local (worker) `LOCAL_VARIABLES` for cache/fetch.

**Required Behavior (Detailed)**
- `_CACHED_VARIABLES = "monolith_cached_variables"`: TF collection name storing original vars that are cached.
- Data classes:
  - `CachedVariableAssociates(async_fetched_var, async_cached_var)`.
  - `CachedVariableMeta(var_id_to_assoc: Dict[int, CachedVariableAssociates])`.
- `_get_meta()`:
  - Uses `graph_meta.get_meta("cached_variables_meta", CachedVariableMeta)` to create/retrieve per-graph metadata.
- `cached_value(var, async_cached_var)` (custom gradient):
  - Forward: returns `async_cached_var` (cached local value).
  - Backward: returns gradient `(dy, None)` → gradients flow to `var` only; cached var has no gradient.
- `_get_valid_op_name(name)`:
  - Replaces `":"` and `"/"` with `"_"` for safe op naming.
- `cached_variable_creator(next_creator, **kwargs)`:
  - Creates `var = next_creator(**kwargs)`.
  - Validates: `var` must be `ResourceVariable` else `ValueError("Only ResourceVariable is supported. Do you disable V2 behavior or use strategy?")`.
  - If `var._cached_value` already set: `ValueError("The variable has already been cached. Consider about removing cache_device.")`.
  - Creates `async_cached_var` and `async_fetched_var` under `tf.device(None)`:
    - `resource_variable_ops.ResourceVariable`
    - `initial_value=var.initial_value`, `trainable=False`
    - `collections=[tf.compat.v1.GraphKeys.LOCAL_VARIABLES]`
    - `shape=var.shape`, `dtype=var.dtype`
  - If `async_cached_var.device == var.device`, returns `var` unchanged (skip cache).
  - Else:
    - Adds `var` to `_CACHED_VARIABLES` collection.
    - Sets `var._cached_value = cached_value(var, async_cached_var)`.
    - Stores associates in meta: `var_id_to_assoc[id(var)] = CachedVariableAssociates(...)`.
  - Returns `var`.
- `fetch_all_cached_variables()`:
  - For each `var` in `_CACHED_VARIABLES`:
    - Gets `fetched_var = meta.var_id_to_assoc[id(var)].async_fetched_var`.
    - Adds assign op: `fetched_var.assign(var._read_variable_op(), name="fetch_from_{device}", read_value=False)`, where device is sanitized via `_get_valid_op_name`.
  - Returns `tf.group(ops)` (no name).
- `assign_all_cached_variables()`:
  - For each cached `var`:
    - Assigns `associates.async_cached_var.assign(associates.async_fetched_var, name="assign_cached_var", read_value=False)`.
  - Returns `tf.group(ops, name="assign_all_cached_variables")`.
- `FetchAllCachedVariablesHook`:
  - Initializes `_fetch_op`, `_assign_op`, `_first_run = True`.
  - `after_create_session`: resets `_first_run`.
  - `before_run`:
    - If first run: synchronously `session.run(_fetch_op)` then `session.run(_assign_op)`.
    - Returns `SessionRunArgs(_fetch_op)` to fetch each step.
  - `after_run`: runs `_assign_op` to update cached vars after fetch.
- Observed behavior:
  - `var * 1.0` uses cached value (`_cached_value`), while `var` reads the actual remote value.
  - Updates to `var` only appear in cached reads after fetch/assign.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/variables.rs` (new).
- Rust public API surface:
  - `cached_variable_creator(...)` equivalent for TF backend.
  - `fetch_all_cached_variables()`, `assign_all_cached_variables()`.
  - `FetchAllCachedVariablesHook` analog (or callback in training loop).
- Data model mapping:
  - TF resource variables ↔ TF runtime handles.
  - Cached local variables stored in a per-graph registry keyed by var identity.
- Feature gating:
  - Only meaningful under `tf-runtime` (Candle backend has no TF graph/collections).
- Integration points:
  - Distributed training loops that rely on cached reads (e.g., PS-based training).

**Implementation Steps (Detailed)**
1. Add TF-backend registry for cached variables (keyed by var identity/handle).
2. Implement cached read wrapper (custom gradient or TF graph rewrite) so reads use cached local vars but gradients flow to originals.
3. Create local (worker) cached + fetched variables with `LOCAL_VARIABLES` collection.
4. Implement fetch/assign ops; preserve op naming via `_get_valid_op_name`.
5. Add a hook/callback in training loop that mirrors `FetchAllCachedVariablesHook` scheduling.
6. Define behavior when cached vars colocate with originals (skip caching).
7. Gate all behavior behind TF runtime feature; document unsupported for Candle.

**Tests (Detailed)**
- Python tests: `monolith/native_training/variables_test.py`.
- Rust tests:
  - `test_basic_cached_variable` (TF backend).
  - `test_hook_cached_variable` (TF backend).
  - `test_gradient_cached_variable` (TF backend).
- Cross-language parity test:
  - Run Python and Rust graphs with same PS setup and compare cached vs direct reads after updates.

**Gaps / Notes**
- Uses private TF APIs: `var._cached_value`, `var._read_variable_op()`.
- Imports `variables_lib` and `core` but does not use them.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/variables_test.py`
<a id="monolith-native-training-variables-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 89
- Purpose/role: Validates cached-variable behavior under PS training, hook scheduling, and gradient updates.
- Key symbols/classes/functions: `CachedVariableTest.testBasic`, `.testHook`, `.testGradient`.
- External dependencies: TensorFlow, `variables`, `test_utils.create_test_ps_cluster`.
- Side effects: Starts local PS servers for test sessions.

**Required Behavior (Detailed)**
- `testBasic`:
  - Create local PS cluster with 2 servers.
  - Use `tf.variable_creator_scope(cached_variable_creator)` and place `var` on `/job:ps/task:1`.
  - After init:
    - `var * 1.0` returns cached value (5.0).
    - After `var.assign_add(2.0)`: `var * 1.0` still 5.0; `var` reads 7.0.
  - After `fetch_all_cached_variables()` + `assign_all_cached_variables()`: `var * 1.0` becomes 7.0.
- `testHook`:
  - Build `var` on PS with cache creator; `var_cached = var * 1.0`.
  - Use `FetchAllCachedVariablesHook` with `SingularMonitoredSession`.
  - After `assign_sub(var, 1.0)`:
    - `var_cached` may take up to two runs to reflect update; final expected value is 4.0.
- `testGradient`:
  - Use SGD optimizer with `loss = var`.
  - After `opt.minimize(loss)`: `var` reads 4.0 (updated).
  - Cached read (`var * 1.0`) still 5.0 (not fetched yet).
- Tests run with TF v1 sessions; eager disabled in `__main__`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/variables.rs` (new).
- Rust public API surface: `cached_variable_creator`, `fetch_all_cached_variables`, `assign_all_cached_variables`, `FetchAllCachedVariablesHook`.
- Data model mapping: TF runtime variables and sessions.
- Feature gating: TF backend only.
- Integration points: PS cluster creation helpers (Rust equivalent of `test_utils.create_test_ps_cluster`).

**Implementation Steps (Detailed)**
1. Port `testBasic` using a local PS cluster and cached variable creator.
2. Port `testHook` with a Rust equivalent of session hook (before_run/after_run callbacks).
3. Port `testGradient` to ensure gradients update original var but cached reads remain stale.
4. Ensure the cached read path (`var * 1.0`) uses cached value in Rust TF backend.

**Tests (Detailed)**
- Python tests: `CachedVariableTest.testBasic`, `.testHook`, `.testGradient`.
- Rust tests:
  - `cached_variable_basic` (TF backend)
  - `cached_variable_hook_updates` (TF backend)
  - `cached_variable_gradient_reads` (TF backend)
- Cross-language parity test:
  - Compare cached vs direct reads across the same update sequence in Python and Rust.

**Gaps / Notes**
- Hook behavior is sensitive to session scheduling; Rust must preserve the “fetch before run, assign after run” pattern.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/yarn_runtime.py`
<a id="monolith-native-training-yarn-runtime-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 127
- Purpose/role: Yarn/Primus runtime helpers for local host resolution and AppMaster gRPC control (kill, finish, savepoint).
- Key symbols/classes/functions: `get_local_host`, `_get_primus_am_host`, `_get_channel`, `maybe_kill_application`, `maybe_finish_application`, `create_primus_save_point`.
- External dependencies: `grpc`, `primus_am_service_pb2(_grpc)`, `net_utils.get_local_ip`, env vars.
- Side effects:
  - Creates/caches gRPC channels.
  - Sends kill/succeed/savepoint requests to AppMaster.
  - Sleeps while waiting for savepoint completion.
  - Logs info/error messages.

**Required Behavior (Detailed)**
- `get_local_host()`:
  - If `CLOUDNATIVE_INET_ADDR` env var exists: use first entry before comma.
  - Else if `YARN_INET_ADDR` exists: use that value.
  - Else: `net_utils.get_local_ip()`.
  - Asserts non-empty result and returns it.
- `_get_primus_am_host()`:
  - If both `PRIMUS_AM_RPC_HOST` and `PRIMUS_AM_RPC_PORT` are set, returns `"host:port"`.
  - Else returns empty string.
- `_get_channel(addr)`:
  - Caches `grpc.insecure_channel(addr)` in `_CHANNEL_MAP`.
  - Returns cached channel for the same addr.
- `maybe_kill_application(reason) -> bool`:
  - If Primus AM host is available:
    - Builds `KillRequest` with `exit_code=1`, `diagnose=reason`, `graceful_shutdown_timeout_ms=20000`.
    - Calls `stub.kill(req, timeout=10)`.
    - Returns `True` on success; `False` on `grpc.RpcError`.
  - If no host: logs “Current framework doesn't support kill. Ignore killing...” and returns `False`.
- `maybe_finish_application()`:
  - If Primus AM host is available:
    - Builds `SucceedRequest` with `graceful_shutdown_timeout_ms=20000`.
    - Calls `stub.succeed(req, timeout=10)`.
    - Returns `True` on success; logs on `grpc.RpcError` (returns `None`).
  - If no host: returns `None` (no log).
- `create_primus_save_point(dst) -> bool`:
  - If Primus AM host is available:
    - Calls `createSavepoint` with `savepoint_dir=dst`.
    - If response `code != 0`, logs error and returns `False`.
    - Else polls `createSavepointStatus` every 5s:
      - `PENDING`/`RUNNING`: sleep and continue.
      - `SUCCEEDED`: log success and return `True`.
      - Any other state: log error and return `False`.
    - On `grpc.RpcError`: logs and returns `False`.
  - If no host: returns `None`.
- Threading/concurrency:
  - `_CHANNEL_MAP` is a global dict without a lock; concurrent callers can race.
- Performance:
  - Savepoint polling is blocking with 5s sleep intervals.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/yarn_runtime.rs` (new).
- Rust public API surface:
  - `get_local_host() -> String`
  - `maybe_kill_application(reason: &str) -> bool`
  - `maybe_finish_application() -> Option<bool>`
  - `create_primus_save_point(dst: &str) -> Option<bool>`
- Data model mapping:
  - gRPC service `primus_am_service.proto` already in `monolith-rs/proto`.
  - Use tonic/grpc-generated client stubs for `AppMasterService`.
- Feature gating:
  - Requires gRPC + proto build; optional if runtime doesn't use Primus.
- Integration points:
  - Distributed training orchestration that needs to terminate or checkpoint jobs.

**Implementation Steps (Detailed)**
1. Generate Rust gRPC client from `primus_am_service.proto`.
2. Implement env-var host resolution (`CLOUDNATIVE_INET_ADDR` > `YARN_INET_ADDR` > `net_utils` equivalent).
3. Add a channel cache (e.g., `DashMap` or `Mutex<HashMap<...>>`) to mirror `_CHANNEL_MAP`.
4. Implement `maybe_kill_application` and `maybe_finish_application` with the same timeout and fields.
5. Implement `create_primus_save_point` with polling loop + 5s sleep.
6. Mirror return semantics (False vs None) or document a deliberate normalization.

**Tests (Detailed)**
- Python tests: `monolith/native_training/yarn_runtime_test.py`.
- Rust tests:
  - `get_local_host_env_override` (CLOUDNATIVE_INET_ADDR + YARN_INET_ADDR cases).
  - gRPC kill/finish/savepoint flow using an in-process gRPC server.
- Cross-language parity test:
  - Simulate the same gRPC server and verify request fields + timeouts.

**Gaps / Notes**
- `maybe_finish_application` does not return `False` on errors; it logs and returns `None`.
- `create_primus_save_point` and `maybe_finish_application` return `None` when no Primus host is configured.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/yarn_runtime_test.py`
<a id="monolith-native-training-yarn-runtime-test-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 133
- Purpose/role: Exercises env-based host resolution and gRPC AppMaster controls (kill/finish/savepoint).
- Key symbols/classes/functions: `YarnRuntimeTest`, `test_get_local_host_*`, `test_kill`, `test_finish`, `test_save_primus`.
- External dependencies: gRPC, Primus AM proto stubs, `unittest.mock` for env vars.
- Side effects: Starts in-process gRPC servers (unix: addresses).

**Required Behavior (Detailed)**
- `test_get_local_host_overwrite`:
  - With `YARN_INET_ADDR=1.2.3.4`, `get_local_host()` returns `1.2.3.4`.
- `test_get_local_host_overwrite_by_cloudnative`:
  - With `CLOUDNATIVE_INET_ADDR=1.2.3.4,5.6.7.8`, returns `1.2.3.4`.
- `test_get_local_host_basic`:
  - Calls `get_local_host()` without explicit env overrides (no assertion).
- `test_kill`:
  - Sets `PRIMUS_AM_RPC_HOST=unix`, `PRIMUS_AM_RPC_PORT=test_kill`.
  - Starts gRPC server with `kill` handler; validates `request.diagnose` equals reason.
  - Calls `maybe_kill_application(reason)` and asserts handler invoked.
- `test_finish`:
  - Similar setup; installs `succeed` handler; asserts invoked after `maybe_finish_application()`.
- `test_save_primus`:
  - Sets Primus host/port; installs `createSavepoint` and `createSavepointStatus` handlers.
  - Status handler returns `SUCCEEDED`; `create_primus_save_point(dst)` returns `True`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/tests/yarn_runtime.rs` (new).
- Rust public API surface: `get_local_host`, `maybe_kill_application`, `maybe_finish_application`, `create_primus_save_point`.
- Data model mapping: use Rust gRPC server/client for `AppMasterService`.
- Feature gating: tests require gRPC + proto build; optional for non-Primus builds.
- Integration points: gRPC server harness for tests (likely tokio + tonic).

**Implementation Steps (Detailed)**
1. Port env override tests for `get_local_host`.
2. Implement a local gRPC server with unary handlers for `kill`, `succeed`, `createSavepoint`, `createSavepointStatus`.
3. Validate request fields (e.g., `diagnose` for kill) and that calls occur.
4. Ensure unix-domain or loopback TCP addressing works in Rust test harness.

**Tests (Detailed)**
- Python tests: `YarnRuntimeTest.test_get_local_host_*`, `.test_kill`, `.test_finish`, `.test_save_primus`.
- Rust tests:
  - `get_local_host_env_overrides`
  - `maybe_kill_application_calls_stub`
  - `maybe_finish_application_calls_stub`
  - `create_primus_save_point_succeeds`
- Cross-language parity test:
  - Compare request fields and call ordering with a mock gRPC server.

**Gaps / Notes**
- Python tests use gRPC addresses like `unix:test_kill`; Rust must support unix-domain channels or adapt tests to TCP.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/native_training/zk_utils.py`
<a id="monolith-native-training-zk-utils-py"></a>

**Status:** IN PROGRESS (manual review complete)

**Python Summary**
- Lines: 96
- Purpose/role: Zookeeper utilities for host selection (IPv4 vs IPv6), default server list, authenticated Kazoo client, and cleanup of stale ZK paths.
- Key symbols/classes/functions: `_PORT`, `_HOSTS`, `_HOSTS_IPV6`, `is_ipv6_only`, `default_zk_servers`, `MonolithKazooClient`, `clear_zk_path`.
- External dependencies: `kazoo.client.KazooClient`, `env_utils.get_zk_auth_data`, `socket`, `datetime`.
- Side effects:
  - Connects to ZK, mutates nodes (delete paths).
  - Logs informational messages about IP resolution.

**Required Behavior (Detailed)**
- Globals:
  - `_PORT = 2181`.
  - `_HOSTS` and `_HOSTS_IPV6` are defined with IPs, then overwritten to empty lists later (current effective values are empty).
- `is_ipv6_only()`:
  - If any of `MY_HOST_IP`, `MY_POD_IP`, `MY_HOST_IPV6` env vars are set:
    - Treat as “tce/byterec env”.
    - `ipv4_addr` from `MY_HOST_IP` or `MY_POD_IP` (may be `None`).
  - Else:
    - `ipv4_addr = socket.gethostbyname(socket.gethostname())` (except → `None`).
  - Logs `ipv4_addr`, then sets `ipv6_only = not ipv4_addr` and logs the result.
  - Returns `ipv6_only`.
- `default_zk_servers(use_ipv6: bool = False)`:
  - If `use_ipv6` or `is_ipv6_only()`:
    - Returns comma-joined `"[ip]:port"` entries from `_HOSTS_IPV6`.
  - Else:
    - Returns comma-joined `"ip:port"` entries from `_HOSTS`.
  - With current `_HOSTS`/`_HOSTS_IPV6` emptied, returns empty string.
- `MonolithKazooClient`:
  - Subclasses `KazooClient`.
  - If `auth_data` not provided, injects `get_zk_auth_data()` into kwargs.
- `clear_zk_path(zk_server, job_name, force_clear_zk_path)`:
  - Connects with `MonolithKazooClient(zk_server or default_zk_servers())`.
  - `base_path = "/monolith"`, `delta = timedelta(weeks=9)`.
  - `ensure_path(base_path)`.
  - For each child under `/monolith`:
    - `get_children(path, include_data=True)` → stat; if `stat.mtime // 1000 + delta < now`, delete recursively (ignore errors).
  - For current `job_name`:
    - If exists and `force_clear_zk_path`: delete recursively.
    - Else: raise `ValueError("there are [<children>] in monolith zk path")` using current children list.
  - Always stops client in `finally`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/zk_utils.rs` (new).
- Rust public API surface:
  - `is_ipv6_only() -> bool`
  - `default_zk_servers(use_ipv6: bool) -> String`
  - `MonolithZkClient` wrapper (auth inject)
  - `clear_zk_path(zk_server: Option<&str>, job_name: &str, force: bool) -> Result<(), Error>`
- Data model mapping:
  - Kazoo client ↔ Rust ZK client (e.g., `zookeeper` crate).
  - `stat.mtime` (ms) ↔ Rust stat `mtime` (ms).
- Feature gating:
  - ZK client optional for environments without ZK.
- Integration points:
  - Any training components relying on ZK-based coordination or cleanup.

**Implementation Steps (Detailed)**
1. Decide Rust ZK client crate and auth mechanism matching `get_zk_auth_data()`.
2. Implement `is_ipv6_only` using env vars and hostname resolution.
3. Port `default_zk_servers` formatting (IPv6 `[ip]:port`).
4. Implement `clear_zk_path` with the same TTL logic (9 weeks) and error message.
5. Preserve the “ignore delete errors” behavior on stale node cleanup.

**Tests (Detailed)**
- Python tests: none in repo.
- Rust tests:
  - Unit tests for `default_zk_servers` formatting and env-driven `is_ipv6_only`.
  - Integration tests for `clear_zk_path` if ZK test container is available.
- Cross-language parity test:
  - Compare TTL deletion behavior with a mocked ZK server.

**Gaps / Notes**
- `_HOSTS` and `_HOSTS_IPV6` are overwritten to empty lists, so `default_zk_servers` returns empty string by default (likely a sanitization artifact).

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/path_utils.py`
<a id="monolith-path-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 47
- Purpose/role: Locate monolith base directory and resolve paths to bundled libraries.
- Key symbols/classes/functions: `find_main`, `get_libops_path`.
- External dependencies: `os` only (explicitly avoids third-party imports).
- Side effects: raises ValueError when base directory cannot be found.

**Required Behavior (Detailed)**
- `find_main()`:
  - Uses `__file__` path; searches for split markers in order: `/__main__/`, `/site-packages/`, `/monolith/`.
  - If marker found:
    - For `/monolith/`: `main_dir` is path prefix before marker.
    - Else: `main_dir` is prefix joined with the marker path component.
  - Returns `main_dir` only if `main_dir/monolith` exists; else raises ValueError with full path in message.
- `get_libops_path(lib_name)`:
  - Returns `os.path.join(find_main(), lib_name)`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-core/src/path_utils.rs`.
- Rust public API surface: `find_main()` and `get_libops_path()`.

**Implementation Steps (Detailed)**
1. Implement path resolution with the same marker order and behavior.
2. Preserve error message details for diagnostics.

**Tests (Detailed)**
- Python tests: `monolith/utils_test.py` (find_main / get_libops_path).
- Rust tests: replicate `find_main` behavior under test harness.

**Gaps / Notes**
- Behavior depends on Bazel layout (`__main__`); Rust tests should simulate or adjust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/tpu_runner.py`
<a id="monolith-tpu-runner-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 429
- Purpose/role: TPU training/eval runner using `tf.estimator.tpu.TPUEstimator` with embedding support and optional CPU test mode.
- Key symbols/classes/functions: `TPURunner`, `create_tpu_estimator`, `create_tpu_estimator_on_cpu`, `run`, CLI `main`.
- External dependencies: `tensorflow.compat.v1`, `cloud_tpu_client`, `BaseEmbeddingTask`, `model_registry`.
- Side effects: configures TPU versions, launches training/eval, writes summaries.

**Required Behavior (Detailed)**
- CLI flags include TPU location (`tpu`, `gcp_project`, `tpu_zone`), mode, model_dir, checkpoint interval, iteration counts, embedding options, CPU test, partition strategy, overwrite_end_date.
- `TPURunner.__init__`:
  - Reads flags and task_param; sets accelerator to `tpu`.
  - Allows task_param to override save_checkpoints_steps.
  - Optionally overwrites `train.end_date` when flag provided.
- `create_tpu_estimator`:
  - Optionally configures TPU version and waits for healthy.
  - Builds TPU cluster resolver, computes total replicas and global batch size.
  - Sets TPUConfig with iterations_per_loop and host_call settings.
  - Uses `TPUInfeedOutfeedSessionWithEndOfStreamHandlingHook` when stopping signals enabled.
  - Builds embedding_config_spec if feature/table configs exist.
  - Returns TPUEstimator and total_replicas.
- `create_tpu_estimator_on_cpu`:
  - Creates TPUEstimator with `use_tpu=False` and small batch size; used for CPU test.
- `run()`:
  - Loads global step (or 0).
  - Instantiates task; if BaseEmbeddingTask, prepares feature/table configs.
  - Uses CPU test wrapper if enabled.
  - `train`: `est.train`.
  - `eval`: iterates checkpoints and evaluates; writes summaries and stops at max_steps.
  - `train_and_eval`: not supported (raises TypeError).
- `main`: logs FLAGS and task_param, runs runner.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-training/src/tpu_runner.rs`.
- Rust public API surface: runner struct + CLI entrypoint.
- Data model mapping: TPU/embedding configs likely require TF runtime bindings.
- Feature gating: `tf-runtime`, `tpu` features.

**Implementation Steps (Detailed)**
1. Port CLI flag set and task registry lookup.
2. Decide runtime strategy (native Rust vs TF bridge).
3. Preserve TPU version config and host_call/embedding config semantics if using TF.
4. Mirror evaluation-by-checkpoint loop and summary writing.

**Tests (Detailed)**
- Python tests: none specific.
- Rust tests: CLI parsing + control flow tests; TPU behavior likely gated/skipped in CI.

**Gaps / Notes**
- Full parity depends on TensorFlow TPU Estimator which is not available natively in Rust.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/utils.py`
<a id="monolith-utils-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 81
- Purpose/role: Small utility helpers for TF monkey patching and recursive file copy with tf.io.gfile.
- Key symbols/classes/functions: `enable_monkey_patch`, `CopyFile`, `CopyRecursively`.
- External dependencies: `tensorflow`, `ThreadPoolExecutor`.
- Side effects: modifies `tensorflow.python.training.monitored_session` module attribute; copies files (possibly remote).

**Required Behavior (Detailed)**
- `enable_monkey_patch()`:
  - Imports `tensorflow.python.training.monitored_session` and sets `_PREEMPTION_ERRORS` to `(tf.errors.AbortedError,)`.
- `CopyFile(src, dst, overwrite=True, skip_nonexist=True, max_retries=5)`:
  - Uses `tf.io.gfile.copy` and retries on NotFoundError (skip if `skip_nonexist`).
- `CopyRecursively(src, dst, max_workers=1, skip_nonexist=True, max_retries=5)`:
  - Recursively copies a directory tree via `tf.io.gfile`.
  - If `src` missing and `skip_nonexist`, returns; else raises ValueError.
  - If `dst` exists, removes it then recreates.
  - If `max_workers > 1`, uses ThreadPoolExecutor to copy files in parallel.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/src/utils.rs` (TF-dependent utilities) plus `monolith-core/src/path_utils.rs` for re-exports.
- Rust public API surface: equivalents for monkey patch (if bridging TF) and recursive copy.

**Implementation Steps (Detailed)**
1. Provide TF monkey patch equivalent when using Python/TF runtime; otherwise document unsupported.
2. Implement recursive copy with retries and optional parallelism.
3. Ensure semantics match tf.io.gfile for remote paths (HDFS/GCS) where needed.

**Tests (Detailed)**
- Python tests: `monolith/utils_test.py`
- Rust tests: port tests for `find_main`, `get_libops_path`, and CopyRecursively.

**Gaps / Notes**
- Full parity requires tf.io.gfile support; Rust may need a virtual filesystem abstraction.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed

### `monolith/utils_test.py`
<a id="monolith-utils-test-py"></a>

**Status:** IN PROGRESS (manual)

**Python Summary**
- Lines: 65
- Purpose/role: Tests path utilities, monkey patch, and recursive copy.
- Key symbols/classes/functions: `UtilsTest` test cases.
- External dependencies: `tensorflow`, `monitored_session`.
- Side effects: writes temp files in `/tmp`.

**Required Behavior (Detailed)**
- `testFindMain`: `utils.find_main()` base dir last path component is `__main__` (Bazel layout assumption).
- `testGetLibopsPath`: `utils.get_libops_path("monolith/utils_test.py")` exists.
- `testLoadMonitoredSession`: `_PREEMPTION_ERRORS` equals `(errors.AbortedError,)` after monkey patch.
- `testMultiThreadedCopy`: creates nested dirs/files and verifies copied content with `CopyRecursively(max_workers=2)`.

**Rust Mapping (Detailed)**
- Target crate/module: `monolith-rs/crates/monolith-tf/tests/utils.rs`.
- Rust public API surface: path utils + recursive copy.

**Implementation Steps (Detailed)**
1. Port tests with temp dirs and file content checks.
2. If TF monkey patch is unsupported, document and adjust tests accordingly.

**Tests (Detailed)**
- Python tests: this file
- Rust tests: parity tests for path and copy semantics.

**Gaps / Notes**
- `find_main` behavior is tied to Bazel execution layout; Rust tests may need harness adjustments.

**Verification Checklist (Must be Checked Off)**
- [ ] All public functions/classes mapped to Rust
- [ ] Behavior matches Python on normal inputs
- [ ] Error handling parity confirmed
- [ ] Config/env precedence parity confirmed
- [ ] I/O formats identical (proto/JSON/TFRecord/pbtxt)
- [ ] Threading/concurrency semantics preserved
- [ ] Logging/metrics parity confirmed
- [ ] Performance risks documented
- [ ] Rust tests added and passing
- [ ] Cross-language parity test completed
